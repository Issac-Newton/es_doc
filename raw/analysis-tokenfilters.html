<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Token filter reference | Elasticsearch Guide | Elastic</title>
<meta class="elastic" name="content" content="Token filter reference | Elasticsearch Guide">

<link rel="home" href="index.html" title="Elasticsearch Guide"/>
<link rel="up" href="analysis.html" title="Text analysis"/>
<link rel="prev" href="analysis-tokenizers.html" title="Tokenizer reference"/>
<link rel="next" href="analysis-charfilters.html" title="Character filters reference"/>
<meta class="elastic" name="product_version" content=""/>
<meta class="elastic" name="product_name" content=""/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/"/>
<meta name="DC.subject" content=""/>
<meta name="DC.identifier" content=""/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="analysis.html">Text analysis</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="analysis-tokenizers.html">« Tokenizer reference</a>
</span>
<span class="next">
<a href="analysis-charfilters.html">Character filters reference »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-tokenfilters"></a>Token filter reference<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters.asciidoc">edit</a></h2>
</div></div></div>
<p>Token filters accept a stream of tokens from a
<a class="xref" href="analysis-tokenizers.html" title="Tokenizer reference">tokenizer</a> and can modify tokens
(eg lowercasing), delete tokens (eg remove stopwords)
or add tokens (eg synonyms).</p>
<p>Elasticsearch has a number of built-in token filters you can use
to build <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzers</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-apostrophe-tokenfilter"></a>Apostrophe token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/apostrophe-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Strips all characters after an apostrophe, including the apostrophe itself.</p>
<p>This filter is included in Elasticsearch&#8217;s built-in <a class="xref" href="analysis-analyzers.html#turkish-analyzer" title="turkish analyzer">Turkish language
analyzer</a>. It uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/tr/ApostropheFilter.html" class="ulink" target="_top">ApostropheFilter</a>, which was
built for the Turkish language.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-apostrophe-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/apostrophe-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request demonstrates how the
apostrophe token filter works.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["apostrophe"],
  "text" : "Istanbul'a veya Istanbul'dan"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/402.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Istanbul, veya, Istanbul ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-apostrophe-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/apostrophe-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
apostrophe token filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /apostrophe_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_apostrophe": {
          "tokenizer": "standard",
          "filter": [ "apostrophe" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/403.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-asciifolding-tokenfilter"></a>ASCII folding token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Converts alphabetic, numeric, and symbolic characters that are not in the Basic
Latin Unicode block (first 127 ASCII characters) to their ASCII equivalent, if
one exists. For example, the filter changes <code class="literal">à</code> to <code class="literal">a</code>.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html" class="ulink" target="_top">ASCIIFoldingFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-asciifolding-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">asciifolding</code>
filter to drop the diacritical marks in <code class="literal">açaí à la carte</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["asciifolding"],
  "text" : "açaí à la carte"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/404.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ acai, a, la, carte ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-asciifolding-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">asciifolding</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /asciifold_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_asciifolding": {
          "tokenizer": "standard",
          "filter": [ "asciifolding" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/405.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-asciifolding-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">preserve_original</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, emit both original tokens and folded tokens.
Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-asciifolding-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">asciifolding</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">asciifolding</code> filter with
<code class="literal">preserve_original</code> set to true:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /asciifold_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_asciifolding": {
          "tokenizer": "standard",
          "filter": [ "my_ascii_folding" ]
        }
      },
      "filter": {
        "my_ascii_folding": {
          "type": "asciifolding",
          "preserve_original": true
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/406.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-cjk-bigram-tokenfilter"></a>CJK bigram token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/cjk-bigram-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Forms <a href="https://en.wikipedia.org/wiki/Bigram" class="ulink" target="_top">bigrams</a> out of CJK (Chinese,
Japanese, and Korean) tokens.</p>
<p>This filter is included in Elasticsearch&#8217;s built-in <a class="xref" href="analysis-analyzers.html#cjk-analyzer" title="cjk analyzer">CJK language
analyzer</a>. It uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/cjk/CJKBigramFilter.html" class="ulink" target="_top">CJKBigramFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-cjk-bigram-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/cjk-bigram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request demonstrates how the
CJK bigram token filter works.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["cjk_bigram"],
  "text" : "東京都は、日本の首都であり"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/407.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ 東京, 京都, 都は, 日本, 本の, の首, 首都, 都で, であ, あり ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-cjk-bigram-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/cjk-bigram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
CJK bigram token filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /cjk_bigram_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_cjk_bigram": {
          "tokenizer": "standard",
          "filter": [ "cjk_bigram" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/408.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-cjk-bigram-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/cjk-bigram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">ignored_scripts</code>
</span>
</dt>
<dd>
<p>(Optional, array of character scripts)
Array of character scripts for which to disable bigrams.
Possible values:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">han</code>
</li>
<li class="listitem">
<code class="literal">hangul</code>
</li>
<li class="listitem">
<code class="literal">hiragana</code>
</li>
<li class="listitem">
<code class="literal">katakana</code>
</li>
</ul>
</div>
<p>All non-CJK input is passed through unmodified.</p>
</dd>
</dl>
</div>
<p><code class="literal">output_unigrams</code>
(Optional, Boolean)
If <code class="literal">true</code>, emit tokens in both bigram and
<a href="https://en.wikipedia.org/wiki/N-gram" class="ulink" target="_top">unigram</a> form. If <code class="literal">false</code>, a CJK character
is output in unigram form when it has no adjacent characters. Defaults to
<code class="literal">false</code>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-cjk-bigram-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/cjk-bigram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the CJK bigram token filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /cjk_bigram_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "han_bigrams": {
          "tokenizer": "standard",
          "filter": [ "han_bigrams_filter" ]
        }
      },
      "filter": {
        "han_bigrams_filter": {
          "type": "cjk_bigram",
          "ignored_scripts": [
            "hangul",
            "hiragana",
            "katakana"
          ],
          "output_unigrams": true
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/409.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-cjk-width-tokenfilter"></a>CJK width token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/cjk-width-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Normalizes width differences in CJK (Chinese, Japanese, and Korean) characters
as follows:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Folds full-width ASCII character variants into the equivalent basic Latin
characters
</li>
<li class="listitem">
Folds half-width Katakana character variants into the equivalent Kana
characters
</li>
</ul>
</div>
<p>This filter is included in Elasticsearch&#8217;s built-in <a class="xref" href="analysis-analyzers.html#cjk-analyzer" title="cjk analyzer">CJK language
analyzer</a>. It uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/cjk/CJKWidthFilter.html" class="ulink" target="_top">CJKWidthFilter</a>.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>This token filter can be viewed as a subset of NFKC/NFKD Unicode
normalization. See the
<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.10/analysis-icu-normalization-charfilter.html" class="ulink" target="_top"><code class="literal">analysis-icu</code> plugin</a> for
full normalization support.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-cjk-width-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/cjk-width-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["cjk_width"],
  "text" : "ｼｰｻｲﾄﾞﾗｲﾅｰ"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/410.console"></div>
<p>The filter produces the following token:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">シーサイドライナー</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-cjk-width-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/cjk-width-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
CJK width token filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /cjk_width_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_cjk_width": {
          "tokenizer": "standard",
          "filter": [ "cjk_width" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/411.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-classic-tokenfilter"></a>Classic token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/classic-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Performs optional post-processing of terms generated by the
<a class="xref" href="analysis-tokenizers.html#analysis-classic-tokenizer" title="Classic tokenizer"><code class="literal">classic</code> tokenizer</a>.</p>
<p>This filter removes the english possessive (<code class="literal">'s</code>) from the end of words and
removes dots from acronyms. It uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/standard/ClassicFilter.html" class="ulink" target="_top">ClassicFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-classic-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/classic-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request demonstrates how the
classic token filter works.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer" : "classic",
  "filter" : ["classic"],
  "text" : "The 2 Q.U.I.C.K. Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/412.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog, bone ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-classic-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/classic-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
classic token filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /classic_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "classic_analyzer": {
          "tokenizer": "classic",
          "filter": [ "classic" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/413.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-common-grams-tokenfilter"></a>Common grams token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/common-grams-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Generates <a href="https://en.wikipedia.org/wiki/Bigram" class="ulink" target="_top">bigrams</a> for a specified set of
common words.</p>
<p>For example, you can specify <code class="literal">is</code> and <code class="literal">the</code> as common words. This filter then
converts the tokens <code class="literal">[the, quick, fox, is, brown]</code> to <code class="literal">[the, the_quick, quick,
fox, fox_is, is, is_brown, brown]</code>.</p>
<p>You can use the <code class="literal">common_grams</code> filter in place of the
<a class="xref" href="analysis-tokenfilters.html#analysis-stop-tokenfilter" title="Stop token filter">stop token filter</a> when you don&#8217;t want to
completely ignore common words.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/commongrams/CommonGramsFilter.html" class="ulink" target="_top">CommonGramsFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-common-grams-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/common-grams-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request creates bigrams for <code class="literal">is</code>
and <code class="literal">the</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer" : "whitespace",
  "filter" : [
    {
      "type": "common_grams",
      "common_words": ["is", "the"]
    }
  ],
  "text" : "the quick fox is brown"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/414.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, the_quick, quick, fox, fox_is, is, is_brown, brown ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-common-grams-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/common-grams-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">common_grams</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /common_grams_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "index_grams": {
          "tokenizer": "whitespace",
          "filter": [ "common_grams" ]
        }
      },
      "filter": {
        "common_grams": {
          "type": "common_grams",
          "common_words": [ "a", "is", "the" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/415.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-common-grams-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/common-grams-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">common_words</code>
</span>
</dt>
<dd>
<p>(Required*, array of strings)
A list of tokens. The filter generates bigrams for these tokens.</p>
<p>Either this or the <code class="literal">common_words_path</code> parameter is required.</p>
</dd>
<dt>
<span class="term">
<code class="literal">common_words_path</code>
</span>
</dt>
<dd>
<p>(Required*, string)
Path to a file containing a list of tokens. The filter generates bigrams for
these tokens.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location. The file must
be UTF-8 encoded. Each token in the file must be separated by a line break.</p>
<p>Either this or the <code class="literal">common_words</code> parameter is required.</p>
</dd>
<dt>
<span class="term">
<code class="literal">ignore_case</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, matches for common words matching are case-insensitive.
Defaults to <code class="literal">false</code>.
</dd>
<dt>
<span class="term">
<code class="literal">query_mode</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the filter excludes the following tokens from the output:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Unigrams for common words
</li>
<li class="listitem">
Unigrams for terms followed by common words
</li>
</ul>
</div>
<p>Defaults to <code class="literal">false</code>. We recommend enabling this parameter for
<a class="xref" href="mapping-params.html#search-analyzer" title="search_analyzer">search analyzers</a>.</p>
<p>For example, you can enable this parameter and specify <code class="literal">is</code> and <code class="literal">the</code> as
common words. This filter converts the tokens <code class="literal">[the, quick, fox, is, brown]</code> to
<code class="literal">[the_quick, quick, fox_is, is_brown,]</code>.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-common-grams-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/common-grams-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">common_grams</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">common_grams</code> filter with
<code class="literal">ignore_case</code> and <code class="literal">query_mode</code> set to <code class="literal">true</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /common_grams_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "index_grams": {
          "tokenizer": "whitespace",
          "filter": [ "common_grams_query" ]
        }
      },
      "filter": {
        "common_grams_query": {
          "type": "common_grams",
          "common_words": [ "a", "is", "the" ],
          "ignore_case": true,
          "query_mode": true
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/416.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-condition-tokenfilter"></a>Conditional token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/condition-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Applies a set of token filters to tokens that match conditions in a provided
predicate script.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/ConditionalTokenFilter.html" class="ulink" target="_top">ConditionalTokenFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-condition-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/condition-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">condition</code>
filter to match tokens with fewer than 5 characters in <code class="literal">THE QUICK BROWN FOX</code>.
It then applies the <a class="xref" href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter" title="Lowercase token filter"><code class="literal">lowercase</code></a> filter to
those matching tokens, converting them to lowercase.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "condition",
      "filter": [ "lowercase" ],
      "script": {
        "source": "token.getTerm().length() &lt; 5"
      }
    }
  ],
  "text": "THE QUICK BROWN FOX"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/417.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, QUICK, BROWN, fox ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-condition-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/condition-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">filter</code>
</span>
</dt>
<dd>
<p>(Required, array of token filters)
Array of token filters. If a token matches the predicate script in the <code class="literal">script</code>
parameter, these filters are applied to the token in the order provided.</p>
<p>These filters can include custom token filters defined in the index mapping.</p>
</dd>
<dt>
<span class="term">
<code class="literal">script</code>
</span>
</dt>
<dd>
<p>(Required, <a class="xref" href="modules-scripting-using.html" title="How to use scripts">script object</a>)
Predicate script used to apply token filters. If a token
matches this script, the filters in the <code class="literal">filter</code> parameter are applied to the
token.</p>
<p>For valid parameters, see <a class="xref" href="modules-scripting-using.html#_script_parameters" title="Script parameters">Script parameters</a>. Only inline scripts are
supported. Painless scripts are executed in the
<a href="https://www.elastic.co/guide/en/elasticsearch/painless/7.10/painless-analysis-predicate-context.html" class="ulink" target="_top">analysis predicate context</a>
and require a <code class="literal">token</code> property.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-condition-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/condition-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">condition</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses a custom <code class="literal">condition</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>. The custom <code class="literal">condition</code> filter
matches the first token in a stream. It then reverses that matching token using
the <a class="xref" href="analysis-tokenfilters.html#analysis-reverse-tokenfilter" title="Reverse token filter"><code class="literal">reverse</code></a> filter.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /palindrome_list
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_reverse_first_token": {
          "tokenizer": "whitespace",
          "filter": [ "reverse_first_token" ]
        }
      },
      "filter": {
        "reverse_first_token": {
          "type": "condition",
          "filter": [ "reverse" ],
          "script": {
            "source": "token.getPosition() === 0"
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/418.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-decimal-digit-tokenfilter"></a>Decimal digit token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/decimal-digit-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Converts all digits in the Unicode <code class="literal">Decimal_Number</code> General Category to <code class="literal">0-9</code>.
For example, the filter changes the Bengali numeral <code class="literal">৩</code> to <code class="literal">3</code>.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/core/DecimalDigitFilter.html" class="ulink" target="_top">DecimalDigitFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-decimal-digit-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/decimal-digit-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">decimal_digit</code>
filter to convert Devanagari numerals to <code class="literal">0-9</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["decimal_digit"],
  "text" : "१-one two-२ ३"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/419.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ 1-one, two-2, 3]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-decimal-digit-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/decimal-digit-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">decimal_digit</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /decimal_digit_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_decimal_digit": {
          "tokenizer": "whitespace",
          "filter": [ "decimal_digit" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/420.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-delimited-payload-tokenfilter"></a>Delimited payload token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/delimited-payload-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>The older name <code class="literal">delimited_payload_filter</code> is deprecated and should not be used
with new indices. Use <code class="literal">delimited_payload</code> instead.</p>
</div>
</div>
<p>Separates a token stream into tokens and payloads based on a specified
delimiter.</p>
<p>For example, you can use the <code class="literal">delimited_payload</code> filter with a <code class="literal">|</code> delimiter to
split <code class="literal">the|1 quick|2 fox|3</code> into the tokens <code class="literal">the</code>, <code class="literal">quick</code>, and <code class="literal">fox</code>
with respective payloads of <code class="literal">1</code>, <code class="literal">2</code>, and <code class="literal">3</code>.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilter.html" class="ulink" target="_top">DelimitedPayloadTokenFilter</a>.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<h3>Payloads</h3>
<p>A payload is user-defined binary data associated with a token position and
stored as base64-encoded bytes.</p>
<p>Elasticsearch does not store token payloads by default. To store payloads, you must:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Set the <a class="xref" href="mapping-params.html#term-vector" title="term_vector"><code class="literal">term_vector</code></a> mapping parameter to
<code class="literal">with_positions_payloads</code> or <code class="literal">with_positions_offsets_payloads</code> for any field
storing payloads.
</li>
<li class="listitem">
Use an index analyzer that includes the <code class="literal">delimited_payload</code> filter
</li>
</ul>
</div>
<p>You can view stored payloads using the <a class="xref" href="docs.html#docs-termvectors" title="Term vectors API">term vectors API</a>.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-delimited-payload-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/delimited-payload-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<code class="literal">delimited_payload</code> filter with the default <code class="literal">|</code> delimiter to split
<code class="literal">the|0 brown|10 fox|5 is|0 quick|10</code> into tokens and payloads.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["delimited_payload"],
  "text": "the|0 brown|10 fox|5 is|0 quick|10"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/421.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, brown, fox, is, quick ]</pre>
</div>
<p>Note that the analyze API does not return stored payloads. For an example that
includes returned payloads, see
<a class="xref" href="analysis-tokenfilters.html#analysis-delimited-payload-tokenfilter-return-stored-payloads" title="Return stored payloads">Return stored payloads</a>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-delimited-payload-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/delimited-payload-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">delimited-payload</code> filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom
analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT delimited_payload
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_delimited_payload": {
          "tokenizer": "whitespace",
          "filter": [ "delimited_payload" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/422.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-delimited-payload-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/delimited-payload-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">delimiter</code>
</span>
</dt>
<dd>
(Optional, string)
Character used to separate tokens from payloads. Defaults to <code class="literal">|</code>.
</dd>
<dt>
<span class="term">
<code class="literal">encoding</code>
</span>
</dt>
<dd>
<p>(Optional, string)
Data type for the stored payload. Valid values are:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">float</code>
</span>
</dt>
<dd>
(Default) Float
</dd>
<dt>
<span class="term">
<code class="literal">identity</code>
</span>
</dt>
<dd>
Characters
</dd>
<dt>
<span class="term">
<code class="literal">int</code>
</span>
</dt>
<dd>
Integer
</dd>
</dl>
</div>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-delimited-payload-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/delimited-payload-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">delimited_payload</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses a custom <code class="literal">delimited_payload</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>. The custom <code class="literal">delimited_payload</code>
filter uses the <code class="literal">+</code> delimiter to separate tokens from payloads. Payloads are
encoded as integers.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT delimited_payload_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_plus_delimited": {
          "tokenizer": "whitespace",
          "filter": [ "plus_delimited" ]
        }
      },
      "filter": {
        "plus_delimited": {
          "type": "delimited_payload",
          "delimiter": "+",
          "encoding": "int"
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/423.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-delimited-payload-tokenfilter-return-stored-payloads"></a>Return stored payloads<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/delimited-payload-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>Use the <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> to create an index that:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Includes a field that stores term vectors with payloads.
</li>
<li class="listitem">
Uses a <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom index analyzer</a> with the
<code class="literal">delimited_payload</code> filter.
</li>
</ul>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT text_payloads
{
  "mappings": {
    "properties": {
      "text": {
        "type": "text",
        "term_vector": "with_positions_payloads",
        "analyzer": "payload_delimiter"
      }
    }
  },
  "settings": {
    "analysis": {
      "analyzer": {
        "payload_delimiter": {
          "tokenizer": "whitespace",
          "filter": [ "delimited_payload" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/424.console"></div>
<p>Add a document containing payloads to the index.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST text_payloads/_doc/1
{
  "text": "the|0 brown|3 fox|4 is|0 quick|10"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/425.console"></div>
<p>Use the <a class="xref" href="docs.html#docs-termvectors" title="Term vectors API">term vectors API</a> to return the document&#8217;s tokens
and base64-encoded payloads.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET text_payloads/_termvectors/1
{
  "fields": [ "text" ],
  "payloads": true
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/426.console"></div>
<p>The API returns the following response:</p>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "_index": "text_payloads",
  "_type": "_doc",
  "_id": "1",
  "_version": 1,
  "found": true,
  "took": 8,
  "term_vectors": {
    "text": {
      "field_statistics": {
        "sum_doc_freq": 5,
        "doc_count": 1,
        "sum_ttf": 5
      },
      "terms": {
        "brown": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 1,
              "payload": "QEAAAA=="
            }
          ]
        },
        "fox": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 2,
              "payload": "QIAAAA=="
            }
          ]
        },
        "is": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 3,
              "payload": "AAAAAA=="
            }
          ]
        },
        "quick": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 4,
              "payload": "QSAAAA=="
            }
          ]
        },
        "the": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 0,
              "payload": "AAAAAA=="
            }
          ]
        }
      }
    }
  }
}</pre>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-dict-decomp-tokenfilter"></a>Dictionary decompounder token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/dictionary-decompounder-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>In most cases, we recommend using the faster
<a class="xref" href="analysis-tokenfilters.html#analysis-hyp-decomp-tokenfilter" title="Hyphenation decompounder token filter"><code class="literal">hyphenation_decompounder</code></a> token filter
in place of this filter. However, you can use the
<code class="literal">dictionary_decompounder</code> filter to check the quality of a word list before
implementing it in the <code class="literal">hyphenation_decompounder</code> filter.</p>
</div>
</div>
<p>Uses a specified list of words and a brute force approach to find subwords in
compound words. If found, these subwords are included in the token output.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.html" class="ulink" target="_top">DictionaryCompoundWordTokenFilter</a>,
which was built for Germanic languages.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-dict-decomp-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/dictionary-decompounder-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<code class="literal">dictionary_decompounder</code> filter to find subwords in <code class="literal">Donaudampfschiff</code>. The
filter then checks these subwords against the specified list of words: <code class="literal">Donau</code>,
<code class="literal">dampf</code>, <code class="literal">meer</code>, and <code class="literal">schiff</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "dictionary_decompounder",
      "word_list": ["Donau", "dampf", "meer", "schiff"]
    }
  ],
  "text": "Donaudampfschiff"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/427.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Donaudampfschiff, Donau, dampf, schiff ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-dict-decomp-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/dictionary-decompounder-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">word_list</code>
</span>
</dt>
<dd>
<p>(Required*, array of strings)
A list of subwords to look for in the token stream. If found, the subword is
included in the token output.</p>
<p>Either this parameter or <code class="literal">word_list_path</code> must be specified.</p>
</dd>
<dt>
<span class="term">
<code class="literal">word_list_path</code>
</span>
</dt>
<dd>
<p>(Required*, string)
Path to a file that contains a list of subwords to find in the token stream. If
found, the subword is included in the token output.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location, and the file
must be UTF-8 encoded. Each token in the file must be separated by a line break.</p>
<p>Either this parameter or <code class="literal">word_list</code> must be specified.</p>
</dd>
<dt>
<span class="term">
<code class="literal">max_subword_size</code>
</span>
</dt>
<dd>
(Optional, integer)
Maximum subword character length. Longer subword tokens are excluded from the
output. Defaults to <code class="literal">15</code>.
</dd>
<dt>
<span class="term">
<code class="literal">min_subword_size</code>
</span>
</dt>
<dd>
(Optional, integer)
Minimum subword character length. Shorter subword tokens are excluded from the
output. Defaults to <code class="literal">2</code>.
</dd>
<dt>
<span class="term">
<code class="literal">min_word_size</code>
</span>
</dt>
<dd>
(Optional, integer)
Minimum word character length. Shorter word tokens are excluded from the
output. Defaults to <code class="literal">5</code>.
</dd>
<dt>
<span class="term">
<code class="literal">only_longest_match</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, only include the longest matching subword. Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-dict-decomp-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/dictionary-decompounder-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">dictionary_decompounder</code> filter, duplicate it to create the
basis for a new custom token filter. You can modify the filter using its
configurable parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses a custom <code class="literal">dictionary_decompounder</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<p>The custom <code class="literal">dictionary_decompounder</code> filter find subwords in the
<code class="literal">analysis/example_word_list.txt</code> file. Subwords longer than 22 characters are
excluded from the token output.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT dictionary_decompound_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_dictionary_decompound": {
          "tokenizer": "standard",
          "filter": [ "22_char_dictionary_decompound" ]
        }
      },
      "filter": {
        "22_char_dictionary_decompound": {
          "type": "dictionary_decompounder",
          "word_list_path": "analysis/example_word_list.txt",
          "max_subword_size": 22
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/428.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-edgengram-tokenfilter"></a>Edge n-gram token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/edgengram-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Forms an <a href="https://en.wikipedia.org/wiki/N-gram" class="ulink" target="_top">n-gram</a> of a specified length from
the beginning of a token.</p>
<p>For example, you can use the <code class="literal">edge_ngram</code> token filter to change <code class="literal">quick</code> to
<code class="literal">qu</code>.</p>
<p>When not customized, the filter creates 1-character edge n-grams by default.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html" class="ulink" target="_top">EdgeNGramTokenFilter</a>.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The <code class="literal">edge_ngram</code> filter is similar to the <a class="xref" href="analysis-tokenizers.html#analysis-ngram-tokenizer" title="N-gram tokenizer"><code class="literal">ngram</code>
token filter</a>. However, the <code class="literal">edge_ngram</code> only outputs n-grams that start at the
beginning of a token. These edge n-grams are useful for
<a class="xref" href="mapping-types.html#search-as-you-type" title="Search-as-you-type field type">search-as-you-type</a> queries.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-edgengram-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/edgengram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">edge_ngram</code>
filter to convert <code class="literal">the quick brown fox jumps</code> to 1-character and 2-character
edge n-grams:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    { "type": "edge_ngram",
      "min_gram": 1,
      "max_gram": 2
    }
  ],
  "text": "the quick brown fox jumps"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/429.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ t, th, q, qu, b, br, f, fo, j, ju ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-edgengram-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/edgengram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">edge_ngram</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT edge_ngram_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_edge_ngram": {
          "tokenizer": "standard",
          "filter": [ "edge_ngram" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/430.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-edgengram-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/edgengram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">max_gram</code>
</span>
</dt>
<dd>
<p>(Optional, integer)
Maximum character length of a gram. For custom token filters, defaults to <code class="literal">2</code>.
For the built-in <code class="literal">edge_ngram</code> filter, defaults to <code class="literal">1</code>.</p>
<p>See <a class="xref" href="analysis-tokenfilters.html#analysis-edgengram-tokenfilter-max-gram-limits" title="Limitations of the max_gram parameter">Limitations of the <code class="literal">max_gram</code> parameter</a>.</p>
</dd>
<dt>
<span class="term">
<code class="literal">min_gram</code>
</span>
</dt>
<dd>
(Optional, integer)
Minimum character length of a gram. Defaults to <code class="literal">1</code>.
</dd>
<dt>
<span class="term">
<code class="literal">preserve_original</code>
</span>
</dt>
<dd>
(Optional, Boolean)
Emits original token when set to <code class="literal">true</code>. Defaults to <code class="literal">false</code>.
</dd>
<dt>
<span class="term">
<code class="literal">side</code>
</span>
</dt>
<dd>
<p>(Optional, string)
Deprecated. Indicates whether to truncate tokens from the <code class="literal">front</code> or <code class="literal">back</code>.
Defaults to <code class="literal">front</code>.</p>
<p>Instead of using the <code class="literal">back</code> value, you can use the
<a class="xref" href="analysis-tokenfilters.html#analysis-reverse-tokenfilter" title="Reverse token filter"><code class="literal">reverse</code></a> token filter before and after the
<code class="literal">edge_ngram</code> filter to achieve the same results.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-edgengram-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/edgengram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">edge_ngram</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">edge_ngram</code>
filter that forms n-grams between 3-5 characters.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT edge_ngram_custom_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "3_5_edgegrams" ]
        }
      },
      "filter": {
        "3_5_edgegrams": {
          "type": "edge_ngram",
          "min_gram": 3,
          "max_gram": 5
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/431.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-edgengram-tokenfilter-max-gram-limits"></a>Limitations of the <code class="literal">max_gram</code> parameter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/edgengram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The <code class="literal">edge_ngram</code> filter&#8217;s <code class="literal">max_gram</code> value limits the character length of
tokens. When the <code class="literal">edge_ngram</code> filter is used with an index analyzer, this
means search terms longer than the <code class="literal">max_gram</code> length may not match any indexed
terms.</p>
<p>For example, if the <code class="literal">max_gram</code> is <code class="literal">3</code>, searches for <code class="literal">apple</code> won&#8217;t match the
indexed term <code class="literal">app</code>.</p>
<p>To account for this, you can use the
<a class="xref" href="analysis-tokenfilters.html#analysis-truncate-tokenfilter" title="Truncate token filter"><code class="literal">truncate</code></a> filter with a search analyzer
to shorten search terms to the <code class="literal">max_gram</code> character length. However, this could
return irrelevant results.</p>
<p>For example, if the <code class="literal">max_gram</code> is <code class="literal">3</code> and search terms are truncated to three
characters, the search term <code class="literal">apple</code> is shortened to <code class="literal">app</code>. This means searches
for <code class="literal">apple</code> return any indexed terms matching <code class="literal">app</code>, such as <code class="literal">apply</code>, <code class="literal">snapped</code>,
and <code class="literal">apple</code>.</p>
<p>We recommend testing both approaches to see which best fits your
use case and desired search experience.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-elision-tokenfilter"></a>Elision token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/elision-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Removes specified <a href="https://en.wikipedia.org/wiki/Elision" class="ulink" target="_top">elisions</a> from
the beginning of tokens. For example, you can use this filter to change
<code class="literal">l'avion</code> to <code class="literal">avion</code>.</p>
<p>When not customized, the filter removes the following French elisions by default:</p>
<p><code class="literal">l'</code>, <code class="literal">m'</code>, <code class="literal">t'</code>, <code class="literal">qu'</code>, <code class="literal">n'</code>, <code class="literal">s'</code>, <code class="literal">j'</code>, <code class="literal">d'</code>, <code class="literal">c'</code>, <code class="literal">jusqu'</code>, <code class="literal">quoiqu'</code>,
<code class="literal">lorsqu'</code>, <code class="literal">puisqu'</code></p>
<p>Customized versions of this filter are included in several of Elasticsearch&#8217;s built-in
<a class="xref" href="analysis-analyzers.html#analysis-lang-analyzer" title="Language analyzers">language analyzers</a>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="analysis-analyzers.html#catalan-analyzer" title="catalan analyzer">Catalan analyzer</a>
</li>
<li class="listitem">
<a class="xref" href="analysis-analyzers.html#french-analyzer" title="french analyzer">French analyzer</a>
</li>
<li class="listitem">
<a class="xref" href="analysis-analyzers.html#irish-analyzer" title="irish analyzer">Irish analyzer</a>
</li>
<li class="listitem">
<a class="xref" href="analysis-analyzers.html#italian-analyzer" title="italian analyzer">Italian analyzer</a>
</li>
</ul>
</div>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/util/ElisionFilter.html" class="ulink" target="_top">ElisionFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-elision-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/elision-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">elision</code>
filter to remove <code class="literal">j'</code> from <code class="literal">j’examine près du wharf</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["elision"],
  "text" : "j’examine près du wharf"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/432.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ examine, près, du, wharf ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-elision-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/elision-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">elision</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /elision_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_elision": {
          "tokenizer": "whitespace",
          "filter": [ "elision" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/433.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-elision-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/elision-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<a id="analysis-elision-tokenfilter-articles"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">articles</code>
</span>
</dt>
<dd>
<p>(Required*, array of string)
List of elisions to remove.</p>
<p>To be removed, the elision must be at the beginning of a token and be
immediately followed by an apostrophe. Both the elision and apostrophe are
removed.</p>
<p>For custom <code class="literal">elision</code> filters, either this parameter or <code class="literal">articles_path</code> must be
specified.</p>
</dd>
<dt>
<span class="term">
<code class="literal">articles_path</code>
</span>
</dt>
<dd>
<p>(Required*, string)
Path to a file that contains a list of elisions to remove.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location, and the file
must be UTF-8 encoded. Each elision in the file must be separated by a line
break.</p>
<p>To be removed, the elision must be at the beginning of a token and be
immediately followed by an apostrophe. Both the elision and apostrophe are
removed.</p>
<p>For custom <code class="literal">elision</code> filters, either this parameter or <code class="literal">articles</code> must be
specified.</p>
</dd>
<dt>
<span class="term">
<code class="literal">articles_case</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, elision matching is case insensitive. If <code class="literal">false</code>, elision matching is
case sensitive. Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-elision-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/elision-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">elision</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom case-insensitive <code class="literal">elision</code>
filter that removes the <code class="literal">l'</code>, <code class="literal">m'</code>, <code class="literal">t'</code>, <code class="literal">qu'</code>, <code class="literal">n'</code>, <code class="literal">s'</code>,
and <code class="literal">j'</code> elisions:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /elision_case_insensitive_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "elision_case_insensitive" ]
        }
      },
      "filter": {
        "elision_case_insensitive": {
          "type": "elision",
          "articles": [ "l", "m", "t", "qu", "n", "s", "j" ],
          "articles_case": true
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/434.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-fingerprint-tokenfilter"></a>Fingerprint token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/fingerprint-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Sorts and removes duplicate tokens from a token stream, then concatenates the
stream into a single output token.</p>
<p>For example, this filter changes the <code class="literal">[ the, fox, was, very, very, quick ]</code>
token stream as follows:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Sorts the tokens alphabetically to <code class="literal">[ fox, quick, the, very, very, was ]</code>
</li>
<li class="listitem">
Removes a duplicate instance of the <code class="literal">very</code> token.
</li>
<li class="listitem">
Concatenates the token stream to a output single token: <code class="literal">[fox quick the very was ]</code>
</li>
</ol>
</div>
<p>Output tokens produced by this filter are useful for
fingerprinting and clustering a body of text as described in the
<a href="https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth#fingerprint" class="ulink" target="_top">OpenRefine
project</a>.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/FingerprintFilter.html" class="ulink" target="_top">FingerprintFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-fingerprint-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/fingerprint-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">fingerprint</code>
filter to create a single output token for the text <code class="literal">zebra jumps over resting
resting dog</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["fingerprint"],
  "text" : "zebra jumps over resting resting dog"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/435.console"></div>
<p>The filter produces the following token:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ dog jumps over resting zebra ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-fingerprint-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/fingerprint-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">fingerprint</code> filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom
analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT fingerprint_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_fingerprint": {
          "tokenizer": "whitespace",
          "filter": [ "fingerprint" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/436.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-fingerprint-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/fingerprint-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<a id="analysis-fingerprint-tokenfilter-max-size"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">max_output_size</code>
</span>
</dt>
<dd>
(Optional, integer)
Maximum character length, including whitespace, of the output token. Defaults to
<code class="literal">255</code>. Concatenated tokens longer than this will result in no token output.
</dd>
<dt>
<span class="term">
<code class="literal">separator</code>
</span>
</dt>
<dd>
(Optional, string)
Character to use to concatenate the token stream input. Defaults to a space.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-fingerprint-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/fingerprint-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">fingerprint</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">fingerprint</code> filter with
that use <code class="literal">+</code> to concatenate token streams. The filter also limits
output tokens to <code class="literal">100</code> characters or fewer.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT custom_fingerprint_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_": {
          "tokenizer": "whitespace",
          "filter": [ "fingerprint_plus_concat" ]
        }
      },
      "filter": {
        "fingerprint_plus_concat": {
          "type": "fingerprint",
          "max_output_size": 100,
          "separator": "+"
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/437.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-flatten-graph-tokenfilter"></a>Flatten graph token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/flatten-graph-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Flattens a <a class="xref" href="analysis-concepts.html#token-graphs" title="Token graphs">token graph</a> produced by a graph token filter, such
as <a class="xref" href="analysis-tokenfilters.html#analysis-synonym-graph-tokenfilter" title="Synonym graph token filter"><code class="literal">synonym_graph</code></a> or
<a class="xref" href="analysis-tokenfilters.html#analysis-word-delimiter-graph-tokenfilter" title="Word delimiter graph token filter"><code class="literal">word_delimiter_graph</code></a>.</p>
<p>Flattening a token graph containing
<a class="xref" href="analysis-concepts.html#token-graphs-multi-position-tokens" title="Multi-position tokens">multi-position tokens</a> makes the graph
suitable for <a class="xref" href="analysis-concepts.html#analysis-index-search-time" title="Index and search analysis">indexing</a>. Otherwise, indexing does
not support token graphs containing multi-position tokens.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Flattening graphs is a lossy process.</p>
<p>If possible, avoid using the <code class="literal">flatten_graph</code> filter. Instead, use graph token
filters in <a class="xref" href="analysis-concepts.html#analysis-index-search-time" title="Index and search analysis">search analyzers</a> only. This eliminates
the need for the <code class="literal">flatten_graph</code> filter.</p>
</div>
</div>
<p>The <code class="literal">flatten_graph</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/core/FlattenGraphFilter.html" class="ulink" target="_top">FlattenGraphFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-flatten-graph-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/flatten-graph-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To see how the <code class="literal">flatten_graph</code> filter works, you first need to produce a token
graph containing multi-position tokens.</p>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">synonym_graph</code>
filter to add <code class="literal">dns</code> as a multi-position synonym for <code class="literal">domain name system</code> in the
text <code class="literal">domain name system is fragile</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "synonym_graph",
      "synonyms": [ "dns, domain name system" ]
    }
  ],
  "text": "domain name system is fragile"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/438.console"></div>
<p>The filter produces the following token graph with <code class="literal">dns</code> as a multi-position
token.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-dns-synonym-ex.svg" alt="token graph dns synonym ex">
</div>
</div>
<p>Indexing does not support token graphs containing multi-position tokens. To make
this token graph suitable for indexing, it needs to be flattened.</p>
<p>To flatten the token graph, add the <code class="literal">flatten_graph</code> filter after the
<code class="literal">synonym_graph</code> filter in the previous analyze API request.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "synonym_graph",
      "synonyms": [ "dns, domain name system" ]
    },
    "flatten_graph"
  ],
  "text": "domain name system is fragile"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/439.console"></div>
<p>The filter produces the following flattened token graph, which is suitable for
indexing.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-dns-invalid-ex.svg" alt="token graph dns invalid ex">
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keyword-marker-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/flatten-graph-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">flatten_graph</code> token filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<p>In this analyzer, a custom <code class="literal">word_delimiter_graph</code> filter produces token graphs
containing catenated, multi-position tokens. The <code class="literal">flatten_graph</code> filter flattens
these token graphs, making them suitable for indexing.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_index_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "my_custom_word_delimiter_graph_filter",
            "flatten_graph"
          ]
        }
      },
      "filter": {
        "my_custom_word_delimiter_graph_filter": {
          "type": "word_delimiter_graph",
          "catenate_all": true
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/440.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-hunspell-tokenfilter"></a>Hunspell token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hunspell-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Provides <a class="xref" href="analysis-concepts.html#dictionary-stemmers" title="Dictionary stemmers">dictionary stemming</a> based on a provided
<a href="https://en.wikipedia.org/wiki/Hunspell" class="ulink" target="_top">Hunspell dictionary</a>. The <code class="literal">hunspell</code>
filter requires
<a class="xref" href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter-dictionary-config" title="Configure Hunspell dictionaries">configuration</a> of one or more
language-specific Hunspell dictionaries.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/hunspell/HunspellStemFilter.html" class="ulink" target="_top">HunspellStemFilter</a>.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>If available, we recommend trying an algorithmic stemmer for your language
before using the <a class="xref" href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter" title="Hunspell token filter"><code class="literal">hunspell</code></a> token filter.
In practice, algorithmic stemmers typically outperform dictionary stemmers.
See <a class="xref" href="analysis-concepts.html#dictionary-stemmers" title="Dictionary stemmers">Dictionary stemmers</a>.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-hunspell-tokenfilter-dictionary-config"></a>Configure Hunspell dictionaries<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hunspell-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>Hunspell dictionaries are stored and detected on a dedicated
<code class="literal">hunspell</code> directory on the filesystem: <code class="literal">&lt;$ES_PATH_CONF&gt;/hunspell</code>. Each dictionary
is expected to have its own directory, named after its associated language and
locale (e.g., <code class="literal">pt_BR</code>, <code class="literal">en_GB</code>). This dictionary directory is expected to hold a
single <code class="literal">.aff</code> and one or more <code class="literal">.dic</code> files, all of which will automatically be
picked up. For example, the following directory layout will define the <code class="literal">en_US</code> dictionary:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">- config
    |-- hunspell
    |    |-- en_US
    |    |    |-- en_US.dic
    |    |    |-- en_US.aff</pre>
</div>
<p>Each dictionary can be configured with one setting:</p>
<div class="variablelist">
<a id="analysis-hunspell-ignore-case-settings"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">ignore_case</code>
</span>
</dt>
<dd>
<p>
(Static, Boolean)
If true, dictionary matching will be case insensitive. Defaults to <code class="literal">false</code>.
</p>
<p>This setting can be configured globally in <code class="literal">elasticsearch.yml</code> using
<code class="literal">indices.analysis.hunspell.dictionary.ignore_case</code>.</p>
<p>To configure the setting for a specific locale, use the
<code class="literal">indices.analysis.hunspell.dictionary.&lt;locale&gt;.ignore_case</code> setting (e.g., for
the <code class="literal">en_US</code> (American English) locale, the setting is
<code class="literal">indices.analysis.hunspell.dictionary.en_US.ignore_case</code>).</p>
<p>You can also add a <code class="literal">settings.yml</code> file under the dictionary
directory which holds these settings. This overrides any other <code class="literal">ignore_case</code>
settings defined in <code class="literal">elasticsearch.yml</code>.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-hunspell-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hunspell-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following analyze API request uses the <code class="literal">hunspell</code> filter to stem
<code class="literal">the foxes jumping quickly</code> to <code class="literal">the fox jump quick</code>.</p>
<p>The request specifies the <code class="literal">en_US</code> locale, meaning that the
<code class="literal">.aff</code> and <code class="literal">.dic</code> files in the <code class="literal">&lt;$ES_PATH_CONF&gt;/hunspell/en_US</code> directory are used
for the Hunspell dictionary.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "hunspell",
      "locale": "en_US"
    }
  ],
  "text": "the foxes jumping quickly"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/441.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, fox, jump, quick ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-hunspell-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hunspell-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<a id="analysis-hunspell-tokenfilter-dictionary-param"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">dictionary</code>
</span>
</dt>
<dd>
<p>
(Optional, string or array of strings)
One or more <code class="literal">.dic</code> files (e.g, <code class="literal">en_US.dic, my_custom.dic</code>) to use for the
Hunspell dictionary.
</p>
<p>By default, the <code class="literal">hunspell</code> filter uses all <code class="literal">.dic</code> files in the
<code class="literal">&lt;$ES_PATH_CONF&gt;/hunspell/&lt;locale&gt;</code> directory specified using the
<code class="literal">lang</code>, <code class="literal">language</code>, or <code class="literal">locale</code> parameter.</p>
</dd>
<dt>
<span class="term">
<code class="literal">dedup</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, duplicate tokens are removed from the filter&#8217;s output. Defaults to
<code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">lang</code>
</span>
</dt>
<dd>
<p>
(Required*, string)
An alias for the <a class="xref" href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter-locale-param"><code class="literal">locale</code>
parameter</a>.
</p>
<p>If this parameter is not specified, the <code class="literal">language</code> or <code class="literal">locale</code> parameter is
required.</p>
</dd>
<dt>
<span class="term">
<code class="literal">language</code>
</span>
</dt>
<dd>
<p>
(Required*, string)
An alias for the <a class="xref" href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter-locale-param"><code class="literal">locale</code>
parameter</a>.
</p>
<p>If this parameter is not specified, the <code class="literal">lang</code> or <code class="literal">locale</code> parameter is
required.</p>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="analysis-hunspell-tokenfilter-locale-param"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">locale</code>
</span>
</dt>
<dd>
<p>
(Required*, string)
Locale directory used to specify the <code class="literal">.aff</code> and <code class="literal">.dic</code> files for a Hunspell
dictionary. See <a class="xref" href="analysis-tokenfilters.html#analysis-hunspell-tokenfilter-dictionary-config" title="Configure Hunspell dictionaries">Configure Hunspell dictionaries</a>.
</p>
<p>If this parameter is not specified, the <code class="literal">lang</code> or <code class="literal">language</code> parameter is
required.</p>
</dd>
<dt>
<span class="term">
<code class="literal">longest_only</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, only the longest stemmed version of each token is
included in the output. If <code class="literal">false</code>, all stemmed versions of the token are
included. Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-hunspell-tokenfilter-analyzer-ex"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hunspell-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">hunspell</code> filter, duplicate it to create the
basis for a new custom token filter. You can modify the filter using its
configurable parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses a custom <code class="literal">hunspell</code> filter, <code class="literal">my_en_US_dict_stemmer</code>, to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<p>The <code class="literal">my_en_US_dict_stemmer</code> filter uses a <code class="literal">locale</code> of <code class="literal">en_US</code>, meaning that the
<code class="literal">.aff</code> and <code class="literal">.dic</code> files in the <code class="literal">&lt;$ES_PATH_CONF&gt;/hunspell/en_US</code> directory are
used. The filter also includes a <code class="literal">dedup</code> argument of <code class="literal">false</code>, meaning that
duplicate tokens added from the dictionary are not removed from the filter&#8217;s
output.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "en": {
          "tokenizer": "standard",
          "filter": [ "my_en_US_dict_stemmer" ]
        }
      },
      "filter": {
        "my_en_US_dict_stemmer": {
          "type": "hunspell",
          "locale": "en_US",
          "dedup": false
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/442.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-hunspell-tokenfilter-settings"></a>Settings<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hunspell-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>In addition to the <a class="xref" href="analysis-tokenfilters.html#analysis-hunspell-ignore-case-settings"><code class="literal">ignore_case</code>
settings</a>, you can configure the following global settings for the <code class="literal">hunspell</code>
filter using <code class="literal">elasticsearch.yml</code>:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">indices.analysis.hunspell.dictionary.lazy</code>
</span>
</dt>
<dd>
(Static, Boolean)
If <code class="literal">true</code>, the loading of Hunspell dictionaries is deferred until a dictionary
is used. If <code class="literal">false</code>, the dictionary directory is checked for dictionaries when
the node starts, and any dictionaries are automatically loaded. Defaults to
<code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-hyp-decomp-tokenfilter"></a>Hyphenation decompounder token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hyphenation-decompounder-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Uses XML-based hyphenation patterns to find potential subwords in compound
words. These subwords are then checked against the specified word list. Subwords not
in the list are excluded from the token output.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilter.html" class="ulink" target="_top">HyphenationCompoundWordTokenFilter</a>,
which was built for Germanic languages.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-hyp-decomp-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hyphenation-decompounder-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<code class="literal">hyphenation_decompounder</code> filter to find subwords in <code class="literal">Kaffeetasse</code> based on
German hyphenation patterns in the <code class="literal">analysis/hyphenation_patterns.xml</code> file. The
filter then checks these subwords against a list of specified words: <code class="literal">kaffee</code>,
<code class="literal">zucker</code>, and <code class="literal">tasse</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "hyphenation_decompounder",
      "hyphenation_patterns_path": "analysis/hyphenation_patterns.xml",
      "word_list": ["Kaffee", "zucker", "tasse"]
    }
  ],
  "text": "Kaffeetasse"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/443.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Kaffeetasse, Kaffee, tasse ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-hyp-decomp-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hyphenation-decompounder-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">hyphenation_patterns_path</code>
</span>
</dt>
<dd>
<p>(Required, string)
Path to an Apache FOP (Formatting Objects Processor) XML hyphenation pattern file.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location. Only FOP v1.2
compatible files are supported.</p>
<p>For example FOP XML hyphenation pattern files, refer to:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a href="http://offo.sourceforge.net/#FOP+XML+Hyphenation+Patterns" class="ulink" target="_top">Objects For Formatting Objects (OFFO) Sourceforge project</a>
</li>
<li class="listitem">
<a href="https://sourceforge.net/projects/offo/files/offo-hyphenation/1.2/offo-hyphenation_v1.2.zip/download" class="ulink" target="_top">offo-hyphenation_v1.2.zip direct download</a> (v2.0 and above hyphenation pattern files are not supported)
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">word_list</code>
</span>
</dt>
<dd>
<p>(Required*, array of strings)
A list of subwords. Subwords found using the hyphenation pattern but not in this
list are excluded from the token output.</p>
<p>You can use the <a class="xref" href="analysis-tokenfilters.html#analysis-dict-decomp-tokenfilter" title="Dictionary decompounder token filter"><code class="literal">dictionary_decompounder</code></a>
filter to test the quality of word lists before implementing them.</p>
<p>Either this parameter or <code class="literal">word_list_path</code> must be specified.</p>
</dd>
<dt>
<span class="term">
<code class="literal">word_list_path</code>
</span>
</dt>
<dd>
<p>(Required*, string)
Path to a file containing a list of subwords. Subwords found using the
hyphenation pattern but not in this list are excluded from the token output.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location, and the file
must be UTF-8 encoded. Each token in the file must be separated by a line break.</p>
<p>You can use the <a class="xref" href="analysis-tokenfilters.html#analysis-dict-decomp-tokenfilter" title="Dictionary decompounder token filter"><code class="literal">dictionary_decompounder</code></a>
filter to test the quality of word lists before implementing them.</p>
<p>Either this parameter or <code class="literal">word_list</code> must be specified.</p>
</dd>
<dt>
<span class="term">
<code class="literal">max_subword_size</code>
</span>
</dt>
<dd>
(Optional, integer)
Maximum subword character length. Longer subword tokens are excluded from the
output. Defaults to <code class="literal">15</code>.
</dd>
<dt>
<span class="term">
<code class="literal">min_subword_size</code>
</span>
</dt>
<dd>
(Optional, integer)
Minimum subword character length. Shorter subword tokens are excluded from the
output. Defaults to <code class="literal">2</code>.
</dd>
<dt>
<span class="term">
<code class="literal">min_word_size</code>
</span>
</dt>
<dd>
(Optional, integer)
Minimum word character length. Shorter word tokens are excluded from the
output. Defaults to <code class="literal">5</code>.
</dd>
<dt>
<span class="term">
<code class="literal">only_longest_match</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, only include the longest matching subword. Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-hyp-decomp-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/hyphenation-decompounder-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">hyphenation_decompounder</code> filter, duplicate it to create the
basis for a new custom token filter. You can modify the filter using its
configurable parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses a custom <code class="literal">hyphenation_decompounder</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<p>The custom <code class="literal">hyphenation_decompounder</code> filter find subwords based on hyphenation
patterns in the <code class="literal">analysis/hyphenation_patterns.xml</code> file. The filter then checks
these subwords against the list of words specified in the
<code class="literal">analysis/example_word_list.txt</code> file. Subwords longer than 22 characters are
excluded from the token output.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT hyphenation_decompound_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_hyphenation_decompound": {
          "tokenizer": "standard",
          "filter": [ "22_char_hyphenation_decompound" ]
        }
      },
      "filter": {
        "22_char_hyphenation_decompound": {
          "type": "hyphenation_decompounder",
          "word_list_path": "analysis/example_word_list.txt",
          "hyphenation_patterns_path": "analysis/hyphenation_patterns.xml",
          "max_subword_size": 22
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/444.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-keep-types-tokenfilter"></a>Keep types token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keep-types-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Keeps or removes tokens of a specific type. For example, you can use this filter
to change <code class="literal">3 quick foxes</code> to <code class="literal">quick foxes</code> by keeping only <code class="literal">&lt;ALPHANUM&gt;</code>
(alphanumeric) tokens.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<h3>Token types</h3>
<p>Token types are set by the <a class="xref" href="analysis-tokenizers.html" title="Tokenizer reference">tokenizer</a> when converting
characters to tokens. Token types can vary between tokenizers.</p>
<p>For example, the <a class="xref" href="analysis-tokenizers.html#analysis-standard-tokenizer" title="Standard tokenizer"><code class="literal">standard</code></a> tokenizer can
produce a variety of token types, including <code class="literal">&lt;ALPHANUM&gt;</code>, <code class="literal">&lt;HANGUL&gt;</code>, and
<code class="literal">&lt;NUM&gt;</code>. Simpler analyzers, like the
<a class="xref" href="analysis-tokenizers.html#analysis-lowercase-tokenizer" title="Lowercase tokenizer"><code class="literal">lowercase</code></a> tokenizer, only produce the <code class="literal">word</code>
token type.</p>
<p>Certain token filters can also add token types. For example, the
<a class="xref" href="analysis-tokenfilters.html#analysis-synonym-tokenfilter" title="Synonym token filter"><code class="literal">synonym</code></a> filter can add the <code class="literal">&lt;SYNONYM&gt;</code> token
type.</p>
</div>
</div>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/core/TypeTokenFilter.html" class="ulink" target="_top">TypeTokenFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keep-types-tokenfilter-analyze-include-ex"></a>Include example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keep-types-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">keep_types</code>
filter to keep only <code class="literal">&lt;NUM&gt;</code> (numeric) tokens from <code class="literal">1 quick fox 2 lazy dogs</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "keep_types",
      "types": [ "&lt;NUM&gt;" ]
    }
  ],
  "text": "1 quick fox 2 lazy dogs"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/445.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ 1, 2 ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keep-types-tokenfilter-analyze-exclude-ex"></a>Exclude example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keep-types-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">keep_types</code>
filter to remove <code class="literal">&lt;NUM&gt;</code> tokens from <code class="literal">1 quick fox 2 lazy dogs</code>. Note the <code class="literal">mode</code>
parameter is set to <code class="literal">exclude</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "keep_types",
      "types": [ "&lt;NUM&gt;" ],
      "mode": "exclude"
    }
  ],
  "text": "1 quick fox 2 lazy dogs"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/446.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ quick, fox, lazy, dogs ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keep-types-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keep-types-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">types</code>
</span>
</dt>
<dd>
(Required, array of strings)
List of token types to keep or remove.
</dd>
<dt>
<span class="term">
<code class="literal">mode</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates whether to keep or remove the specified token types.
Valid values are:
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">include</code>
</span>
</dt>
<dd>
(Default) Keep only the specified token types.
</dd>
<dt>
<span class="term">
<code class="literal">exclude</code>
</span>
</dt>
<dd>
Remove the specified token types.
</dd>
</dl>
</div>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keep-types-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keep-types-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">keep_types</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses a custom <code class="literal">keep_types</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>. The custom <code class="literal">keep_types</code> filter
keeps only <code class="literal">&lt;ALPHANUM&gt;</code> (alphanumeric) tokens.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT keep_types_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "extract_alpha" ]
        }
      },
      "filter": {
        "extract_alpha": {
          "type": "keep_types",
          "types": [ "&lt;ALPHANUM&gt;" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/447.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-keep-words-tokenfilter"></a>Keep words token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keep-words-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Keeps only tokens contained in a specified word list.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/KeepWordFilter.html" class="ulink" target="_top">KeepWordFilter</a>.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>To remove a list of words from a token stream, use the
<a class="xref" href="analysis-tokenfilters.html#analysis-stop-tokenfilter" title="Stop token filter"><code class="literal">stop</code></a> filter.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keep-words-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keep-words-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">keep</code> filter to
keep only the <code class="literal">fox</code> and <code class="literal">dog</code> tokens from
<code class="literal">the quick fox jumps over the lazy dog</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "keep",
      "keep_words": [ "dog", "elephant", "fox" ]
    }
  ],
  "text": "the quick fox jumps over the lazy dog"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/448.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ fox, dog ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keep-words-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keep-words-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">keep_words</code>
</span>
</dt>
<dd>
<p>(Required*, array of strings)
List of words to keep. Only tokens that match words in this list are included in
the output.</p>
<p>Either this parameter or <code class="literal">keep_words_path</code> must be specified.</p>
</dd>
<dt>
<span class="term">
<code class="literal">keep_words_path</code>
</span>
</dt>
<dd>
<p>(Required*, array of strings)
Path to a file that contains a list of words to keep. Only tokens that match
words in this list are included in the output.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location, and the file
must be UTF-8 encoded. Each word in the file must be separated by a line break.</p>
<p>Either this parameter or <code class="literal">keep_words</code> must be specified.</p>
</dd>
<dt>
<span class="term">
<code class="literal">keep_words_case</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, lowercase all keep words. Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keep-words-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keep-words-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">keep</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses custom <code class="literal">keep</code> filters to configure two new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzers</a>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">standard_keep_word_array</code>, which uses a custom <code class="literal">keep</code> filter with an inline
array of keep words
</li>
<li class="listitem">
<code class="literal">standard_keep_word_file</code>, which uses a customer <code class="literal">keep</code> filter with a keep
words file
</li>
</ul>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT keep_words_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_keep_word_array": {
          "tokenizer": "standard",
          "filter": [ "keep_word_array" ]
        },
        "standard_keep_word_file": {
          "tokenizer": "standard",
          "filter": [ "keep_word_file" ]
        }
      },
      "filter": {
        "keep_word_array": {
          "type": "keep",
          "keep_words": [ "one", "two", "three" ]
        },
        "keep_word_file": {
          "type": "keep",
          "keep_words_path": "analysis/example_word_list.txt"
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/449.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-keyword-marker-tokenfilter"></a>Keyword marker token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keyword-marker-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Marks specified tokens as keywords, which are not stemmed.</p>
<p>The <code class="literal">keyword_marker</code> filter assigns specified tokens a <code class="literal">keyword</code> attribute of
<code class="literal">true</code>. Stemmer token filters, such as
<a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter" title="Stemmer token filter"><code class="literal">stemmer</code></a> or
<a class="xref" href="analysis-tokenfilters.html#analysis-porterstem-tokenfilter" title="Porter stem token filter"><code class="literal">porter_stem</code></a>, skip tokens with a <code class="literal">keyword</code>
attribute of <code class="literal">true</code>.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>To work properly, the <code class="literal">keyword_marker</code> filter must be listed before any stemmer
token filters in the <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">analyzer configuration</a>.</p>
</div>
</div>
<p>The <code class="literal">keyword_marker</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/KeywordMarkerFilter.html" class="ulink" target="_top">KeywordMarkerFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keyword-marker-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keyword-marker-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To see how the <code class="literal">keyword_marker</code> filter works, you first need to produce a token
stream containing stemmed tokens.</p>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter" title="Stemmer token filter"><code class="literal">stemmer</code></a> filter to create stemmed tokens for
<code class="literal">fox running and jumping</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [ "stemmer" ],
  "text": "fox running and jumping"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/450.console"></div>
<p>The request produces the following tokens. Note that <code class="literal">running</code> was stemmed to
<code class="literal">run</code> and <code class="literal">jumping</code> was stemmed to <code class="literal">jump</code>.</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ fox, run, and, jump ]</pre>
</div>
<p>To prevent <code class="literal">jumping</code> from being stemmed, add the <code class="literal">keyword_marker</code> filter before
the <code class="literal">stemmer</code> filter in the previous analyze API request. Specify <code class="literal">jumping</code> in
the <code class="literal">keywords</code> parameter of the <code class="literal">keyword_marker</code> filter.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "keyword_marker",
      "keywords": [ "jumping" ]
    },
    "stemmer"
  ],
  "text": "fox running and jumping"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/451.console"></div>
<p>The request produces the following tokens. <code class="literal">running</code> is still stemmed to <code class="literal">run</code>,
but <code class="literal">jumping</code> is not stemmed.</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ fox, run, and, jumping ]</pre>
</div>
<p>To see the <code class="literal">keyword</code> attribute for these tokens, add the following arguments to
the analyze API request:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">explain</code>: <code class="literal">true</code>
</li>
<li class="listitem">
<code class="literal">attributes</code>: <code class="literal">keyword</code>
</li>
</ul>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "keyword_marker",
      "keywords": [ "jumping" ]
    },
    "stemmer"
  ],
  "text": "fox running and jumping",
  "explain": true,
  "attributes": "keyword"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/452.console"></div>
<p>The API returns the following response. Note the <code class="literal">jumping</code> token has a
<code class="literal">keyword</code> attribute of <code class="literal">true</code>.</p>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "detail": {
    "custom_analyzer": true,
    "charfilters": [],
    "tokenizer": {
      "name": "whitespace",
      "tokens": [
        {
          "token": "fox",
          "start_offset": 0,
          "end_offset": 3,
          "type": "word",
          "position": 0
        },
        {
          "token": "running",
          "start_offset": 4,
          "end_offset": 11,
          "type": "word",
          "position": 1
        },
        {
          "token": "and",
          "start_offset": 12,
          "end_offset": 15,
          "type": "word",
          "position": 2
        },
        {
          "token": "jumping",
          "start_offset": 16,
          "end_offset": 23,
          "type": "word",
          "position": 3
        }
      ]
    },
    "tokenfilters": [
      {
        "name": "__anonymous__keyword_marker",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": false
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": false
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          }
        ]
      },
      {
        "name": "stemmer",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": false
          },
          {
            "token": "run",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": false
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          }
        ]
      }
    ]
  }
}</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keyword-marker-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keyword-marker-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">ignore_case</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, matching for the <code class="literal">keywords</code> and <code class="literal">keywords_path</code> parameters ignores
letter case. Defaults to <code class="literal">false</code>.
</dd>
<dt>
<span class="term">
<code class="literal">keywords</code>
</span>
</dt>
<dd>
<p>
(Required*, array of strings)
Array of keywords. Tokens that match these keywords are not stemmed.
</p>
<p>This parameter, <code class="literal">keywords_path</code>, or <code class="literal">keywords_pattern</code> must be specified.
You cannot specify this parameter and <code class="literal">keywords_pattern</code>.</p>
</dd>
<dt>
<span class="term">
<code class="literal">keywords_path</code>
</span>
</dt>
<dd>
<p>(Required*, string)
Path to a file that contains a list of keywords. Tokens that match these
keywords are not stemmed.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location, and the file
must be UTF-8 encoded. Each word in the file must be separated by a line break.</p>
<p>This parameter, <code class="literal">keywords</code>, or <code class="literal">keywords_pattern</code> must be specified.
You cannot specify this parameter and <code class="literal">keywords_pattern</code>.</p>
</dd>
<dt>
<span class="term">
<code class="literal">keywords_pattern</code>
</span>
</dt>
<dd>
<p>(Required*, string)
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html" class="ulink" target="_top">Java
regular expression</a> used to match tokens. Tokens that match this expression are
marked as keywords and not stemmed.</p>
<p>This parameter, <code class="literal">keywords</code>, or <code class="literal">keywords_path</code> must be specified. You
cannot specify this parameter and <code class="literal">keywords</code> or <code class="literal">keywords_pattern</code>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Poorly written regular expressions can cause Elasticsearch to run slowly or result
in stack overflow errors, causing the running node to suddenly exit.</p>
</div>
</div>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keyword-marker-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keyword-marker-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">keyword_marker</code> filter, duplicate it to create the basis for a
new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses a custom <code class="literal">keyword_marker</code> filter and the <code class="literal">porter_stem</code>
filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<p>The custom <code class="literal">keyword_marker</code> filter marks tokens specified in the
<code class="literal">analysis/example_word_list.txt</code> file as keywords. The <code class="literal">porter_stem</code> filter does
not stem these tokens.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "my_custom_keyword_marker_filter",
            "porter_stem"
          ]
        }
      },
      "filter": {
        "my_custom_keyword_marker_filter": {
          "type": "keyword_marker",
          "keywords_path": "analysis/example_word_list.txt"
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/453.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-keyword-repeat-tokenfilter"></a>Keyword repeat token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keyword-repeat-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Outputs a keyword version of each token in a stream. These keyword tokens are
not stemmed.</p>
<p>The <code class="literal">keyword_repeat</code> filter assigns keyword tokens a <code class="literal">keyword</code> attribute of
<code class="literal">true</code>. Stemmer token filters, such as
<a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter" title="Stemmer token filter"><code class="literal">stemmer</code></a> or
<a class="xref" href="analysis-tokenfilters.html#analysis-porterstem-tokenfilter" title="Porter stem token filter"><code class="literal">porter_stem</code></a>, skip tokens with a <code class="literal">keyword</code>
attribute of <code class="literal">true</code>.</p>
<p>You can use the <code class="literal">keyword_repeat</code> filter with a stemmer token filter to output a
stemmed and unstemmed version of each token in a stream.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>To work properly, the <code class="literal">keyword_repeat</code> filter must be listed before any stemmer
token filters in the <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">analyzer configuration</a>.</p>
<p>Stemming does not affect all tokens. This means streams could contain duplicate
tokens in the same position, even after stemming.</p>
<p>To remove these duplicate tokens, add the
<a class="xref" href="analysis-tokenfilters.html#analysis-remove-duplicates-tokenfilter" title="Remove duplicates token filter"><code class="literal">remove_duplicates</code></a> filter after the
stemmer filter in the analyzer configuration.</p>
</div>
</div>
<p>The <code class="literal">keyword_repeat</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/KeywordRepeatFilter.html" class="ulink" target="_top">KeywordRepeatFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keyword-repeat-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keyword-repeat-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">keyword_repeat</code>
filter to output a keyword and non-keyword version of each token in
<code class="literal">fox running and jumping</code>.</p>
<p>To return the <code class="literal">keyword</code> attribute for these tokens, the analyze API request also
includes the following arguments:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">explain</code>:  <code class="literal">true</code>
</li>
<li class="listitem">
<code class="literal">attributes</code>: <code class="literal">keyword</code>
</li>
</ul>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat"
  ],
  "text": "fox running and jumping",
  "explain": true,
  "attributes": "keyword"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/454.console"></div>
<p>The API returns the following response. Note that one version of each token has
a <code class="literal">keyword</code> attribute of <code class="literal">true</code>.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Response</strong></span></summary>
<div class="content">
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "detail": {
    "custom_analyzer": true,
    "charfilters": [],
    "tokenizer": ...,
    "tokenfilters": [
      {
        "name": "keyword_repeat",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": true
          },
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": false
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": true
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": true
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": false
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": false
          }
        ]
      }
    ]
  }
}</pre>
</div>
</div>
</details>
<p>To stem the non-keyword tokens, add the <code class="literal">stemmer</code> filter after the
<code class="literal">keyword_repeat</code> filter in the previous analyze API request.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer"
  ],
  "text": "fox running and jumping",
  "explain": true,
  "attributes": "keyword"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/455.console"></div>
<p>The API returns the following response. Note the following changes:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The non-keyword version of <code class="literal">running</code> was stemmed to <code class="literal">run</code>.
</li>
<li class="listitem">
The non-keyword version of <code class="literal">jumping</code> was stemmed to <code class="literal">jump</code>.
</li>
</ul>
</div>
<details>
<summary class="title"><span class="strong strong"><strong>Response</strong></span></summary>
<div class="content">
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "detail": {
    "custom_analyzer": true,
    "charfilters": [],
    "tokenizer": ...,
    "tokenfilters": [
      {
        "name": "keyword_repeat",
        "tokens": ...
      },
      {
        "name": "stemmer",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": true
          },
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": false
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": true
          },
          {
            "token": "run",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": true
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": false
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          },
          {
            "token": "jump",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": false
          }
        ]
      }
    ]
  }
}</pre>
</div>
</div>
</details>
<p>However, the keyword and non-keyword versions of <code class="literal">fox</code> and <code class="literal">and</code> are
identical and in the same respective positions.</p>
<p>To remove these duplicate tokens, add the <code class="literal">remove_duplicates</code> filter after
<code class="literal">stemmer</code> in the analyze API request.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer",
    "remove_duplicates"
  ],
  "text": "fox running and jumping",
  "explain": true,
  "attributes": "keyword"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/456.console"></div>
<p>The API returns the following response. Note that the duplicate tokens for <code class="literal">fox</code>
and <code class="literal">and</code> have been removed.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Response</strong></span></summary>
<div class="content">
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "detail": {
    "custom_analyzer": true,
    "charfilters": [],
    "tokenizer": ...,
    "tokenfilters": [
      {
        "name": "keyword_repeat",
        "tokens": ...
      },
      {
        "name": "stemmer",
        "tokens": ...
      },
      {
        "name": "remove_duplicates",
        "tokens": [
          {
            "token": "fox",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "keyword": true
          },
          {
            "token": "running",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": true
          },
          {
            "token": "run",
            "start_offset": 4,
            "end_offset": 11,
            "type": "word",
            "position": 1,
            "keyword": false
          },
          {
            "token": "and",
            "start_offset": 12,
            "end_offset": 15,
            "type": "word",
            "position": 2,
            "keyword": true
          },
          {
            "token": "jumping",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": true
          },
          {
            "token": "jump",
            "start_offset": 16,
            "end_offset": 23,
            "type": "word",
            "position": 3,
            "keyword": false
          }
        ]
      }
    ]
  }
}</pre>
</div>
</div>
</details>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-keyword-repeat-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/keyword-repeat-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">keyword_repeat</code> filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom
analyzer</a>.</p>
<p>This custom analyzer uses the <code class="literal">keyword_repeat</code> and <code class="literal">porter_stem</code> filters to
create a stemmed and unstemmed version of each token in a stream. The
<code class="literal">remove_duplicates</code> filter then removes any duplicate tokens from the stream.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "keyword_repeat",
            "porter_stem",
            "remove_duplicates"
          ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/457.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-kstem-tokenfilter"></a>KStem token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/kstem-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Provides <a href="https://ciir.cs.umass.edu/pubfiles/ir-35.pdf" class="ulink" target="_top">KStem</a>-based stemming for
the English language. The <code class="literal">kstem</code> filter combines
<a class="xref" href="analysis-concepts.html#algorithmic-stemmers" title="Algorithmic stemmers">algorithmic stemming</a> with a built-in
<a class="xref" href="analysis-concepts.html#dictionary-stemmers" title="Dictionary stemmers">dictionary</a>.</p>
<p>The <code class="literal">kstem</code> filter tends to stem less aggressively than other English stemmer
filters, such as the <a class="xref" href="analysis-tokenfilters.html#analysis-porterstem-tokenfilter" title="Porter stem token filter"><code class="literal">porter_stem</code></a> filter.</p>
<p>The <code class="literal">kstem</code> filter is equivalent to the
<a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter" title="Stemmer token filter"><code class="literal">stemmer</code></a> filter&#8217;s
<a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter-language-parm"><code class="literal">light_english</code></a> variant.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/en/KStemFilter.html" class="ulink" target="_top">KStemFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-kstem-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/kstem-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following analyze API request uses the <code class="literal">kstem</code> filter to stem <code class="literal">the foxes
jumping quickly</code> to <code class="literal">the fox jump quick</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [ "kstem" ],
  "text": "the foxes jumping quickly"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/458.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, fox, jump, quick ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-kstem-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/kstem-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">kstem</code> filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom
analyzer</a>.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>To work properly, the <code class="literal">kstem</code> filter requires lowercase tokens. To ensure tokens
are lowercased, add the <a class="xref" href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter" title="Lowercase token filter"><code class="literal">lowercase</code></a> filter
before the <code class="literal">kstem</code> filter in the analyzer configuration.</p>
</div>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [
            "lowercase",
            "kstem"
          ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/459.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-length-tokenfilter"></a>Length token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/length-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Removes tokens shorter or longer than specified character lengths.
For example, you can use the <code class="literal">length</code> filter to exclude tokens shorter than 2
characters and tokens longer than 5 characters.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/LengthFilter.html" class="ulink" target="_top">LengthFilter</a>.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>The <code class="literal">length</code> filter removes entire tokens. If you&#8217;d prefer to shorten tokens to
a specific length, use the <a class="xref" href="analysis-tokenfilters.html#analysis-truncate-tokenfilter" title="Truncate token filter"><code class="literal">truncate</code></a> filter.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-length-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/length-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">length</code>
filter to remove tokens longer than 4 characters:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "length",
      "min": 0,
      "max": 4
    }
  ],
  "text": "the quick brown fox jumps over the lazy dog"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/460.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, fox, over, the, lazy, dog ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-length-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/length-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">length</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT length_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_length": {
          "tokenizer": "standard",
          "filter": [ "length" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/461.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-length-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/length-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">min</code>
</span>
</dt>
<dd>
(Optional, integer)
Minimum character length of a token. Shorter tokens are excluded from the
output. Defaults to <code class="literal">0</code>.
</dd>
<dt>
<span class="term">
<code class="literal">max</code>
</span>
</dt>
<dd>
(Optional, integer)
Maximum character length of a token. Longer tokens are excluded from the output.
Defaults to <code class="literal">Integer.MAX_VALUE</code>, which is <code class="literal">2^31-1</code> or <code class="literal">2147483647</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-length-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/length-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">length</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">length</code> filter that removes
tokens shorter than 2 characters and tokens longer than 10 characters:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT length_custom_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_length_2_to_10_char": {
          "tokenizer": "whitespace",
          "filter": [ "length_2_to_10_char" ]
        }
      },
      "filter": {
        "length_2_to_10_char": {
          "type": "length",
          "min": 2,
          "max": 10
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/462.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-limit-token-count-tokenfilter"></a>Limit token count token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/limit-token-count-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Limits the number of output tokens. The <code class="literal">limit</code> filter is commonly used to limit
the size of document field values based on token count.</p>
<p>By default, the <code class="literal">limit</code> filter keeps only the first token in a stream. For
example, the filter can change the token stream <code class="literal">[ one, two, three ]</code> to
<code class="literal">[ one ]</code>.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilter.html" class="ulink" target="_top">LimitTokenCountFilter</a>.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<pre class="literallayout"> If you want to limit the size of field values based on
_character length_, use the &lt;&lt;ignore-above,`ignore_above`&gt;&gt; mapping parameter.</pre>

</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-limit-token-count-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/limit-token-count-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">max_token_count</code>
</span>
</dt>
<dd>
(Optional, integer)
Maximum number of tokens to keep. Once this limit is reached, any remaining
tokens are excluded from the output. Defaults to <code class="literal">1</code>.
</dd>
<dt>
<span class="term">
<code class="literal">consume_all_tokens</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the <code class="literal">limit</code> filter exhausts the token stream, even if the
<code class="literal">max_token_count</code> has already been reached. Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-limit-token-count-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/limit-token-count-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">limit</code>
filter to keep only the first two tokens in <code class="literal">quick fox jumps over lazy dog</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "standard",
    "filter": [
    {
      "type": "limit",
      "max_token_count": 2
    }
  ],
  "text": "quick fox jumps over lazy dog"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/463.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ quick, fox ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-limit-token-count-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/limit-token-count-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">limit</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT limit_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_one_token_limit": {
          "tokenizer": "standard",
          "filter": [ "limit" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/464.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-limit-token-count-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/limit-token-count-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">limit</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">limit</code> filter that keeps
only the first five tokens of a stream:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT custom_limit_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_five_token_limit": {
          "tokenizer": "whitespace",
          "filter": [ "five_token_limit" ]
        }
      },
      "filter": {
        "five_token_limit": {
          "type": "limit",
          "max_token_count": 5
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/465.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-lowercase-tokenfilter"></a>Lowercase token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/lowercase-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Changes token text to lowercase. For example, you can use the <code class="literal">lowercase</code> filter
to change <code class="literal">THE Lazy DoG</code> to <code class="literal">the lazy dog</code>.</p>
<p>In addition to a default filter, the <code class="literal">lowercase</code> token filter provides access to
Lucene&#8217;s language-specific lowercase filters for Greek, Irish, and Turkish.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-lowercase-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/lowercase-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the default
<code class="literal">lowercase</code> filter to change the <code class="literal">THE Quick FoX JUMPs</code> to lowercase:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["lowercase"],
  "text" : "THE Quick FoX JUMPs"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/466.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, quick, fox, jumps ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-lowercase-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/lowercase-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">lowercase</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT lowercase_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_lowercase": {
          "tokenizer": "whitespace",
          "filter": [ "lowercase" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/467.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-lowercase-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/lowercase-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">language</code>
</span>
</dt>
<dd>
<p>(Optional, string)
Language-specific lowercase token filter to use. Valid values include:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">greek</code>
</span>
</dt>
<dd>
Uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/el/GreekLowerCaseFilter.html" class="ulink" target="_top">GreekLowerCaseFilter</a>
</dd>
<dt>
<span class="term">
<code class="literal">irish</code>
</span>
</dt>
<dd>
Uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/ga/IrishLowerCaseFilter.html" class="ulink" target="_top">IrishLowerCaseFilter</a>
</dd>
<dt>
<span class="term">
<code class="literal">turkish</code>
</span>
</dt>
<dd>
Uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/tr/TurkishLowerCaseFilter.html" class="ulink" target="_top">TurkishLowerCaseFilter</a>
</dd>
</dl>
</div>
<p>If not specified, defaults to Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.html" class="ulink" target="_top">LowerCaseFilter</a>.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-lowercase-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/lowercase-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">lowercase</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">lowercase</code> filter for the
Greek language:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT custom_lowercase_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "greek_lowercase_example": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["greek_lowercase"]
        }
      },
      "filter": {
        "greek_lowercase": {
          "type": "lowercase",
          "language": "greek"
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/468.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-minhash-tokenfilter"></a>MinHash token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/minhash-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Uses the <a href="https://en.wikipedia.org/wiki/MinHash" class="ulink" target="_top">MinHash</a> technique to produce a
signature for a token stream. You can use MinHash signatures to estimate the
similarity of documents. See <a class="xref" href="analysis-tokenfilters.html#analysis-minhash-tokenfilter-similarity-search" title="Using the min_hash token filter for similarity search">Using the <code class="literal">min_hash</code> token filter for similarity search</a>.</p>
<p>The <code class="literal">min_hash</code> filter performs the following operations on a token stream in
order:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Hashes each token in the stream.
</li>
<li class="listitem">
Assigns the hashes to buckets, keeping only the smallest hashes of each
bucket.
</li>
<li class="listitem">
Outputs the smallest hash from each bucket as a token stream.
</li>
</ol>
</div>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/minhash/MinHashFilter.html" class="ulink" target="_top">MinHashFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-minhash-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/minhash-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">bucket_count</code>
</span>
</dt>
<dd>
(Optional, integer)
Number of buckets to which hashes are assigned. Defaults to <code class="literal">512</code>.
</dd>
<dt>
<span class="term">
<code class="literal">hash_count</code>
</span>
</dt>
<dd>
(Optional, integer)
Number of ways to hash each token in the stream. Defaults to <code class="literal">1</code>.
</dd>
<dt>
<span class="term">
<code class="literal">hash_set_size</code>
</span>
</dt>
<dd>
<p>
(Optional, integer)
Number of hashes to keep from each bucket. Defaults to <code class="literal">1</code>.
</p>
<p>Hashes are retained by ascending size, starting with the bucket&#8217;s smallest hash
first.</p>
</dd>
<dt>
<span class="term">
<code class="literal">with_rotation</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter fills empty buckets with the value of the first non-empty
bucket to its circular right if the <code class="literal">hash_set_size</code> is <code class="literal">1</code>. If the
<code class="literal">bucket_count</code> argument is greater than <code class="literal">1</code>, this parameter defaults to <code class="literal">true</code>.
Otherwise, this parameter defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-minhash-tokenfilter-configuration-tips"></a>Tips for configuring the <code class="literal">min_hash</code> filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/minhash-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">min_hash</code> filter input tokens should typically be k-words shingles produced
from <a class="xref" href="analysis-tokenfilters.html#analysis-shingle-tokenfilter" title="Shingle token filter">shingle token filter</a>. You should
choose <code class="literal">k</code> large enough so that the probability of any given shingle
occurring in a document is low. At the same time, as
internally each shingle is hashed into to 128-bit hash, you should choose
<code class="literal">k</code> small enough so that all possible
different k-words shingles can be hashed to 128-bit hash with
minimal collision.
</li>
<li class="listitem">
<p>We recommend you test different arguments for the <code class="literal">hash_count</code>, <code class="literal">bucket_count</code> and
<code class="literal">hash_set_size</code> parameters:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
To improve precision, increase the <code class="literal">bucket_count</code> or
<code class="literal">hash_set_size</code> arguments. Higher <code class="literal">bucket_count</code> and <code class="literal">hash_set_size</code> values
increase the likelihood that different tokens are indexed to different
buckets.
</li>
<li class="listitem">
To improve the recall, increase the value of the <code class="literal">hash_count</code> argument. For
example, setting <code class="literal">hash_count</code> to <code class="literal">2</code> hashes each token in two different ways,
increasing the number of potential candidates for search.
</li>
</ul>
</div>
</li>
<li class="listitem">
By default, the <code class="literal">min_hash</code> filter produces 512 tokens for each document. Each
token is 16 bytes in size. This means each document&#8217;s size will be increased by
around 8Kb.
</li>
<li class="listitem">
The <code class="literal">min_hash</code> filter is used for Jaccard similarity. This means
that it doesn&#8217;t matter how many times a document contains a certain token,
only that if it contains it or not.
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-minhash-tokenfilter-similarity-search"></a>Using the <code class="literal">min_hash</code> token filter for similarity search<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/minhash-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The <code class="literal">min_hash</code> token filter allows you to hash documents for similarity search.
Similarity search, or nearest neighbor search is a complex problem.
A naive solution requires an exhaustive pairwise comparison between a query
document and every document in an index. This is a prohibitive operation
if the index is large. A number of approximate nearest neighbor search
solutions have been developed to make similarity search more practical and
computationally feasible. One of these solutions involves hashing of documents.</p>
<p>Documents are hashed in a way that similar documents are more likely
to produce the same hash code and are put into the same hash bucket,
while dissimilar documents are more likely to be hashed into
different hash buckets. This type of hashing is known as
locality sensitive hashing (LSH).</p>
<p>Depending on what constitutes the similarity between documents,
various LSH functions <a href="https://arxiv.org/abs/1408.2927" class="ulink" target="_top">have been proposed</a>.
For <a href="https://en.wikipedia.org/wiki/Jaccard_index" class="ulink" target="_top">Jaccard similarity</a>, a popular
LSH function is <a href="https://en.wikipedia.org/wiki/MinHash" class="ulink" target="_top">MinHash</a>.
A general idea of the way MinHash produces a signature for a document
is by applying a random permutation over the whole index vocabulary (random
numbering for the vocabulary), and recording the minimum value for this permutation
for the document (the minimum number for a vocabulary word that is present
in the document). The permutations are run several times;
combining the minimum values for all of them will constitute a
signature for the document.</p>
<p>In practice, instead of random permutations, a number of hash functions
are chosen. A hash function calculates a hash code for each of a
document&#8217;s tokens and chooses the minimum hash code among them.
The minimum hash codes from all hash functions are combined
to form a signature for the document.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-minhash-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/minhash-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">min_hash</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses the following custom token filters to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">my_shingle_filter</code>, a custom <a class="xref" href="analysis-tokenfilters.html#analysis-shingle-tokenfilter" title="Shingle token filter"><code class="literal">shingle</code>
filter</a>. <code class="literal">my_shingle_filter</code> only outputs five-word shingles.
</li>
<li class="listitem">
<code class="literal">my_minhash_filter</code>, a custom <code class="literal">min_hash</code> filter. <code class="literal">my_minhash_filter</code> hashes
each five-word shingle once. It then assigns the hashes into 512 buckets,
keeping only the smallest hash from each bucket.
</li>
</ul>
</div>
<p>The request also assigns the custom analyzer to the <code class="literal">fingerprint</code> field mapping.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "filter": {
        "my_shingle_filter": {      <a id="CO162-1"></a><i class="conum" data-value="1"></i>
          "type": "shingle",
          "min_shingle_size": 5,
          "max_shingle_size": 5,
          "output_unigrams": false
        },
        "my_minhash_filter": {
          "type": "min_hash",
          "hash_count": 1,          <a id="CO162-2"></a><i class="conum" data-value="2"></i>
          "bucket_count": 512,      <a id="CO162-3"></a><i class="conum" data-value="3"></i>
          "hash_set_size": 1,       <a id="CO162-4"></a><i class="conum" data-value="4"></i>
          "with_rotation": true     <a id="CO162-5"></a><i class="conum" data-value="5"></i>
        }
      },
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "my_shingle_filter",
            "my_minhash_filter"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "fingerprint": {
        "type": "text",
        "analyzer": "my_analyzer"
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/469.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO162-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Configures a custom shingle filter to output only five-word shingles.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO162-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Each five-word shingle in the stream is hashed once.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO162-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>The hashes are assigned to 512 buckets.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO162-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>Only the smallest hash in each bucket is retained.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO162-5"><i class="conum" data-value="5"></i></a></p>
</td>
<td align="left" valign="top">
<p>The filter fills empty buckets with the values of neighboring buckets.</p>
</td>
</tr>
</table>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-multiplexer-tokenfilter"></a>Multiplexer token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/multiplexer-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>A token filter of type <code class="literal">multiplexer</code> will emit multiple tokens at the same position,
each version of the token having been run through a different filter.  Identical
output tokens at the same position will be removed.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>If the incoming token stream has duplicate tokens, then these will also be
removed by the multiplexer</p>
</div>
</div>
<h3><a id="_options"></a>Options<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/multiplexer-tokenfilter.asciidoc">edit</a></h3>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
filters
</p>
</td>
<td valign="top">
<p>
a list of token filters to apply to incoming tokens.  These can be any
token filters defined elsewhere in the index mappings.  Filters can be chained
using a comma-delimited string, so for example <code class="literal">"lowercase, porter_stem"</code> would
apply the <code class="literal">lowercase</code> filter and then the <code class="literal">porter_stem</code> filter to a single token.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p><a class="xref" href="analysis-tokenfilters.html#analysis-shingle-tokenfilter" title="Shingle token filter">Shingle</a> or multi-word synonym token filters will not function normally
  when they are declared in the filters array because they read ahead internally
  which is unsupported by the multiplexer</p>
</div>
</div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
preserve_original
</span>
</dt>
<dd>
if <code class="literal">true</code> (the default) then emit the original token in
addition to the filtered tokens
</dd>
</dl>
</div>
<h3><a id="_settings_example"></a>Settings example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/multiplexer-tokenfilter.asciidoc">edit</a></h3>
<p>You can set it up like:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /multiplexer_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "my_multiplexer" ]
        }
      },
      "filter": {
        "my_multiplexer": {
          "type": "multiplexer",
          "filters": [ "lowercase", "lowercase, porter_stem" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/470.console"></div>
<p>And test it like:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST /multiplexer_example/_analyze
{
  "analyzer" : "my_analyzer",
  "text" : "Going HOME"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/471.console"></div>
<p>And it&#8217;d respond:</p>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "tokens": [
    {
      "token": "Going",
      "start_offset": 0,
      "end_offset": 5,
      "type": "&lt;ALPHANUM&gt;",
      "position": 0
    },
    {
      "token": "going",
      "start_offset": 0,
      "end_offset": 5,
      "type": "&lt;ALPHANUM&gt;",
      "position": 0
    },
    {
      "token": "go",
      "start_offset": 0,
      "end_offset": 5,
      "type": "&lt;ALPHANUM&gt;",
      "position": 0
    },
    {
      "token": "HOME",
      "start_offset": 6,
      "end_offset": 10,
      "type": "&lt;ALPHANUM&gt;",
      "position": 1
    },
    {
      "token": "home",          <a id="CO163-1"></a><i class="conum" data-value="1"></i>
      "start_offset": 6,
      "end_offset": 10,
      "type": "&lt;ALPHANUM&gt;",
      "position": 1
    }
  ]
}</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO163-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The stemmer has also emitted a token <code class="literal">home</code> at position 1, but because it is a
duplicate of this token it has been removed from the token stream</p>
</td>
</tr>
</table>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The synonym and synonym_graph filters use their preceding analysis chain to
parse and analyse their synonym lists, and will throw an exception if that chain
contains token filters that produce multiple tokens at the same position.
If you want to apply synonyms to a token stream containing a multiplexer, then you
should append the synonym filter to each relevant multiplexer filter list, rather than
placing it after the multiplexer in the main token chain definition.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-ngram-tokenfilter"></a>N-gram token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/ngram-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Forms <a href="https://en.wikipedia.org/wiki/N-gram" class="ulink" target="_top">n-grams</a> of specified lengths from
a token.</p>
<p>For example, you can use the <code class="literal">ngram</code> token filter to change <code class="literal">fox</code> to
<code class="literal">[ f, fo, o, ox, x ]</code>.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html" class="ulink" target="_top">NGramTokenFilter</a>.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The <code class="literal">ngram</code> filter is similar to the
<a class="xref" href="analysis-tokenfilters.html#analysis-edgengram-tokenfilter" title="Edge n-gram token filter"><code class="literal">edge_ngram</code> token filter</a>. However, the
<code class="literal">edge_ngram</code> only outputs n-grams that start at the beginning of a token.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-ngram-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/ngram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">ngram</code>
filter to convert <code class="literal">Quick fox</code> to 1-character and 2-character n-grams:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "standard",
  "filter": [ "ngram" ],
  "text": "Quick fox"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/472.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Q, Qu, u, ui, i, ic, c, ck, k, f, fo, o, ox, x ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-ngram-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/ngram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the <code class="literal">ngram</code>
filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT ngram_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_ngram": {
          "tokenizer": "standard",
          "filter": [ "ngram" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/473.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-ngram-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/ngram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">max_gram</code>
</span>
</dt>
<dd>
(Optional, integer)
Maximum length of characters in a gram. Defaults to <code class="literal">2</code>.
</dd>
<dt>
<span class="term">
<code class="literal">min_gram</code>
</span>
</dt>
<dd>
(Optional, integer)
Minimum length of characters in a gram. Defaults to <code class="literal">1</code>.
</dd>
<dt>
<span class="term">
<code class="literal">preserve_original</code>
</span>
</dt>
<dd>
(Optional, Boolean)
Emits original token when set to <code class="literal">true</code>. Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
<p>You can use the <a class="xref" href="index-modules.html#index-max-ngram-diff"><code class="literal">index.max_ngram_diff</code></a> index-level
setting to control the maximum allowed difference between the <code class="literal">max_gram</code> and
<code class="literal">min_gram</code> values.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-ngram-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/ngram-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">ngram</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">ngram</code> filter that forms
n-grams between 3-5 characters. The request also increases the
<code class="literal">index.max_ngram_diff</code> setting to <code class="literal">2</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT ngram_custom_example
{
  "settings": {
    "index": {
      "max_ngram_diff": 2
    },
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "3_5_grams" ]
        }
      },
      "filter": {
        "3_5_grams": {
          "type": "ngram",
          "min_gram": 3,
          "max_gram": 5
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/474.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-normalization-tokenfilter"></a>Normalization token filters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/normalization-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>There are several token filters available which try to normalize special
characters of a certain language.</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
Arabic
</p>
</td>
<td valign="top">
<p>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/ar/ArabicNormalizer.html" class="ulink" target="_top"><code class="literal">arabic_normalization</code></a>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
German
</p>
</td>
<td valign="top">
<p>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/de/GermanNormalizationFilter.html" class="ulink" target="_top"><code class="literal">german_normalization</code></a>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
Hindi
</p>
</td>
<td valign="top">
<p>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/hi/HindiNormalizer.html" class="ulink" target="_top"><code class="literal">hindi_normalization</code></a>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
Indic
</p>
</td>
<td valign="top">
<p>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/in/IndicNormalizer.html" class="ulink" target="_top"><code class="literal">indic_normalization</code></a>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
Kurdish (Sorani)
</p>
</td>
<td valign="top">
<p>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/ckb/SoraniNormalizer.html" class="ulink" target="_top"><code class="literal">sorani_normalization</code></a>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
Persian
</p>
</td>
<td valign="top">
<p>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/fa/PersianNormalizer.html" class="ulink" target="_top"><code class="literal">persian_normalization</code></a>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
Scandinavian
</p>
</td>
<td valign="top">
<p>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.html" class="ulink" target="_top"><code class="literal">scandinavian_normalization</code></a>,
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.html" class="ulink" target="_top"><code class="literal">scandinavian_folding</code></a>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
Serbian
</p>
</td>
<td valign="top">
<p>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/sr/SerbianNormalizationFilter.html" class="ulink" target="_top"><code class="literal">serbian_normalization</code></a>
</p>
</td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-pattern-capture-tokenfilter"></a>Pattern capture token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/pattern-capture-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">pattern_capture</code> token filter, unlike the <code class="literal">pattern</code> tokenizer,
emits a token for every capture group in the regular expression.
Patterns are not anchored to the beginning and end of the string, so
each pattern can match multiple times, and matches are allowed to
overlap.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<h3>Beware of Pathological Regular Expressions</h3>
<p>The pattern capture token filter uses
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html" class="ulink" target="_top">Java Regular Expressions</a>.</p>
<p>A badly written regular expression could run very slowly or even throw a
StackOverflowError and cause the node it is running on to exit suddenly.</p>
<p>Read more about <a href="https://www.regular-expressions.info/catastrophic.html" class="ulink" target="_top">pathological regular expressions and how to avoid them</a>.</p>
</div>
</div>
<p>For instance a pattern like :</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">"(([a-z]+)(\d*))"</pre>
</div>
<p>when matched against:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">"abc123def456"</pre>
</div>
<p>would produce the tokens: [ <code class="literal">abc123</code>, <code class="literal">abc</code>, <code class="literal">123</code>, <code class="literal">def456</code>, <code class="literal">def</code>,
<code class="literal">456</code> ]</p>
<p>If <code class="literal">preserve_original</code> is set to <code class="literal">true</code> (the default) then it would also
emit the original token: <code class="literal">abc123def456</code>.</p>
<p>This is particularly useful for indexing text like camel-case code, eg
<code class="literal">stripHTML</code> where a user may search for <code class="literal">"strip html"</code> or <code class="literal">"striphtml"</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT test
{
   "settings" : {
      "analysis" : {
         "filter" : {
            "code" : {
               "type" : "pattern_capture",
               "preserve_original" : true,
               "patterns" : [
                  "(\\p{Ll}+|\\p{Lu}\\p{Ll}+|\\p{Lu}+)",
                  "(\\d+)"
               ]
            }
         },
         "analyzer" : {
            "code" : {
               "tokenizer" : "pattern",
               "filter" : [ "code", "lowercase" ]
            }
         }
      }
   }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/475.console"></div>
<p>When used to analyze the text</p>
<div class="pre_wrapper lang-java">
<pre class="programlisting prettyprint lang-java">import static org.apache.commons.lang.StringEscapeUtils.escapeHtml</pre>
</div>
<p>this emits the tokens: [ <code class="literal">import</code>, <code class="literal">static</code>, <code class="literal">org</code>, <code class="literal">apache</code>, <code class="literal">commons</code>,
<code class="literal">lang</code>, <code class="literal">stringescapeutils</code>, <code class="literal">string</code>, <code class="literal">escape</code>, <code class="literal">utils</code>, <code class="literal">escapehtml</code>,
<code class="literal">escape</code>, <code class="literal">html</code> ]</p>
<p>Another example is analyzing email addresses:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT test
{
   "settings" : {
      "analysis" : {
         "filter" : {
            "email" : {
               "type" : "pattern_capture",
               "preserve_original" : true,
               "patterns" : [
                  "([^@]+)",
                  "(\\p{L}+)",
                  "(\\d+)",
                  "@(.+)"
               ]
            }
         },
         "analyzer" : {
            "email" : {
               "tokenizer" : "uax_url_email",
               "filter" : [ "email", "lowercase",  "unique" ]
            }
         }
      }
   }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/476.console"></div>
<p>When the above analyzer is used on an email address like:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">john-smith_123@foo-bar.com</pre>
</div>
<p>it would produce the following tokens:</p>
<pre class="literallayout">john-smith_123@foo-bar.com, john-smith_123,
john, smith, 123, foo-bar.com, foo, bar, com</pre>

<p>Multiple patterns are required to allow overlapping captures, but also
means that patterns are less dense and easier to understand.</p>
<p><span class="strong strong"><strong>Note:</strong></span> All tokens are emitted in the same position, and with the same
character offsets. This means, for example, that a <code class="literal">match</code> query for
<code class="literal">john-smith_123@foo-bar.com</code> that uses this analyzer will return documents
containing any of these tokens, even when using the <code class="literal">and</code> operator.
Also, when combined with highlighting, the whole original token will
be highlighted, not just the matching subset. For instance, querying
the above email address for <code class="literal">"smith"</code> would highlight:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">  &lt;em&gt;john-smith_123@foo-bar.com&lt;/em&gt;</pre>
</div>
<p>not:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">  john-&lt;em&gt;smith&lt;/em&gt;_123@foo-bar.com</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-pattern_replace-tokenfilter"></a>Pattern replace token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/pattern_replace-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Uses a regular expression to match and replace token substrings.</p>
<p>The <code class="literal">pattern_replace</code> filter uses
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html" class="ulink" target="_top">Java&#8217;s
regular expression syntax</a>. By default, the filter replaces matching substrings
with an empty substring (<code class="literal">""</code>). Replacement substrings can use Java&#8217;s
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Matcher.html#appendReplacement-java.lang.StringBuffer-java.lang.String-" class="ulink" target="_top"><code class="literal">$g</code>
syntax</a> to reference capture groups from the original token text.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>A poorly-written regular expression may run slowly or return a
StackOverflowError, causing the node running the expression to exit suddenly.</p>
<p>Read more about
<a href="https://www.regular-expressions.info/catastrophic.html" class="ulink" target="_top">pathological regular
expressions and how to avoid them</a>.</p>
</div>
</div>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/pattern/PatternReplaceFilter.html" class="ulink" target="_top">PatternReplaceFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-pattern-replace-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/pattern_replace-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">pattern_replace</code>
filter to prepend <code class="literal">watch</code> to the substring <code class="literal">dog</code> in <code class="literal">foxes jump lazy dogs</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "pattern_replace",
      "pattern": "(dog)",
      "replacement": "watch$1"
    }
  ],
  "text": "foxes jump lazy dogs"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/477.console"></div>
<p>The filter produces the following tokens.</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ foxes, jump, lazy, watchdogs ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-pattern-replace-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/pattern_replace-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">all</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, all substrings matching the <code class="literal">pattern</code> parameter&#8217;s regular expression
are replaced. If <code class="literal">false</code>, the filter replaces only the first matching substring
in each token. Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">pattern</code>
</span>
</dt>
<dd>
(Required, string)
Regular expression, written in
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html" class="ulink" target="_top">Java&#8217;s
regular expression syntax</a>. The filter replaces token substrings matching this
pattern with the substring in the <code class="literal">replacement</code> parameter.
</dd>
<dt>
<span class="term">
<code class="literal">replacement</code>
</span>
</dt>
<dd>
(Optional, string)
Replacement substring. Defaults to an empty substring (<code class="literal">""</code>).
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-pattern-replace-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/pattern_replace-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">pattern_replace</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
configures a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a> using a custom
<code class="literal">pattern_replace</code> filter, <code class="literal">my_pattern_replace_filter</code>.</p>
<p>The <code class="literal">my_pattern_replace_filter</code> filter uses the regular expression <code class="literal">[£|€]</code> to
match and remove the currency symbols <code class="literal">£</code> and <code class="literal">€</code>. The filter&#8217;s <code class="literal">all</code>
parameter is <code class="literal">false</code>, meaning only the first matching symbol in each token is
removed.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [
            "my_pattern_replace_filter"
          ]
        }
      },
      "filter": {
        "my_pattern_replace_filter": {
          "type": "pattern_replace",
          "pattern": "[£|€]",
          "replacement": "",
          "all": false
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/478.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-phonetic-tokenfilter"></a>Phonetic token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/phonetic-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">phonetic</code> token filter is provided as the <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.10/analysis-phonetic.html" class="ulink" target="_top"><code class="literal">analysis-phonetic</code></a> plugin.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-porterstem-tokenfilter"></a>Porter stem token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/porterstem-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Provides <a class="xref" href="analysis-concepts.html#algorithmic-stemmers" title="Algorithmic stemmers">algorithmic stemming</a> for the English language,
based on the <a href="https://snowballstem.org/algorithms/porter/stemmer.html" class="ulink" target="_top">Porter
stemming algorithm</a>.</p>
<p>This filter tends to stem more aggressively than other English
stemmer filters, such as the <a class="xref" href="analysis-tokenfilters.html#analysis-kstem-tokenfilter" title="KStem token filter"><code class="literal">kstem</code></a> filter.</p>
<p>The <code class="literal">porter_stem</code> filter is equivalent to the
<a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter" title="Stemmer token filter"><code class="literal">stemmer</code></a> filter&#8217;s
<a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter-language-parm"><code class="literal">english</code></a> variant.</p>
<p>The <code class="literal">porter_stem</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/en/PorterStemFilter.html" class="ulink" target="_top">PorterStemFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-porterstem-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/porterstem-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following analyze API request uses the <code class="literal">porter_stem</code> filter to stem
<code class="literal">the foxes jumping quickly</code> to <code class="literal">the fox jump quickli</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [ "porter_stem" ],
  "text": "the foxes jumping quickly"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/479.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, fox, jump, quickli ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-porterstem-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/porterstem-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">porter_stem</code> filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom
analyzer</a>.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>To work properly, the <code class="literal">porter_stem</code> filter requires lowercase tokens. To ensure
tokens are lowercased, add the <a class="xref" href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter" title="Lowercase token filter"><code class="literal">lowercase</code></a>
filter before the <code class="literal">porter_stem</code> filter in the analyzer configuration.</p>
</div>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [
            "lowercase",
            "porter_stem"
          ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/480.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-predicatefilter-tokenfilter"></a>Predicate script token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/predicate-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Removes tokens that don&#8217;t match a provided predicate script. The filter supports
inline <a href="https://www.elastic.co/guide/en/elasticsearch/painless/7.10/index.html" class="ulink" target="_top">Painless</a> scripts only. Scripts are evaluated in
the <a href="https://www.elastic.co/guide/en/elasticsearch/painless/7.10/painless-analysis-predicate-context.html" class="ulink" target="_top">analysis predicate
context</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-predicatefilter-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/predicate-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<code class="literal">predicate_token_filter</code> filter to only output tokens longer than three
characters from <code class="literal">the fox jumps the lazy dog</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "predicate_token_filter",
      "script": {
        "source": """
          token.term.length() &gt; 3
        """
      }
    }
  ],
  "text": "the fox jumps the lazy dog"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/481.console"></div>
<p>The filter produces the following tokens.</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ jumps, lazy ]</pre>
</div>
<p>The API response contains the position and offsets of each output token. Note
the <code class="literal">predicate_token_filter</code> filter does not change the tokens' original
positions or offets.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Response</strong></span></summary>
<div class="content">
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "tokens" : [
    {
      "token" : "jumps",
      "start_offset" : 8,
      "end_offset" : 13,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "lazy",
      "start_offset" : 18,
      "end_offset" : 22,
      "type" : "word",
      "position" : 4
    }
  ]
}</pre>
</div>
</div>
</details>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-predicatefilter-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/predicate-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">script</code>
</span>
</dt>
<dd>
<p>
(Required, <a class="xref" href="modules-scripting-using.html" title="How to use scripts">script object</a>)
Script containing a condition used to filter incoming tokens. Only tokens that
match this script are included in the output.
</p>
<p>This parameter supports inline <a href="https://www.elastic.co/guide/en/elasticsearch/painless/7.10/index.html" class="ulink" target="_top">Painless</a> scripts only. The
script is evaluated in the
<a href="https://www.elastic.co/guide/en/elasticsearch/painless/7.10/painless-analysis-predicate-context.html" class="ulink" target="_top">analysis predicate context</a>.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-predicatefilter-tokenfilter-customize"></a>Customize and add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/predicate-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">predicate_token_filter</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
configures a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a> using a custom
<code class="literal">predicate_token_filter</code> filter, <code class="literal">my_script_filter</code>.</p>
<p>The <code class="literal">my_script_filter</code> filter removes tokens with of any type other than
<code class="literal">ALPHANUM</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "my_script_filter"
          ]
        }
      },
      "filter": {
        "my_script_filter": {
          "type": "predicate_token_filter",
          "script": {
            "source": """
              token.type.contains("ALPHANUM")
            """
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/482.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-remove-duplicates-tokenfilter"></a>Remove duplicates token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/remove-duplicates-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Removes duplicate tokens in the same position.</p>
<p>The <code class="literal">remove_duplicates</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.html" class="ulink" target="_top">RemoveDuplicatesTokenFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-remove-duplicates-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/remove-duplicates-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To see how the <code class="literal">remove_duplicates</code> filter works, you first need to produce a
token stream containing duplicate tokens in the same position.</p>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<a class="xref" href="analysis-tokenfilters.html#analysis-keyword-repeat-tokenfilter" title="Keyword repeat token filter"><code class="literal">keyword_repeat</code></a> and
<a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter" title="Stemmer token filter"><code class="literal">stemmer</code></a> filters to create stemmed and
unstemmed tokens for <code class="literal">jumping dog</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer"
  ],
  "text": "jumping dog"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/483.console"></div>
<p>The API returns the following response. Note that the <code class="literal">dog</code> token in position
<code class="literal">1</code> is duplicated.</p>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "tokens": [
    {
      "token": "jumping",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "jump",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "dog",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 1
    },
    {
      "token": "dog",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 1
    }
  ]
}</pre>
</div>
<p>To remove one of the duplicate <code class="literal">dog</code> tokens, add the <code class="literal">remove_duplicates</code> filter
to the previous analyze API request.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer",
    "remove_duplicates"
  ],
  "text": "jumping dog"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/484.console"></div>
<p>The API returns the following response. There is now only one <code class="literal">dog</code> token in
position <code class="literal">1</code>.</p>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "tokens": [
    {
      "token": "jumping",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "jump",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "dog",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 1
    }
  ]
}</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-remove-duplicates-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/remove-duplicates-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">remove_duplicates</code> filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom
analyzer</a>.</p>
<p>This custom analyzer uses the <code class="literal">keyword_repeat</code> and <code class="literal">stemmer</code> filters to create a
stemmed and unstemmed version of each token in a stream. The <code class="literal">remove_duplicates</code>
filter then removes any duplicate tokens in the same position.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "keyword_repeat",
            "stemmer",
            "remove_duplicates"
          ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/485.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-reverse-tokenfilter"></a>Reverse token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/reverse-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Reverses each token in a stream. For example, you can use the <code class="literal">reverse</code> filter
to change <code class="literal">cat</code> to <code class="literal">tac</code>.</p>
<p>Reversed tokens are useful for suffix-based searches,
such as finding words that end in <code class="literal">-ion</code> or searching file names by their
extension.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/reverse/ReverseStringFilter.html" class="ulink" target="_top">ReverseStringFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-reverse-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/reverse-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">reverse</code>
filter to reverse each token in <code class="literal">quick fox jumps</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["reverse"],
  "text" : "quick fox jumps"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/486.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ kciuq, xof, spmuj ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-reverse-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/reverse-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">reverse</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT reverse_example
{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "whitespace_reverse" : {
          "tokenizer" : "whitespace",
          "filter" : ["reverse"]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/487.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-shingle-tokenfilter"></a>Shingle token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/shingle-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Add shingles, or word <a href="https://en.wikipedia.org/wiki/N-gram" class="ulink" target="_top">n-grams</a>, to a token
stream by concatenating adjacent tokens. By default, the <code class="literal">shingle</code> token filter
outputs two-word shingles and unigrams.</p>
<p>For example, many tokenizers convert <code class="literal">the lazy dog</code> to <code class="literal">[ the, lazy, dog ]</code>. You
can use the <code class="literal">shingle</code> filter to add two-word shingles to this stream:
<code class="literal">[ the, the lazy, lazy, lazy dog, dog ]</code>.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>Shingles are often used to help speed up phrase queries, such as
<a class="xref" href="full-text-queries.html#query-dsl-match-query-phrase" title="Match phrase query"><code class="literal">match_phrase</code></a>. Rather than creating shingles
using the <code class="literal">shingles</code> filter, we recommend you use the
<a class="xref" href="mapping-params.html#index-phrases" title="index_phrases"><code class="literal">index-phrases</code></a> mapping parameter on the appropriate
<a class="xref" href="mapping-types.html#text" title="Text field type">text</a> field instead.</p>
</div>
</div>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html" class="ulink" target="_top">ShingleFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-shingle-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/shingle-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">shingle</code>
filter to add two-word shingles to the token stream for <code class="literal">quick brown fox jumps</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [ "shingle" ],
  "text": "quick brown fox jumps"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/488.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ quick, quick brown, brown, brown fox, fox, fox jumps, jumps ]</pre>
</div>
<p>To produce shingles of 2-3 words, add the following arguments to the analyze API
request:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">min_shingle_size</code>: <code class="literal">2</code>
</li>
<li class="listitem">
<code class="literal">max_shingle_size</code>: <code class="literal">3</code>
</li>
</ul>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "shingle",
      "min_shingle_size": 2,
      "max_shingle_size": 3
    }
  ],
  "text": "quick brown fox jumps"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/489.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ quick, quick brown, quick brown fox, brown, brown fox, brown fox jumps, fox, fox jumps, jumps ]</pre>
</div>
<p>To only include shingles in the output, add an <code class="literal">output_unigrams</code> argument of
<code class="literal">false</code> to the request.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "shingle",
      "min_shingle_size": 2,
      "max_shingle_size": 3,
      "output_unigrams": false
    }
  ],
  "text": "quick brown fox jumps"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/490.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ quick brown, quick brown fox, brown fox, brown fox jumps, fox jumps ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-shingle-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/shingle-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">shingle</code> filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom
analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_shingle": {
          "tokenizer": "standard",
          "filter": [ "shingle" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/491.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-shingle-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/shingle-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">max_shingle_size</code>
</span>
</dt>
<dd>
<p>
(Optional, integer)
Maximum number of tokens to concatenate when creating shingles. Defaults to <code class="literal">2</code>.
</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>This value cannot be lower than the <code class="literal">min_shingle_size</code> argument, which
defaults to <code class="literal">2</code>. The difference between this value and the <code class="literal">min_shingle_size</code>
argument cannot exceed the <a class="xref" href="index-modules.html#index-max-shingle-diff"><code class="literal">index.max_shingle_diff</code></a>
index-level setting, which defaults to <code class="literal">3</code>.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">min_shingle_size</code>
</span>
</dt>
<dd>
<p>
(Optional, integer)
Minimum number of tokens to concatenate when creating shingles. Defaults to <code class="literal">2</code>.
</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>This value cannot exceed the <code class="literal">max_shingle_size</code> argument, which defaults
to <code class="literal">2</code>. The difference between the <code class="literal">max_shingle_size</code> argument and this value
cannot exceed the <a class="xref" href="index-modules.html#index-max-shingle-diff"><code class="literal">index.max_shingle_diff</code></a>
index-level setting, which defaults to <code class="literal">3</code>.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">output_unigrams</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the output includes the original input tokens. If <code class="literal">false</code>, the output
only includes shingles; the original input tokens are removed. Defaults to
<code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">output_unigrams_if_no_shingles</code>
</span>
</dt>
<dd>
<p>
If <code class="literal">true</code>, the output includes the original input tokens only if no shingles are
produced; if shingles are produced, the output only includes shingles. Defaults
to <code class="literal">false</code>.
</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>If both this and the <code class="literal">output_unigrams</code> parameter are <code class="literal">true</code>, only the
<code class="literal">output_unigrams</code> argument is used.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">token_separator</code>
</span>
</dt>
<dd>
(Optional, string)
Separator used to concatenate adjacent tokens to form a shingle. Defaults to a
space (<code class="literal">" "</code>).
</dd>
<dt>
<span class="term">
<code class="literal">filler_token</code>
</span>
</dt>
<dd>
<p>(Optional, string)
String used in shingles as a replacement for empty positions that do not contain
a token. This filler token is only used in shingles, not original unigrams.
Defaults to an underscore (<code class="literal">_</code>).</p>
<p>Some token filters, such as the <code class="literal">stop</code> filter, create empty positions when
removing stop words with a position increment greater than one.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Example</strong></span></summary>
<div class="content">
<p>In the following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request, the <code class="literal">stop</code> filter
removes the stop word <code class="literal">a</code> from <code class="literal">fox jumps a lazy dog</code>, creating an empty
position. The subsequent <code class="literal">shingle</code> filter replaces this empty position with a
plus sign (<code class="literal">+</code>) in shingles.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    {
      "type": "stop",
      "stopwords": [ "a" ]
    },
    {
      "type": "shingle",
      "filler_token": "+"
    }
  ],
  "text": "fox jumps a lazy dog"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/492.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ fox, fox jumps, jumps, jumps +, + lazy, lazy, lazy dog, dog ]</pre>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-shingle-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/shingle-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">shingle</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request
uses a custom <code class="literal">shingle</code> filter, <code class="literal">my_shingle_filter</code>, to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<p>The <code class="literal">my_shingle_filter</code> filter uses a <code class="literal">min_shingle_size</code> of <code class="literal">2</code> and a
<code class="literal">max_shingle_size</code> of <code class="literal">5</code>, meaning it produces shingles of 2-5 words.
The filter also includes a <code class="literal">output_unigrams</code> argument of <code class="literal">false</code>, meaning that
only shingles are included in the output.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "en": {
          "tokenizer": "standard",
          "filter": [ "my_shingle_filter" ]
        }
      },
      "filter": {
        "my_shingle_filter": {
          "type": "shingle",
          "min_shingle_size": 2,
          "max_shingle_size": 5,
          "output_unigrams": false
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/493.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-snowball-tokenfilter"></a>Snowball token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/snowball-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>A filter that stems words using a Snowball-generated stemmer. The
<code class="literal">language</code> parameter controls the stemmer with the following available
values: <code class="literal">Arabic</code>, <code class="literal">Armenian</code>, <code class="literal">Basque</code>, <code class="literal">Catalan</code>, <code class="literal">Danish</code>, <code class="literal">Dutch</code>, <code class="literal">English</code>,
<code class="literal">Estonian</code>, <code class="literal">Finnish</code>, <code class="literal">French</code>, <code class="literal">German</code>, <code class="literal">German2</code>, <code class="literal">Hungarian</code>, <code class="literal">Italian</code>, <code class="literal">Irish</code>, <code class="literal">Kp</code>,
<code class="literal">Lithuanian</code>, <code class="literal">Lovins</code>, <code class="literal">Norwegian</code>, <code class="literal">Porter</code>, <code class="literal">Portuguese</code>, <code class="literal">Romanian</code>,
<code class="literal">Russian</code>, <code class="literal">Spanish</code>, <code class="literal">Swedish</code>, <code class="literal">Turkish</code>.</p>
<p>For example:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "lowercase", "my_snow" ]
        }
      },
      "filter": {
        "my_snow": {
          "type": "snowball",
          "language": "Lovins"
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/494.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-stemmer-tokenfilter"></a>Stemmer token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stemmer-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Provides <a class="xref" href="analysis-concepts.html#algorithmic-stemmers" title="Algorithmic stemmers">algorithmic stemming</a> for several languages,
some with additional variants. For a list of supported languages, see the
<a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter-language-parm"><code class="literal">language</code></a> parameter.</p>
<p>When not customized, the filter uses the
<a href="https://snowballstem.org/algorithms/porter/stemmer.html" class="ulink" target="_top">porter stemming
algorithm</a> for English.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-stemmer-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stemmer-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following analyze API request uses the <code class="literal">stemmer</code> filter&#8217;s default porter
stemming algorithm to stem <code class="literal">the foxes jumping quickly</code> to <code class="literal">the fox jump
quickli</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [ "stemmer" ],
  "text": "the foxes jumping quickly"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/495.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, fox, jump, quickli ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-stemmer-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stemmer-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">stemmer</code> filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom
analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [ "stemmer" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/496.console"></div>
</div>

<div class="section child_attributes">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-stemmer-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stemmer-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<a id="analysis-stemmer-tokenfilter-language-parm"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">language</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Language-dependent stemming algorithm used to stem tokens. If both this and the
<code class="literal">name</code> parameter are specified, the <code class="literal">language</code> parameter argument is used.
</p>
<details open>
<summary class="title">Valid values for <code class="literal">language</code></summary>
<div class="content">
<p>Valid values are sorted by language. Defaults to
<a href="https://snowballstem.org/algorithms/porter/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">english</code></strong></span></a>.
Recommended algorithms are <span class="strong strong"><strong>bolded</strong></span>.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
Arabic
</span>
</dt>
<dd>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/ar/ArabicStemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">arabic</code></strong></span></a>
</dd>
<dt>
<span class="term">
Armenian
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/armenian/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">armenian</code></strong></span></a>
</dd>
<dt>
<span class="term">
Basque
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/basque/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">basque</code></strong></span></a>
</dd>
<dt>
<span class="term">
Bengali
</span>
</dt>
<dd>
<a href="https://www.tandfonline.com/doi/abs/10.1080/02564602.1993.11437284" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">bengali</code></strong></span></a>
</dd>
<dt>
<span class="term">
Brazilian Portuguese
</span>
</dt>
<dd>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/br/BrazilianStemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">brazilian</code></strong></span></a>
</dd>
<dt>
<span class="term">
Bulgarian
</span>
</dt>
<dd>
<a href="http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">bulgarian</code></strong></span></a>
</dd>
<dt>
<span class="term">
Catalan
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/catalan/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">catalan</code></strong></span></a>
</dd>
<dt>
<span class="term">
Czech
</span>
</dt>
<dd>
<a href="https://dl.acm.org/doi/10.1016/j.ipm.2009.06.001" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">czech</code></strong></span></a>
</dd>
<dt>
<span class="term">
Danish
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/danish/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">danish</code></strong></span></a>
</dd>
<dt>
<span class="term">
Dutch
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/dutch/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">dutch</code></strong></span></a>,
<a href="https://snowballstem.org/algorithms/kraaij_pohlmann/stemmer.html" class="ulink" target="_top"><code class="literal">dutch_kp</code></a>
</dd>
<dt>
<span class="term">
English
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/porter/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">english</code></strong></span></a>,
<a href="https://ciir.cs.umass.edu/pubfiles/ir-35.pdf" class="ulink" target="_top"><code class="literal">light_english</code></a>,
<a href="https://snowballstem.org/algorithms/lovins/stemmer.html" class="ulink" target="_top"><code class="literal">lovins</code></a>,
<a href="https://www.researchgate.net/publication/220433848_How_effective_is_suffixing" class="ulink" target="_top"><code class="literal">minimal_english</code></a>,
<a href="https://snowballstem.org/algorithms/english/stemmer.html" class="ulink" target="_top"><code class="literal">porter2</code></a>,
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/en/EnglishPossessiveFilter.html" class="ulink" target="_top"><code class="literal">possessive_english</code></a>
</dd>
<dt>
<span class="term">
Estonian
</span>
</dt>
<dd>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/tartarus/snowball/ext/EstonianStemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">estonian</code></strong></span></a>
</dd>
<dt>
<span class="term">
Finnish
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/finnish/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">finnish</code></strong></span></a>,
<a href="http://clef.isti.cnr.it/2003/WN_web/22.pdf" class="ulink" target="_top"><code class="literal">light_finnish</code></a>
</dd>
<dt>
<span class="term">
French
</span>
</dt>
<dd>
<a href="https://dl.acm.org/citation.cfm?id=1141523" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">light_french</code></strong></span></a>,
<a href="https://snowballstem.org/algorithms/french/stemmer.html" class="ulink" target="_top"><code class="literal">french</code></a>,
<a href="https://dl.acm.org/citation.cfm?id=318984" class="ulink" target="_top"><code class="literal">minimal_french</code></a>
</dd>
<dt>
<span class="term">
Galician
</span>
</dt>
<dd>
<a href="http://bvg.udc.es/recursos_lingua/stemming.jsp" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">galician</code></strong></span></a>,
<a href="http://bvg.udc.es/recursos_lingua/stemming.jsp" class="ulink" target="_top"><code class="literal">minimal_galician</code></a> (Plural step only)
</dd>
<dt>
<span class="term">
German
</span>
</dt>
<dd>
<a href="https://dl.acm.org/citation.cfm?id=1141523" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">light_german</code></strong></span></a>,
<a href="https://snowballstem.org/algorithms/german/stemmer.html" class="ulink" target="_top"><code class="literal">german</code></a>,
<a href="https://snowballstem.org/algorithms/german2/stemmer.html" class="ulink" target="_top"><code class="literal">german2</code></a>,
<a href="http://members.unine.ch/jacques.savoy/clef/morpho.pdf" class="ulink" target="_top"><code class="literal">minimal_german</code></a>
</dd>
<dt>
<span class="term">
Greek
</span>
</dt>
<dd>
<a href="https://sais.se/mthprize/2007/ntais2007.pdf" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">greek</code></strong></span></a>
</dd>
<dt>
<span class="term">
Hindi
</span>
</dt>
<dd>
<a href="http://computing.open.ac.uk/Sites/EACLSouthAsia/Papers/p6-Ramanathan.pdf" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">hindi</code></strong></span></a>
</dd>
<dt>
<span class="term">
Hungarian
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/hungarian/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">hungarian</code></strong></span></a>,
<a href="https://dl.acm.org/citation.cfm?id=1141523&amp;dl=ACM&amp;coll=DL&amp;CFID=179095584&amp;CFTOKEN=80067181" class="ulink" target="_top"><code class="literal">light_hungarian</code></a>
</dd>
<dt>
<span class="term">
Indonesian
</span>
</dt>
<dd>
<a href="http://www.illc.uva.nl/Publications/ResearchReports/MoL-2003-02.text.pdf" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">indonesian</code></strong></span></a>
</dd>
<dt>
<span class="term">
Irish
</span>
</dt>
<dd>
<a href="https://snowballstem.org/otherapps/oregan/" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">irish</code></strong></span></a>
</dd>
<dt>
<span class="term">
Italian
</span>
</dt>
<dd>
<a href="https://www.ercim.eu/publication/ws-proceedings/CLEF2/savoy.pdf" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">light_italian</code></strong></span></a>,
<a href="https://snowballstem.org/algorithms/italian/stemmer.html" class="ulink" target="_top"><code class="literal">italian</code></a>
</dd>
<dt>
<span class="term">
Kurdish (Sorani)
</span>
</dt>
<dd>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/ckb/SoraniStemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">sorani</code></strong></span></a>
</dd>
<dt>
<span class="term">
Latvian
</span>
</dt>
<dd>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/lv/LatvianStemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">latvian</code></strong></span></a>
</dd>
<dt>
<span class="term">
Lithuanian
</span>
</dt>
<dd>
<a href="https://svn.apache.org/viewvc/lucene/dev/branches/lucene_solr_5_3/lucene/analysis/common/src/java/org/apache/lucene/analysis/lt/stem_ISO_8859_1.sbl?view=markup" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">lithuanian</code></strong></span></a>
</dd>
<dt>
<span class="term">
Norwegian (Bokmål)
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/norwegian/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">norwegian</code></strong></span></a>,
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/no/NorwegianLightStemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">light_norwegian</code></strong></span></a>,
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/no/NorwegianMinimalStemmer.html" class="ulink" target="_top"><code class="literal">minimal_norwegian</code></a>
</dd>
<dt>
<span class="term">
Norwegian (Nynorsk)
</span>
</dt>
<dd>
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/no/NorwegianLightStemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">light_nynorsk</code></strong></span></a>,
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/no/NorwegianMinimalStemmer.html" class="ulink" target="_top"><code class="literal">minimal_nynorsk</code></a>
</dd>
<dt>
<span class="term">
Portuguese
</span>
</dt>
<dd>
<a href="https://dl.acm.org/citation.cfm?id=1141523&amp;dl=ACM&amp;coll=DL&amp;CFID=179095584&amp;CFTOKEN=80067181" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">light_portuguese</code></strong></span></a>,
<a href="http://www.inf.ufrgs.br/~buriol/papers/Orengo_CLEF07.pdf" class="ulink" target="_top"><code class="literal">minimal_portuguese</code></a>,
<a href="https://snowballstem.org/algorithms/portuguese/stemmer.html" class="ulink" target="_top"><code class="literal">portuguese</code></a>,
<a href="https://www.inf.ufrgs.br/\~viviane/rslp/index.htm" class="ulink" target="_top"><code class="literal">portuguese_rslp</code></a>
</dd>
<dt>
<span class="term">
Romanian
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/romanian/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">romanian</code></strong></span></a>
</dd>
<dt>
<span class="term">
Russian
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/russian/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">russian</code></strong></span></a>,
<a href="https://doc.rero.ch/lm.php?url=1000%2C43%2C4%2C20091209094227-CA%2FDolamic_Ljiljana_-_Indexing_and_Searching_Strategies_for_the_Russian_20091209.pdf" class="ulink" target="_top"><code class="literal">light_russian</code></a>
</dd>
<dt>
<span class="term">
Spanish
</span>
</dt>
<dd>
<a href="https://www.ercim.eu/publication/ws-proceedings/CLEF2/savoy.pdf" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">light_spanish</code></strong></span></a>,
<a href="https://snowballstem.org/algorithms/spanish/stemmer.html" class="ulink" target="_top"><code class="literal">spanish</code></a>
</dd>
<dt>
<span class="term">
Swedish
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/swedish/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">swedish</code></strong></span></a>,
<a href="http://clef.isti.cnr.it/2003/WN_web/22.pdf" class="ulink" target="_top"><code class="literal">light_swedish</code></a>
</dd>
<dt>
<span class="term">
Turkish
</span>
</dt>
<dd>
<a href="https://snowballstem.org/algorithms/turkish/stemmer.html" class="ulink" target="_top"><span class="strong strong"><strong><code class="literal">turkish</code></strong></span></a>
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">name</code>
</span>
</dt>
<dd>
An alias for the <a class="xref" href="analysis-tokenfilters.html#analysis-stemmer-tokenfilter-language-parm"><code class="literal">language</code></a>
parameter. If both this and the <code class="literal">language</code> parameter are specified, the
<code class="literal">language</code> parameter argument is used.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-stemmer-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stemmer-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">stemmer</code> filter, duplicate it to create the basis for a new
custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">stemmer</code> filter that stems
words using the <code class="literal">light_german</code> algorithm:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "my_stemmer"
          ]
        }
      },
      "filter": {
        "my_stemmer": {
          "type": "stemmer",
          "language": "light_german"
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/497.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-stemmer-override-tokenfilter"></a>Stemmer override token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stemmer-override-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Overrides stemming algorithms, by applying a custom mapping, then
protecting these terms from being modified by stemmers. Must be placed
before any stemming filters.</p>
<p>Rules are mappings in the form of <code class="literal">token1[, ..., tokenN] =&gt; override</code>.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Setting</th>
<th align="left" valign="top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">rules</code></p></td>
<td align="left" valign="top"><p>A list of mapping rules to use.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">rules_path</code></p></td>
<td align="left" valign="top"><p>A path (either relative to <code class="literal">config</code> location, or
absolute) to a list of mappings.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Here is an example:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "lowercase", "custom_stems", "porter_stem" ]
        }
      },
      "filter": {
        "custom_stems": {
          "type": "stemmer_override",
          "rules_path": "analysis/stemmer_override.txt"
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/498.console"></div>
<p>Where the file looks like:</p>
<div class="pre_wrapper lang-stemmer_override">
<pre class="programlisting prettyprint lang-stemmer_override">running, runs =&gt; run

stemmer =&gt; stemmer</pre>
</div>
<p>You can also define the overrides rules inline:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [ "lowercase", "custom_stems", "porter_stem" ]
        }
      },
      "filter": {
        "custom_stems": {
          "type": "stemmer_override",
          "rules": [
            "running, runs =&gt; run",
            "stemmer =&gt; stemmer"
          ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/499.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-stop-tokenfilter"></a>Stop token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stop-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Removes <a href="https://en.wikipedia.org/wiki/Stop_words" class="ulink" target="_top">stop words</a> from a token
stream.</p>
<p>When not customized, the filter removes the following English stop words by
default:</p>
<p><code class="literal">a</code>, <code class="literal">an</code>, <code class="literal">and</code>, <code class="literal">are</code>, <code class="literal">as</code>, <code class="literal">at</code>, <code class="literal">be</code>, <code class="literal">but</code>, <code class="literal">by</code>, <code class="literal">for</code>, <code class="literal">if</code>, <code class="literal">in</code>,
<code class="literal">into</code>, <code class="literal">is</code>, <code class="literal">it</code>, <code class="literal">no</code>, <code class="literal">not</code>, <code class="literal">of</code>, <code class="literal">on</code>, <code class="literal">or</code>, <code class="literal">such</code>, <code class="literal">that</code>, <code class="literal">the</code>,
<code class="literal">their</code>, <code class="literal">then</code>, <code class="literal">there</code>, <code class="literal">these</code>, <code class="literal">they</code>, <code class="literal">this</code>, <code class="literal">to</code>, <code class="literal">was</code>, <code class="literal">will</code>, <code class="literal">with</code></p>
<p>In addition to English, the <code class="literal">stop</code> filter supports predefined
<a class="xref" href="analysis-tokenfilters.html#analysis-stop-tokenfilter-stop-words-by-lang" title="Stop words by language">stop word lists for several
languages</a>. You can also specify your own stop words as an array or file.</p>
<p>The <code class="literal">stop</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/core/org/apache/lucene/analysis/StopFilter.html" class="ulink" target="_top">StopFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-stop-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stop-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following analyze API request uses the <code class="literal">stop</code> filter to remove the stop words
<code class="literal">a</code> and <code class="literal">the</code> from <code class="literal">a quick fox jumps over the lazy dog</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "standard",
  "filter": [ "stop" ],
  "text": "a quick fox jumps over the lazy dog"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/500.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ quick, fox, jumps, over, lazy, dog ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-stop-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stop-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the <code class="literal">stop</code>
filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [ "stop" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/501.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-stop-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stop-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">stopwords</code>
</span>
</dt>
<dd>
<p>(Optional, string or array of strings)
Language value, such as <code class="literal">_arabic_</code> or <code class="literal">_thai_</code>. Defaults to
<a class="xref" href="analysis-tokenfilters.html#english-stop-words"><code class="literal">_english_</code></a>.</p>
<p>Each language value corresponds to a predefined list of stop words in Lucene.
See <a class="xref" href="analysis-tokenfilters.html#analysis-stop-tokenfilter-stop-words-by-lang" title="Stop words by language">Stop words by language</a> for supported language
values and their stop words.</p>
<p>Also accepts an array of stop words.</p>
<p>For an empty list of stop words, use <code class="literal">_none_</code>.</p>
</dd>
<dt>
<span class="term">
<code class="literal">stopwords_path</code>
</span>
</dt>
<dd>
<p>(Optional, string)
Path to a file that contains a list of stop words to remove.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location, and the file
must be UTF-8 encoded. Each stop word in the file must be separated by a line
break.</p>
</dd>
<dt>
<span class="term">
<code class="literal">ignore_case</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, stop word matching is case insensitive. For example, if <code class="literal">true</code>, a
stop word of <code class="literal">the</code> matches and removes <code class="literal">The</code>, <code class="literal">THE</code>, or <code class="literal">the</code>. Defaults to
<code class="literal">false</code>.
</dd>
<dt>
<span class="term">
<code class="literal">remove_trailing</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the last token of a stream is removed if it&#8217;s a stop word. Defaults
to <code class="literal">true</code>.</p>
<p>This parameter should be <code class="literal">false</code> when using the filter with a
<a class="xref" href="search.html#completion-suggester" title="Completion Suggester">completion suggester</a>. This would ensure a query like
<code class="literal">green a</code> matches and suggests <code class="literal">green apple</code> while still removing other stop
words.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-stop-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stop-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">stop</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom case-insensitive <code class="literal">stop</code>
filter that removes stop words from the <a class="xref" href="analysis-tokenfilters.html#english-stop-words"><code class="literal">_english_</code></a> stop
words list:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "my_custom_stop_words_filter" ]
        }
      },
      "filter": {
        "my_custom_stop_words_filter": {
          "type": "stop",
          "ignore_case": true
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/502.console"></div>
<p>You can also specify your own list of stop words. For example, the following
request creates a custom case-sensitive <code class="literal">stop</code> filter that removes only the stop
words <code class="literal">and</code>, <code class="literal">is</code>, and <code class="literal">the</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "my_custom_stop_words_filter" ]
        }
      },
      "filter": {
        "my_custom_stop_words_filter": {
          "type": "stop",
          "ignore_case": true,
          "stopwords": [ "and", "is", "the" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/503.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-stop-tokenfilter-stop-words-by-lang"></a>Stop words by language<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/stop-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following list contains supported language values for the <code class="literal">stopwords</code>
parameter and a link to their predefined stop words in Lucene.</p>
<div class="variablelist">
<a id="arabic-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_arabic_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ar/stopwords.txt" class="ulink" target="_top">Arabic stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="armenian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_armenian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/hy/stopwords.txt" class="ulink" target="_top">Armenian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="basque-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_basque_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/eu/stopwords.txt" class="ulink" target="_top">Basque stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="bengali-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_bengali_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/bn/stopwords.txt" class="ulink" target="_top">Bengali stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="brazilian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_brazilian_</code> (Brazilian Portuguese)
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/br/stopwords.txt" class="ulink" target="_top">Brazilian Portuguese stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="bulgarian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_bulgarian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/bg/stopwords.txt" class="ulink" target="_top">Bulgarian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="catalan-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_catalan_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ca/stopwords.txt" class="ulink" target="_top">Catalan stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="cjk-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_cjk_</code> (Chinese, Japanese, and Korean)
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/cjk/stopwords.txt" class="ulink" target="_top">CJK stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="czech-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_czech_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/cz/stopwords.txt" class="ulink" target="_top">Czech stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="danish-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_danish_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/danish_stop.txt" class="ulink" target="_top">Danish stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="dutch-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_dutch_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/dutch_stop.txt" class="ulink" target="_top">Dutch stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="english-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_english_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java#L46" class="ulink" target="_top">English stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="estonian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_estonian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/et/stopwords.txt" class="ulink" target="_top">Estonian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="finnish-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_finnish_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/finnish_stop.txt" class="ulink" target="_top">Finnish stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="french-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_french_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/french_stop.txt" class="ulink" target="_top">French stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="galician-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_galician_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/gl/stopwords.txt" class="ulink" target="_top">Galician stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="german-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_german_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/german_stop.txt" class="ulink" target="_top">German stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="greek-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_greek_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/el/stopwords.txt" class="ulink" target="_top">Greek stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="hindi-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_hindi_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/hi/stopwords.txt" class="ulink" target="_top">Hindi stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="hungarian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_hungarian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/hungarian_stop.txt" class="ulink" target="_top">Hungarian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="indonesian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_indonesian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/id/stopwords.txt" class="ulink" target="_top">Indonesian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="irish-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_irish_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ga/stopwords.txt" class="ulink" target="_top">Irish stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="italian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_italian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/italian_stop.txt" class="ulink" target="_top">Italian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="latvian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_latvian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/lv/stopwords.txt" class="ulink" target="_top">Latvian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="lithuanian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_lithuanian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/lt/stopwords.txt" class="ulink" target="_top">Lithuanian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="norwegian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_norwegian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/norwegian_stop.txt" class="ulink" target="_top">Norwegian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="persian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_persian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/fa/stopwords.txt" class="ulink" target="_top">Persian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="portuguese-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_portuguese_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/portuguese_stop.txt" class="ulink" target="_top">Portuguese stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="romanian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_romanian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ro/stopwords.txt" class="ulink" target="_top">Romanian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="russian-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_russian_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/russian_stop.txt" class="ulink" target="_top">Russian stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="sorani-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_sorani_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ckb/stopwords.txt" class="ulink" target="_top">Sorani stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="spanish-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_spanish_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/spanish_stop.txt" class="ulink" target="_top">Spanish stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="swedish-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_swedish_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/swedish_stop.txt" class="ulink" target="_top">Swedish stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="thai-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_thai_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/th/stopwords.txt" class="ulink" target="_top">Thai stop words</a>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="turkish-stop-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">_turkish_</code>
</span>
</dt>
<dd>
<a href="https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/resources/org/apache/lucene/analysis/tr/stopwords.txt" class="ulink" target="_top">Turkish stop words</a>
</dd>
</dl>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-synonym-tokenfilter"></a>Synonym token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">synonym</code> token filter allows to easily handle synonyms during the
analysis process. Synonyms are configured using a configuration file.
Here is an example:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "synonym": {
            "tokenizer": "whitespace",
            "filter": [ "synonym" ]
          }
        },
        "filter": {
          "synonym": {
            "type": "synonym",
            "synonyms_path": "analysis/synonym.txt"
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/504.console"></div>
<p>The above configures a <code class="literal">synonym</code> filter, with a path of
<code class="literal">analysis/synonym.txt</code> (relative to the <code class="literal">config</code> location). The
<code class="literal">synonym</code> analyzer is then configured with the filter.</p>
<p>This filter tokenizes synonyms with whatever tokenizer and token filters
appear before it in the chain.</p>
<p>Additional settings are:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">expand</code> (defaults to <code class="literal">true</code>).
</li>
<li class="listitem">
<code class="literal">lenient</code> (defaults to <code class="literal">false</code>). If <code class="literal">true</code> ignores exceptions while parsing the synonym configuration. It is important
to note that only those synonym rules which cannot get parsed are ignored. For instance consider the following request:
</li>
</ul>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "synonym": {
            "tokenizer": "standard",
            "filter": [ "my_stop", "synonym" ]
          }
        },
        "filter": {
          "my_stop": {
            "type": "stop",
            "stopwords": [ "bar" ]
          },
          "synonym": {
            "type": "synonym",
            "lenient": true,
            "synonyms": [ "foo, bar =&gt; baz" ]
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/505.console"></div>
<p>With the above request the word <code class="literal">bar</code> gets skipped but a mapping <code class="literal">foo =&gt; baz</code> is still added. However, if the mapping
being added was <code class="literal">foo, baz =&gt; bar</code> nothing would get added to the synonym list. This is because the target word for the
mapping is itself eliminated because it was a stop word. Similarly, if the mapping was "bar, foo, baz" and <code class="literal">expand</code> was
set to <code class="literal">false</code> no mapping would get added as when <code class="literal">expand=false</code> the target mapping is the first word. However, if
<code class="literal">expand=true</code> then the mappings added would be equivalent to <code class="literal">foo, baz =&gt; foo, baz</code> i.e, all mappings other than the
stop word.</p>
<h4><a id="synonym-tokenizer-ignore_case-deprecated"></a><code class="literal">tokenizer</code> and <code class="literal">ignore_case</code> are deprecated<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-tokenfilter.asciidoc">edit</a></h4>
<p>The <code class="literal">tokenizer</code> parameter controls the tokenizers that will be used to
tokenize the synonym, this parameter is for backwards compatibility for indices that created before 6.0.
The <code class="literal">ignore_case</code> parameter works with <code class="literal">tokenizer</code> parameter only.</p>
<p>Two synonym formats are supported: Solr, WordNet.</p>
<h4><a id="_solr_synonyms"></a>Solr synonyms<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-tokenfilter.asciidoc">edit</a></h4>
<p>The following is a sample format of the file:</p>
<div class="pre_wrapper lang-synonyms">
<pre class="programlisting prettyprint lang-synonyms"># Blank lines and lines starting with pound are comments.

# Explicit mappings match any token sequence on the LHS of "=&gt;"
# and replace with all alternatives on the RHS.  These types of mappings
# ignore the expand parameter in the schema.
# Examples:
i-pod, i pod =&gt; ipod
sea biscuit, sea biscit =&gt; seabiscuit

# Equivalent synonyms may be separated with commas and give
# no explicit mapping.  In this case the mapping behavior will
# be taken from the expand parameter in the schema.  This allows
# the same synonym file to be used in different synonym handling strategies.
# Examples:
ipod, i-pod, i pod
foozball , foosball
universe , cosmos
lol, laughing out loud

# If expand==true, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod, i-pod, i pod
# If expand==false, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod

# Multiple synonym mapping entries are merged.
foo =&gt; foo bar
foo =&gt; baz
# is equivalent to
foo =&gt; foo bar, baz</pre>
</div>
<p>You can also define synonyms for the filter directly in the
configuration file (note use of <code class="literal">synonyms</code> instead of <code class="literal">synonyms_path</code>):</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym",
            "synonyms": [
              "i-pod, i pod =&gt; ipod",
              "universe, cosmos"
            ]
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/506.console"></div>
<p>However, it is recommended to define large synonyms set in a file using
<code class="literal">synonyms_path</code>, because specifying them inline increases cluster size unnecessarily.</p>
<h4><a id="_wordnet_synonyms"></a>WordNet synonyms<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-tokenfilter.asciidoc">edit</a></h4>
<p>Synonyms based on <a href="https://wordnet.princeton.edu/" class="ulink" target="_top">WordNet</a> format can be
declared using <code class="literal">format</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym",
            "format": "wordnet",
            "synonyms": [
              "s(100000001,1,'abstain',v,1,0).",
              "s(100000001,2,'refrain',v,1,0).",
              "s(100000001,3,'desist',v,1,0)."
            ]
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/507.console"></div>
<p>Using <code class="literal">synonyms_path</code> to define WordNet synonyms in a file is supported
as well.</p>
<h3><a id="_parsing_synonym_files"></a>Parsing synonym files<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-tokenfilter.asciidoc">edit</a></h3>
<p>Elasticsearch will use the token filters preceding the synonym filter
in a tokenizer chain to parse the entries in a synonym file.  So, for example, if a
synonym filter is placed after a stemmer, then the stemmer will also be applied
to the synonym entries.  Because entries in the synonym map cannot have stacked
positions, some token filters may cause issues here.  Token filters that produce
multiple versions of a token may choose which version of the token to emit when
parsing synonyms, e.g. <code class="literal">asciifolding</code> will only produce the folded version of the
token.  Others, e.g. <code class="literal">multiplexer</code>, <code class="literal">word_delimiter_graph</code> or <code class="literal">ngram</code> will throw an
error.</p>
<p>If you need to build analyzers that include both multi-token filters and synonym
filters, consider using the <a class="xref" href="analysis-tokenfilters.html#analysis-multiplexer-tokenfilter" title="Multiplexer token filter">multiplexer</a> filter,
with the multi-token filters in one branch and the synonym filter in the other.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-synonym-graph-tokenfilter"></a>Synonym graph token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-graph-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">synonym_graph</code> token filter allows to easily handle synonyms,
including multi-word synonyms correctly during the analysis process.</p>
<p>In order to properly handle multi-word synonyms this token filter
creates a <a class="xref" href="analysis-concepts.html#token-graphs" title="Token graphs">graph token stream</a> during processing.  For more
information on this topic and its various complexities, please read the
<a href="http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html" class="ulink" target="_top">Lucene&#8217;s TokenStreams are actually graphs</a> blog post.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<a id="synonym-graph-index-note"></a>
<p>This token filter is designed to be used as part of a search analyzer
only.  If you want to apply synonyms during indexing please use the
standard <a class="xref" href="analysis-tokenfilters.html#analysis-synonym-tokenfilter" title="Synonym token filter">synonym token filter</a>.</p>
</div>
</div>
<p>Synonyms are configured using a configuration file.
Here is an example:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "search_synonyms": {
            "tokenizer": "whitespace",
            "filter": [ "graph_synonyms" ]
          }
        },
        "filter": {
          "graph_synonyms": {
            "type": "synonym_graph",
            "synonyms_path": "analysis/synonym.txt"
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/508.console"></div>
<p>The above configures a <code class="literal">search_synonyms</code> filter, with a path of
<code class="literal">analysis/synonym.txt</code> (relative to the <code class="literal">config</code> location). The
<code class="literal">search_synonyms</code> analyzer is then configured with the filter.</p>
<p>Additional settings are:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">expand</code> (defaults to <code class="literal">true</code>).
</li>
<li class="listitem">
<code class="literal">lenient</code> (defaults to <code class="literal">false</code>). If <code class="literal">true</code> ignores exceptions while parsing the synonym configuration. It is important
to note that only those synonym rules which cannot get parsed are ignored. For instance consider the following request:
</li>
</ul>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "synonym": {
            "tokenizer": "standard",
            "filter": [ "my_stop", "synonym_graph" ]
          }
        },
        "filter": {
          "my_stop": {
            "type": "stop",
            "stopwords": [ "bar" ]
          },
          "synonym_graph": {
            "type": "synonym_graph",
            "lenient": true,
            "synonyms": [ "foo, bar =&gt; baz" ]
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/509.console"></div>
<p>With the above request the word <code class="literal">bar</code> gets skipped but a mapping <code class="literal">foo =&gt; baz</code> is still added. However, if the mapping
being added was <code class="literal">foo, baz =&gt; bar</code> nothing would get added to the synonym list. This is because the target word for the
mapping is itself eliminated because it was a stop word. Similarly, if the mapping was "bar, foo, baz" and <code class="literal">expand</code> was
set to <code class="literal">false</code> no mapping would get added as when <code class="literal">expand=false</code> the target mapping is the first word. However, if
<code class="literal">expand=true</code> then the mappings added would be equivalent to <code class="literal">foo, baz =&gt; foo, baz</code> i.e, all mappings other than the
stop word.</p>
<h4><a id="synonym-graph-tokenizer-ignore_case-deprecated"></a><code class="literal">tokenizer</code> and <code class="literal">ignore_case</code> are deprecated<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-graph-tokenfilter.asciidoc">edit</a></h4>
<p>The <code class="literal">tokenizer</code> parameter controls the tokenizers that will be used to
tokenize the synonym, this parameter is for backwards compatibility for indices that created before 6.0..
The <code class="literal">ignore_case</code> parameter works with <code class="literal">tokenizer</code> parameter only.</p>
<p>Two synonym formats are supported: Solr, WordNet.</p>
<h4><a id="_solr_synonyms_2"></a>Solr synonyms<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-graph-tokenfilter.asciidoc">edit</a></h4>
<p>The following is a sample format of the file:</p>
<div class="pre_wrapper lang-synonyms">
<pre class="programlisting prettyprint lang-synonyms"># Blank lines and lines starting with pound are comments.

# Explicit mappings match any token sequence on the LHS of "=&gt;"
# and replace with all alternatives on the RHS.  These types of mappings
# ignore the expand parameter in the schema.
# Examples:
i-pod, i pod =&gt; ipod
sea biscuit, sea biscit =&gt; seabiscuit

# Equivalent synonyms may be separated with commas and give
# no explicit mapping.  In this case the mapping behavior will
# be taken from the expand parameter in the schema.  This allows
# the same synonym file to be used in different synonym handling strategies.
# Examples:
ipod, i-pod, i pod
foozball , foosball
universe , cosmos
lol, laughing out loud

# If expand==true, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod, i-pod, i pod
# If expand==false, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod

# Multiple synonym mapping entries are merged.
foo =&gt; foo bar
foo =&gt; baz
# is equivalent to
foo =&gt; foo bar, baz</pre>
</div>
<p>You can also define synonyms for the filter directly in the
configuration file (note use of <code class="literal">synonyms</code> instead of <code class="literal">synonyms_path</code>):</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym_graph",
            "synonyms": [
              "lol, laughing out loud",
              "universe, cosmos"
            ]
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/510.console"></div>
<p>However, it is recommended to define large synonyms set in a file using
<code class="literal">synonyms_path</code>, because specifying them inline increases cluster size unnecessarily.</p>
<h4><a id="_wordnet_synonyms_2"></a>WordNet synonyms<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-graph-tokenfilter.asciidoc">edit</a></h4>
<p>Synonyms based on <a href="https://wordnet.princeton.edu/" class="ulink" target="_top">WordNet</a> format can be
declared using <code class="literal">format</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym_graph",
            "format": "wordnet",
            "synonyms": [
              "s(100000001,1,'abstain',v,1,0).",
              "s(100000001,2,'refrain',v,1,0).",
              "s(100000001,3,'desist',v,1,0)."
            ]
          }
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/511.console"></div>
<p>Using <code class="literal">synonyms_path</code> to define WordNet synonyms in a file is supported
as well.</p>
<h4><a id="_parsing_synonym_files_2"></a>Parsing synonym files<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/synonym-graph-tokenfilter.asciidoc">edit</a></h4>
<p>Elasticsearch will use the token filters preceding the synonym filter
in a tokenizer chain to parse the entries in a synonym file.  So, for example, if a
synonym filter is placed after a stemmer, then the stemmer will also be applied
to the synonym entries.  Because entries in the synonym map cannot have stacked
positions, some token filters may cause issues here.  Token filters that produce
multiple versions of a token may choose which version of the token to emit when
parsing synonyms, e.g. <code class="literal">asciifolding</code> will only produce the folded version of the
token.  Others, e.g. <code class="literal">multiplexer</code>, <code class="literal">word_delimiter_graph</code> or <code class="literal">ngram</code> will throw an
error.</p>
<p>If you need to build analyzers that include both multi-token filters and synonym
filters, consider using the <a class="xref" href="analysis-tokenfilters.html#analysis-multiplexer-tokenfilter" title="Multiplexer token filter">multiplexer</a> filter,
with the multi-token filters in one branch and the synonym filter in the other.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>The synonym rules should not contain words that are removed by
a filter that appears after in the chain (a <code class="literal">stop</code> filter for instance).
Removing a term from a synonym rule breaks the matching at query time.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-trim-tokenfilter"></a>Trim token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/trim-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Removes leading and trailing whitespace from each token in a stream. While this
can change the length of a token, the <code class="literal">trim</code> filter does <em>not</em> change a token&#8217;s
offsets.</p>
<p>The <code class="literal">trim</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/TrimFilter.html" class="ulink" target="_top">TrimFilter</a>.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>Many commonly used tokenizers, such as the
<a class="xref" href="analysis-tokenizers.html#analysis-standard-tokenizer" title="Standard tokenizer"><code class="literal">standard</code></a> or
<a class="xref" href="analysis-tokenizers.html#analysis-whitespace-tokenizer" title="Whitespace tokenizer"><code class="literal">whitespace</code></a> tokenizer, remove whitespace by
default. When using these tokenizers, you don&#8217;t need to add a separate <code class="literal">trim</code>
filter.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-trim-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/trim-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To see how the <code class="literal">trim</code> filter works, you first need to produce a token
containing whitespace.</p>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<a class="xref" href="analysis-tokenizers.html#analysis-keyword-tokenizer" title="Keyword tokenizer"><code class="literal">keyword</code></a> tokenizer to produce a token for
<code class="literal">" fox "</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer" : "keyword",
  "text" : " fox "
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/512.console"></div>
<p>The API returns the following response. Note the <code class="literal">" fox "</code> token contains the
original text&#8217;s whitespace. Note that despite changing the token&#8217;s length, the
<code class="literal">start_offset</code> and <code class="literal">end_offset</code> remain the same.</p>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "tokens": [
    {
      "token": " fox ",
      "start_offset": 0,
      "end_offset": 5,
      "type": "word",
      "position": 0
    }
  ]
}</pre>
</div>
<p>To remove the whitespace, add the <code class="literal">trim</code> filter to the previous analyze API
request.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer" : "keyword",
  "filter" : ["trim"],
  "text" : " fox "
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/513.console"></div>
<p>The API returns the following response. The returned <code class="literal">fox</code> token does not
include any leading or trailing whitespace.</p>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "tokens": [
    {
      "token": "fox",
      "start_offset": 0,
      "end_offset": 5,
      "type": "word",
      "position": 0
    }
  ]
}</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-trim-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/trim-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the <code class="literal">trim</code>
filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT trim_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "keyword_trim": {
          "tokenizer": "keyword",
          "filter": [ "trim" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/514.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-truncate-tokenfilter"></a>Truncate token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/truncate-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Truncates tokens that exceed a specified character limit. This limit defaults to
<code class="literal">10</code> but can be customized using the <code class="literal">length</code> parameter.</p>
<p>For example, you can use the <code class="literal">truncate</code> filter to shorten all tokens to
<code class="literal">3</code> characters or fewer, changing <code class="literal">jumping fox</code> to <code class="literal">jum fox</code>.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.html" class="ulink" target="_top">TruncateTokenFilter</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-truncate-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/truncate-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">truncate</code> filter
to shorten tokens that exceed 10 characters in
<code class="literal">the quinquennial extravaganza carried on</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["truncate"],
  "text" : "the quinquennial extravaganza carried on"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/515.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, quinquenni, extravagan, carried, on ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-truncate-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/truncate-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">truncate</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT custom_truncate_example
{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "standard_truncate" : {
        "tokenizer" : "standard",
        "filter" : ["truncate"]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/516.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-truncate-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/truncate-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">length</code>
</span>
</dt>
<dd>
(Optional, integer)
Character limit for each token. Tokens exceeding this limit are truncated.
Defaults to <code class="literal">10</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-truncate-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/truncate-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">truncate</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">truncate</code> filter,
<code class="literal">5_char_trunc</code>, that shortens tokens to a <code class="literal">length</code> of <code class="literal">5</code> or fewer characters:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT 5_char_words_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "lowercase_5_char": {
          "tokenizer": "lowercase",
          "filter": [ "5_char_trunc" ]
        }
      },
      "filter": {
        "5_char_trunc": {
          "type": "truncate",
          "length": 5
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/517.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-unique-tokenfilter"></a>Unique token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/unique-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Removes duplicate tokens from a stream. For example, you can use the <code class="literal">unique</code>
filter to change <code class="literal">the lazy lazy dog</code> to <code class="literal">the lazy dog</code>.</p>
<p>If the <code class="literal">only_on_same_position</code> parameter is set to <code class="literal">true</code>, the <code class="literal">unique</code> filter
removes only duplicate tokens <em>in the same position</em>.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>When <code class="literal">only_on_same_position</code> is <code class="literal">true</code>, the <code class="literal">unique</code> filter works the same as
<a class="xref" href="analysis-tokenfilters.html#analysis-remove-duplicates-tokenfilter" title="Remove duplicates token filter"><code class="literal">remove_duplicates</code></a> filter.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-unique-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/unique-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the <code class="literal">unique</code> filter
to remove duplicate tokens from <code class="literal">the quick fox jumps the lazy fox</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["unique"],
  "text" : "the quick fox jumps the lazy fox"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/518.console"></div>
<p>The filter removes duplicated tokens for <code class="literal">the</code> and <code class="literal">fox</code>, producing the
following output:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, quick, fox, jumps, lazy ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-unique-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/unique-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">unique</code> filter to configure a new <a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT custom_unique_example
{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "standard_truncate" : {
        "tokenizer" : "standard",
        "filter" : ["unique"]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/519.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-unique-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/unique-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">only_on_same_position</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, only remove duplicate tokens in the same position.
Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-unique-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/unique-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">unique</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a custom <code class="literal">unique</code> filter with
<code class="literal">only_on_same_position</code> set to <code class="literal">true</code>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT letter_unique_pos_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "letter_unique_pos": {
          "tokenizer": "letter",
          "filter": [ "unique_pos" ]
        }
      },
      "filter": {
        "unique_pos": {
          "type": "unique",
          "only_on_same_position": true
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/520.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-uppercase-tokenfilter"></a>Uppercase token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/uppercase-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Changes token text to uppercase. For example, you can use the <code class="literal">uppercase</code> filter
to change <code class="literal">the Lazy DoG</code> to <code class="literal">THE LAZY DOG</code>.</p>
<p>This filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/core/UpperCaseFilter.html" class="ulink" target="_top">UpperCaseFilter</a>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Depending on the language, an uppercase character can map to multiple
lowercase characters. Using the <code class="literal">uppercase</code> filter could result in the loss of
lowercase character information.</p>
<p>To avoid this loss but still have a consistent letter case, use the
<a class="xref" href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter" title="Lowercase token filter"><code class="literal">lowercase</code></a> filter instead.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-uppercase-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/uppercase-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the default
<code class="literal">uppercase</code> filter to change the <code class="literal">the Quick FoX JUMPs</code> to uppercase:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["uppercase"],
  "text" : "the Quick FoX JUMPs"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/521.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ THE, QUICK, FOX, JUMPS ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-uppercase-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/uppercase-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">uppercase</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT uppercase_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_uppercase": {
          "tokenizer": "whitespace",
          "filter": [ "uppercase" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/522.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-word-delimiter-tokenfilter"></a>Word delimiter token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>We recommend using the
<a class="xref" href="analysis-tokenfilters.html#analysis-word-delimiter-graph-tokenfilter" title="Word delimiter graph token filter"><code class="literal">word_delimiter_graph</code></a> instead of
the <code class="literal">word_delimiter</code> filter.</p>
<p>The <code class="literal">word_delimiter</code> filter can produce invalid token graphs. See
<a class="xref" href="analysis-tokenfilters.html#analysis-word-delimiter-graph-differences" title="Differences between word_delimiter_graph and word_delimiter">Differences between <code class="literal">word_delimiter_graph</code> and <code class="literal">word_delimiter</code></a>.</p>
<p>The <code class="literal">word_delimiter</code> filter also uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.html" class="ulink" target="_top">WordDelimiterFilter</a>,
which is marked as deprecated.</p>
</div>
</div>
<p>Splits tokens at non-alphanumeric characters. The <code class="literal">word_delimiter</code> filter
also performs optional token normalization based on a set of rules. By default,
the filter uses the following rules:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Split tokens at non-alphanumeric characters.
The filter uses these characters as delimiters.
For example: <code class="literal">Super-Duper</code> &#8594; <code class="literal">Super</code>, <code class="literal">Duper</code>
</li>
<li class="listitem">
Remove leading or trailing delimiters from each token.
For example: <code class="literal">XL---42+'Autocoder'</code> &#8594; <code class="literal">XL</code>, <code class="literal">42</code>, <code class="literal">Autocoder</code>
</li>
<li class="listitem">
Split tokens at letter case transitions.
For example: <code class="literal">PowerShot</code> &#8594; <code class="literal">Power</code>, <code class="literal">Shot</code>
</li>
<li class="listitem">
Split tokens at letter-number transitions.
For example: <code class="literal">XL500</code> &#8594; <code class="literal">XL</code>, <code class="literal">500</code>
</li>
<li class="listitem">
Remove the English possessive (<code class="literal">'s</code>) from the end of each token.
For example: <code class="literal">Neil's</code> &#8594; <code class="literal">Neil</code>
</li>
</ul>
</div>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>The <code class="literal">word_delimiter</code> filter was designed to remove punctuation from complex
identifiers, such as product IDs or part numbers. For these use cases, we
recommend using the <code class="literal">word_delimiter</code> filter with the
<a class="xref" href="analysis-tokenizers.html#analysis-keyword-tokenizer" title="Keyword tokenizer"><code class="literal">keyword</code></a> tokenizer.</p>
<p>Avoid using the <code class="literal">word_delimiter</code> filter to split hyphenated words, such as
<code class="literal">wi-fi</code>. Because users often search for these words both with and without
hyphens, we recommend using the
<a class="xref" href="analysis-tokenfilters.html#analysis-synonym-graph-tokenfilter" title="Synonym graph token filter"><code class="literal">synonym_graph</code></a> filter instead.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-word-delimiter-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<code class="literal">word_delimiter</code> filter to split <code class="literal">Neil's-Super-Duper-XL500--42+AutoCoder</code>
into normalized tokens using the filter&#8217;s default rules:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "keyword",
  "filter": [ "word_delimiter" ],
  "text": "Neil's-Super-Duper-XL500--42+AutoCoder"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/523.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">[ Neil, Super, Duper, XL, 500, 42, Auto, Coder ]</pre>
</div>
</div>

<div class="analysis-word-delimiter-tokenfilter-analyzer-ex]">
<div class="titlepage"><div><div>
<h3 class="title"><a id="_add_to_an_analyzer"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">word_delimiter</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [ "word_delimiter" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/524.console"></div>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Avoid using the <code class="literal">word_delimiter</code> filter with tokenizers that remove punctuation,
such as the <a class="xref" href="analysis-tokenizers.html#analysis-standard-tokenizer" title="Standard tokenizer"><code class="literal">standard</code></a> tokenizer. This could
prevent the <code class="literal">word_delimiter</code> filter from splitting tokens correctly. It can also
interfere with the filter&#8217;s configurable parameters, such as <code class="literal">catenate_all</code> or
<code class="literal">preserve_original</code>. We recommend using the
<a class="xref" href="analysis-tokenizers.html#analysis-keyword-tokenizer" title="Keyword tokenizer"><code class="literal">keyword</code></a> or
<a class="xref" href="analysis-tokenizers.html#analysis-whitespace-tokenizer" title="Whitespace tokenizer"><code class="literal">whitespace</code></a> tokenizer instead.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="word-delimiter-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">catenate_all</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the filter produces catenated tokens for chains of alphanumeric
characters separated by non-alphabetic delimiters. For example:
<code class="literal">super-duper-xl-500</code> &#8594; [ <code class="literal">super</code>, <span class="strong strong"><strong><code class="literal">superduperxl500</code></strong></span>, <code class="literal">duper</code>, <code class="literal">xl</code>, <code class="literal">500</code>
]. Defaults to <code class="literal">false</code>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>When used for search analysis, catenated tokens can cause problems for the
<a class="xref" href="full-text-queries.html#query-dsl-match-query-phrase" title="Match phrase query"><code class="literal">match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code class="literal">true</code> if
you plan to use these queries.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">catenate_numbers</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the filter produces catenated tokens for chains of numeric characters
separated by non-alphabetic delimiters. For example: <code class="literal">01-02-03</code> &#8594;
[ <code class="literal">01</code>, <span class="strong strong"><strong><code class="literal">010203</code></strong></span>, <code class="literal">02</code>, <code class="literal">03</code> ]. Defaults to <code class="literal">false</code>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>When used for search analysis, catenated tokens can cause problems for the
<a class="xref" href="full-text-queries.html#query-dsl-match-query-phrase" title="Match phrase query"><code class="literal">match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code class="literal">true</code> if
you plan to use these queries.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">catenate_words</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the filter produces catenated tokens for chains of alphabetical
characters separated by non-alphabetic delimiters. For example: <code class="literal">super-duper-xl</code>
&#8594; [ <code class="literal">super</code>, <span class="strong strong"><strong><code class="literal">superduperxl</code></strong></span>, <code class="literal">duper</code>, <code class="literal">xl</code> ]. Defaults to <code class="literal">false</code>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>When used for search analysis, catenated tokens can cause problems for the
<a class="xref" href="full-text-queries.html#query-dsl-match-query-phrase" title="Match phrase query"><code class="literal">match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code class="literal">true</code> if
you plan to use these queries.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">generate_number_parts</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter includes tokens consisting of only numeric characters in
the output. If <code class="literal">false</code>, the filter excludes these tokens from the output.
Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">generate_word_parts</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter includes tokens consisting of only alphabetical characters
in the output. If <code class="literal">false</code>, the filter excludes these tokens from the output.
Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">preserve_original</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter includes the original version of any split tokens in the
output. This original version includes non-alphanumeric delimiters. For example:
<code class="literal">super-duper-xl-500</code> &#8594; [ <span class="strong strong"><strong><code class="literal">super-duper-xl-500</code></strong></span>, <code class="literal">super</code>, <code class="literal">duper</code>, <code class="literal">xl</code>,
<code class="literal">500</code> ]. Defaults to <code class="literal">false</code>.
</dd>
<dt>
<span class="term">
<code class="literal">protected_words</code>
</span>
</dt>
<dd>
(Optional, array of strings)
Array of tokens the filter won&#8217;t split.
</dd>
<dt>
<span class="term">
<code class="literal">protected_words_path</code>
</span>
</dt>
<dd>
<p>(Optional, string)
Path to a file that contains a list of tokens the filter won&#8217;t split.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location, and the file
must be UTF-8 encoded. Each token in the file must be separated by a line
break.</p>
</dd>
<dt>
<span class="term">
<code class="literal">split_on_case_change</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter splits tokens at letter case transitions. For example:
<code class="literal">camelCase</code> &#8594; [ <code class="literal">camel</code>, <code class="literal">Case</code> ]. Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">split_on_numerics</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter splits tokens at letter-number transitions. For example:
<code class="literal">j2se</code> &#8594; [ <code class="literal">j</code>, <code class="literal">2</code>, <code class="literal">se</code> ]. Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">stem_english_possessive</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter removes the English possessive (<code class="literal">'s</code>) from the end of each
token. For example: <code class="literal">O'Neil's</code> &#8594; [ <code class="literal">O</code>, <code class="literal">Neil</code> ]. Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">type_table</code>
</span>
</dt>
<dd>
<p>(Optional, array of strings)
Array of custom type mappings for characters. This allows you to map
non-alphanumeric characters as numeric or alphanumeric to avoid splitting on
those characters.</p>
<p>For example, the following array maps the plus (<code class="literal">+</code>) and hyphen (<code class="literal">-</code>) characters
as alphanumeric, which means they won&#8217;t be treated as delimiters:</p>
<p><code class="literal">[ "+ =&gt; ALPHA", "- =&gt; ALPHA" ]</code></p>
<p>Supported types include:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">ALPHA</code> (Alphabetical)
</li>
<li class="listitem">
<code class="literal">ALPHANUM</code> (Alphanumeric)
</li>
<li class="listitem">
<code class="literal">DIGIT</code> (Numeric)
</li>
<li class="listitem">
<code class="literal">LOWER</code> (Lowercase alphabetical)
</li>
<li class="listitem">
<code class="literal">SUBWORD_DELIM</code> (Non-alphanumeric delimiter)
</li>
<li class="listitem">
<code class="literal">UPPER</code> (Uppercase alphabetical)
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">type_table_path</code>
</span>
</dt>
<dd>
<p>(Optional, string)
Path to a file that contains custom type mappings for characters. This allows
you to map non-alphanumeric characters as numeric or alphanumeric to avoid
splitting on those characters.</p>
<p>For example, the contents of this file may contain the following:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt"># Map the $, %, '.', and ',' characters to DIGIT
# This might be useful for financial data.
$ =&gt; DIGIT
% =&gt; DIGIT
. =&gt; DIGIT
\\u002C =&gt; DIGIT

# in some cases you might not want to split on ZWJ
# this also tests the case where we need a bigger byte[]
# see https://en.wikipedia.org/wiki/Zero-width_joiner
\\u200D =&gt; ALPHANUM</pre>
</div>
<p>Supported types include:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">ALPHA</code> (Alphabetical)
</li>
<li class="listitem">
<code class="literal">ALPHANUM</code> (Alphanumeric)
</li>
<li class="listitem">
<code class="literal">DIGIT</code> (Numeric)
</li>
<li class="listitem">
<code class="literal">LOWER</code> (Lowercase alphabetical)
</li>
<li class="listitem">
<code class="literal">SUBWORD_DELIM</code> (Non-alphanumeric delimiter)
</li>
<li class="listitem">
<code class="literal">UPPER</code> (Uppercase alphabetical)
</li>
</ul>
</div>
<p>This file path must be absolute or relative to the <code class="literal">config</code> location, and the
file must be UTF-8 encoded. Each mapping in the file must be separated by a line
break.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-word-delimiter-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">word_delimiter</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a <code class="literal">word_delimiter</code>
filter that uses the following rules:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Split tokens at non-alphanumeric characters, <em>except</em> the hyphen (<code class="literal">-</code>)
character.
</li>
<li class="listitem">
Remove leading or trailing delimiters from each token.
</li>
<li class="listitem">
Do <em>not</em> split tokens at letter case transitions.
</li>
<li class="listitem">
Do <em>not</em> split tokens at letter-number transitions.
</li>
<li class="listitem">
Remove the English possessive (<code class="literal">'s</code>) from the end of each token.
</li>
</ul>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [ "my_custom_word_delimiter_filter" ]
        }
      },
      "filter": {
        "my_custom_word_delimiter_filter": {
          "type": "word_delimiter",
          "type_table": [ "- =&gt; ALPHA" ],
          "split_on_case_change": false,
          "split_on_numerics": false,
          "stem_english_possessive": true
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/525.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-word-delimiter-graph-tokenfilter"></a>Word delimiter graph token filter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>

<p>Splits tokens at non-alphanumeric characters. The <code class="literal">word_delimiter_graph</code> filter
also performs optional token normalization based on a set of rules. By default,
the filter uses the following rules:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Split tokens at non-alphanumeric characters.
The filter uses these characters as delimiters.
For example: <code class="literal">Super-Duper</code> &#8594; <code class="literal">Super</code>, <code class="literal">Duper</code>
</li>
<li class="listitem">
Remove leading or trailing delimiters from each token.
For example: <code class="literal">XL---42+'Autocoder'</code> &#8594; <code class="literal">XL</code>, <code class="literal">42</code>, <code class="literal">Autocoder</code>
</li>
<li class="listitem">
Split tokens at letter case transitions.
For example: <code class="literal">PowerShot</code> &#8594; <code class="literal">Power</code>, <code class="literal">Shot</code>
</li>
<li class="listitem">
Split tokens at letter-number transitions.
For example: <code class="literal">XL500</code> &#8594; <code class="literal">XL</code>, <code class="literal">500</code>
</li>
<li class="listitem">
Remove the English possessive (<code class="literal">'s</code>) from the end of each token.
For example: <code class="literal">Neil's</code> &#8594; <code class="literal">Neil</code>
</li>
</ul>
</div>
<p>The <code class="literal">word_delimiter_graph</code> filter uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/WordDelimiterGraphFilter.html" class="ulink" target="_top">WordDelimiterGraphFilter</a>.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>The <code class="literal">word_delimiter_graph</code> filter was designed to remove punctuation from
complex identifiers, such as product IDs or part numbers. For these use cases,
we recommend using the <code class="literal">word_delimiter_graph</code> filter with the
<a class="xref" href="analysis-tokenizers.html#analysis-keyword-tokenizer" title="Keyword tokenizer"><code class="literal">keyword</code></a> tokenizer.</p>
<p>Avoid using the <code class="literal">word_delimiter_graph</code> filter to split hyphenated words, such as
<code class="literal">wi-fi</code>. Because users often search for these words both with and without
hyphens, we recommend using the
<a class="xref" href="analysis-tokenfilters.html#analysis-synonym-graph-tokenfilter" title="Synonym graph token filter"><code class="literal">synonym_graph</code></a> filter instead.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-word-delimiter-graph-tokenfilter-analyze-ex"></a>Example<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<code class="literal">word_delimiter_graph</code> filter to split <code class="literal">Neil's-Super-Duper-XL500--42+AutoCoder</code>
into normalized tokens using the filter&#8217;s default rules:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_analyze
{
  "tokenizer": "keyword",
  "filter": [ "word_delimiter_graph" ],
  "text": "Neil's-Super-Duper-XL500--42+AutoCoder"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/526.console"></div>
<p>The filter produces the following tokens:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">[ Neil, Super, Duper, XL, 500, 42, Auto, Coder ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-word-delimiter-graph-tokenfilter-analyzer-ex"></a>Add to an analyzer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>The following <a class="xref" href="indices.html#indices-create-index" title="Create index API">create index API</a> request uses the
<code class="literal">word_delimiter_graph</code> filter to configure a new
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzer</a>.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [ "word_delimiter_graph" ]
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/527.console"></div>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Avoid using the <code class="literal">word_delimiter_graph</code> filter with tokenizers that remove
punctuation, such as the <a class="xref" href="analysis-tokenizers.html#analysis-standard-tokenizer" title="Standard tokenizer"><code class="literal">standard</code></a> tokenizer.
This could prevent the <code class="literal">word_delimiter_graph</code> filter from splitting tokens
correctly. It can also interfere with the filter&#8217;s configurable parameters, such
as <a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-all"><code class="literal">catenate_all</code></a> or
<a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-preserve-original"><code class="literal">preserve_original</code></a>. We
recommend using the <a class="xref" href="analysis-tokenizers.html#analysis-keyword-tokenizer" title="Keyword tokenizer"><code class="literal">keyword</code></a> or
<a class="xref" href="analysis-tokenizers.html#analysis-whitespace-tokenizer" title="Whitespace tokenizer"><code class="literal">whitespace</code></a> tokenizer instead.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="word-delimiter-graph-tokenfilter-configure-parms"></a>Configurable parameters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<a id="word-delimiter-graph-tokenfilter-adjust-offsets"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">adjust_offsets</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the filter adjusts the offsets of split or catenated tokens to better
reflect their actual position in the token stream. Defaults to <code class="literal">true</code>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Set <code class="literal">adjust_offsets</code> to <code class="literal">false</code> if your analyzer uses filters, such as the
<a class="xref" href="analysis-tokenfilters.html#analysis-trim-tokenfilter" title="Trim token filter"><code class="literal">trim</code></a> filter, that change the length of tokens
without changing their offsets. Otherwise, the <code class="literal">word_delimiter_graph</code> filter
could produce tokens with illegal offsets.</p>
</div>
</div>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="word-delimiter-graph-tokenfilter-catenate-all"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">catenate_all</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the filter produces catenated tokens for chains of alphanumeric
characters separated by non-alphabetic delimiters. For example:
<code class="literal">super-duper-xl-500</code> &#8594; [ <span class="strong strong"><strong><code class="literal">superduperxl500</code></strong></span>, <code class="literal">super</code>, <code class="literal">duper</code>, <code class="literal">xl</code>, <code class="literal">500</code> ].
Defaults to <code class="literal">false</code>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Setting this parameter to <code class="literal">true</code> produces multi-position tokens, which are not
supported by indexing.</p>
<p>If this parameter is <code class="literal">true</code>, avoid using this filter in an index analyzer or
use the <a class="xref" href="analysis-tokenfilters.html#analysis-flatten-graph-tokenfilter" title="Flatten graph token filter"><code class="literal">flatten_graph</code></a> filter after
this filter to make the token stream suitable for indexing.</p>
<p>When used for search analysis, catenated tokens can cause problems for the
<a class="xref" href="full-text-queries.html#query-dsl-match-query-phrase" title="Match phrase query"><code class="literal">match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code class="literal">true</code> if
you plan to use these queries.</p>
</div>
</div>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="word-delimiter-graph-tokenfilter-catenate-numbers"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">catenate_numbers</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the filter produces catenated tokens for chains of numeric characters
separated by non-alphabetic delimiters. For example: <code class="literal">01-02-03</code> &#8594;
[ <span class="strong strong"><strong><code class="literal">010203</code></strong></span>, <code class="literal">01</code>, <code class="literal">02</code>, <code class="literal">03</code> ]. Defaults to <code class="literal">false</code>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Setting this parameter to <code class="literal">true</code> produces multi-position tokens, which are not
supported by indexing.</p>
<p>If this parameter is <code class="literal">true</code>, avoid using this filter in an index analyzer or
use the <a class="xref" href="analysis-tokenfilters.html#analysis-flatten-graph-tokenfilter" title="Flatten graph token filter"><code class="literal">flatten_graph</code></a> filter after
this filter to make the token stream suitable for indexing.</p>
<p>When used for search analysis, catenated tokens can cause problems for the
<a class="xref" href="full-text-queries.html#query-dsl-match-query-phrase" title="Match phrase query"><code class="literal">match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code class="literal">true</code> if
you plan to use these queries.</p>
</div>
</div>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="word-delimiter-graph-tokenfilter-catenate-words"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">catenate_words</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the filter produces catenated tokens for chains of alphabetical
characters separated by non-alphabetic delimiters. For example: <code class="literal">super-duper-xl</code>
&#8594; [ <span class="strong strong"><strong><code class="literal">superduperxl</code></strong></span>, <code class="literal">super</code>, <code class="literal">duper</code>, <code class="literal">xl</code> ]. Defaults to <code class="literal">false</code>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Setting this parameter to <code class="literal">true</code> produces multi-position tokens, which are not
supported by indexing.</p>
<p>If this parameter is <code class="literal">true</code>, avoid using this filter in an index analyzer or
use the <a class="xref" href="analysis-tokenfilters.html#analysis-flatten-graph-tokenfilter" title="Flatten graph token filter"><code class="literal">flatten_graph</code></a> filter after
this filter to make the token stream suitable for indexing.</p>
<p>When used for search analysis, catenated tokens can cause problems for the
<a class="xref" href="full-text-queries.html#query-dsl-match-query-phrase" title="Match phrase query"><code class="literal">match_phrase</code></a> query and other queries that
rely on token position for matching. Avoid setting this parameter to <code class="literal">true</code> if
you plan to use these queries.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">generate_number_parts</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter includes tokens consisting of only numeric characters in
the output. If <code class="literal">false</code>, the filter excludes these tokens from the output.
Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">generate_word_parts</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter includes tokens consisting of only alphabetical characters
in the output. If <code class="literal">false</code>, the filter excludes these tokens from the output.
Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">ignore_keywords</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter skips tokens with
a <code class="literal">keyword</code> attribute of <code class="literal">true</code>.
Defaults to <code class="literal">false</code>.
</dd>
</dl>
</div>
<div class="variablelist">
<a id="word-delimiter-graph-tokenfilter-preserve-original"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">preserve_original</code>
</span>
</dt>
<dd>
<p>(Optional, Boolean)
If <code class="literal">true</code>, the filter includes the original version of any split tokens in the
output. This original version includes non-alphanumeric delimiters. For example:
<code class="literal">super-duper-xl-500</code> &#8594; [ <span class="strong strong"><strong><code class="literal">super-duper-xl-500</code></strong></span>, <code class="literal">super</code>, <code class="literal">duper</code>, <code class="literal">xl</code>,
<code class="literal">500</code> ]. Defaults to <code class="literal">false</code>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Setting this parameter to <code class="literal">true</code> produces multi-position tokens, which are not
supported by indexing.</p>
<p>If this parameter is <code class="literal">true</code>, avoid using this filter in an index analyzer or
use the <a class="xref" href="analysis-tokenfilters.html#analysis-flatten-graph-tokenfilter" title="Flatten graph token filter"><code class="literal">flatten_graph</code></a> filter after
this filter to make the token stream suitable for indexing.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">protected_words</code>
</span>
</dt>
<dd>
(Optional, array of strings)
Array of tokens the filter won&#8217;t split.
</dd>
<dt>
<span class="term">
<code class="literal">protected_words_path</code>
</span>
</dt>
<dd>
<p>(Optional, string)
Path to a file that contains a list of tokens the filter won&#8217;t split.</p>
<p>This path must be absolute or relative to the <code class="literal">config</code> location, and the file
must be UTF-8 encoded. Each token in the file must be separated by a line
break.</p>
</dd>
<dt>
<span class="term">
<code class="literal">split_on_case_change</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter splits tokens at letter case transitions. For example:
<code class="literal">camelCase</code> &#8594; [ <code class="literal">camel</code>, <code class="literal">Case</code> ]. Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">split_on_numerics</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter splits tokens at letter-number transitions. For example:
<code class="literal">j2se</code> &#8594; [ <code class="literal">j</code>, <code class="literal">2</code>, <code class="literal">se</code> ]. Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">stem_english_possessive</code>
</span>
</dt>
<dd>
(Optional, Boolean)
If <code class="literal">true</code>, the filter removes the English possessive (<code class="literal">'s</code>) from the end of each
token. For example: <code class="literal">O'Neil's</code> &#8594; [ <code class="literal">O</code>, <code class="literal">Neil</code> ]. Defaults to <code class="literal">true</code>.
</dd>
<dt>
<span class="term">
<code class="literal">type_table</code>
</span>
</dt>
<dd>
<p>(Optional, array of strings)
Array of custom type mappings for characters. This allows you to map
non-alphanumeric characters as numeric or alphanumeric to avoid splitting on
those characters.</p>
<p>For example, the following array maps the plus (<code class="literal">+</code>) and hyphen (<code class="literal">-</code>) characters
as alphanumeric, which means they won&#8217;t be treated as delimiters:</p>
<p><code class="literal">[ "+ =&gt; ALPHA", "- =&gt; ALPHA" ]</code></p>
<p>Supported types include:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">ALPHA</code> (Alphabetical)
</li>
<li class="listitem">
<code class="literal">ALPHANUM</code> (Alphanumeric)
</li>
<li class="listitem">
<code class="literal">DIGIT</code> (Numeric)
</li>
<li class="listitem">
<code class="literal">LOWER</code> (Lowercase alphabetical)
</li>
<li class="listitem">
<code class="literal">SUBWORD_DELIM</code> (Non-alphanumeric delimiter)
</li>
<li class="listitem">
<code class="literal">UPPER</code> (Uppercase alphabetical)
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">type_table_path</code>
</span>
</dt>
<dd>
<p>(Optional, string)
Path to a file that contains custom type mappings for characters. This allows
you to map non-alphanumeric characters as numeric or alphanumeric to avoid
splitting on those characters.</p>
<p>For example, the contents of this file may contain the following:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt"># Map the $, %, '.', and ',' characters to DIGIT
# This might be useful for financial data.
$ =&gt; DIGIT
% =&gt; DIGIT
. =&gt; DIGIT
\\u002C =&gt; DIGIT

# in some cases you might not want to split on ZWJ
# this also tests the case where we need a bigger byte[]
# see https://en.wikipedia.org/wiki/Zero-width_joiner
\\u200D =&gt; ALPHANUM</pre>
</div>
<p>Supported types include:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">ALPHA</code> (Alphabetical)
</li>
<li class="listitem">
<code class="literal">ALPHANUM</code> (Alphanumeric)
</li>
<li class="listitem">
<code class="literal">DIGIT</code> (Numeric)
</li>
<li class="listitem">
<code class="literal">LOWER</code> (Lowercase alphabetical)
</li>
<li class="listitem">
<code class="literal">SUBWORD_DELIM</code> (Non-alphanumeric delimiter)
</li>
<li class="listitem">
<code class="literal">UPPER</code> (Uppercase alphabetical)
</li>
</ul>
</div>
<p>This file path must be absolute or relative to the <code class="literal">config</code> location, and the
file must be UTF-8 encoded. Each mapping in the file must be separated by a line
break.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-word-delimiter-graph-tokenfilter-customize"></a>Customize<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>To customize the <code class="literal">word_delimiter_graph</code> filter, duplicate it to create the basis
for a new custom token filter. You can modify the filter using its configurable
parameters.</p>
<p>For example, the following request creates a <code class="literal">word_delimiter_graph</code>
filter that uses the following rules:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Split tokens at non-alphanumeric characters, <em>except</em> the hyphen (<code class="literal">-</code>)
character.
</li>
<li class="listitem">
Remove leading or trailing delimiters from each token.
</li>
<li class="listitem">
Do <em>not</em> split tokens at letter case transitions.
</li>
<li class="listitem">
Do <em>not</em> split tokens at letter-number transitions.
</li>
<li class="listitem">
Remove the English possessive (<code class="literal">'s</code>) from the end of each token.
</li>
</ul>
</div>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "filter": [ "my_custom_word_delimiter_graph_filter" ]
        }
      },
      "filter": {
        "my_custom_word_delimiter_graph_filter": {
          "type": "word_delimiter_graph",
          "type_table": [ "- =&gt; ALPHA" ],
          "split_on_case_change": false,
          "split_on_numerics": false,
          "stem_english_possessive": true
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/528.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="analysis-word-delimiter-graph-differences"></a>Differences between <code class="literal">word_delimiter_graph</code> and <code class="literal">word_delimiter</code><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/7.10/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc">edit</a></h3>
</div></div></div>
<p>Both the <code class="literal">word_delimiter_graph</code> and
<a class="xref" href="analysis-tokenfilters.html#analysis-word-delimiter-tokenfilter" title="Word delimiter token filter"><code class="literal">word_delimiter</code></a> filters produce tokens
that span multiple positions when any of the following parameters are <code class="literal">true</code>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-all"><code class="literal">catenate_all</code></a>
</li>
<li class="listitem">
<a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-numbers"><code class="literal">catenate_numbers</code></a>
</li>
<li class="listitem">
<a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-words"><code class="literal">catenate_words</code></a>
</li>
<li class="listitem">
<a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-preserve-original"><code class="literal">preserve_original</code></a>
</li>
</ul>
</div>
<p>However, only the <code class="literal">word_delimiter_graph</code> filter assigns multi-position tokens a
<code class="literal">positionLength</code> attribute, which indicates the number of positions a token
spans. This ensures the <code class="literal">word_delimiter_graph</code> filter always produces valid
<a class="xref" href="analysis-concepts.html#token-graphs" title="Token graphs">token graphs</a>.</p>
<p>The <code class="literal">word_delimiter</code> filter does not assign multi-position tokens a
<code class="literal">positionLength</code> attribute. This means it produces invalid graphs for streams
including these tokens.</p>
<p>While indexing does not support token graphs containing multi-position tokens,
queries, such as the <a class="xref" href="full-text-queries.html#query-dsl-match-query-phrase" title="Match phrase query"><code class="literal">match_phrase</code></a> query, can
use these graphs to generate multiple sub-queries from a single query string.</p>
<p>To see how token graphs produced by the <code class="literal">word_delimiter</code> and
<code class="literal">word_delimiter_graph</code> filters differ, check out the following example.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Example</strong></span></summary>
<div class="content">
<p><a id="analysis-word-delimiter-graph-basic-token-graph"></a><span class="strong strong"><strong>Basic token graph</strong></span></p>
<p>Both the <code class="literal">word_delimiter</code> and <code class="literal">word_delimiter_graph</code> produce the following token
graph for <code class="literal">PowerShot2000</code> when the following parameters are <code class="literal">false</code>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-all"><code class="literal">catenate_all</code></a>
</li>
<li class="listitem">
<a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-numbers"><code class="literal">catenate_numbers</code></a>
</li>
<li class="listitem">
<a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-catenate-words"><code class="literal">catenate_words</code></a>
</li>
<li class="listitem">
<a class="xref" href="analysis-tokenfilters.html#word-delimiter-graph-tokenfilter-preserve-original"><code class="literal">preserve_original</code></a>
</li>
</ul>
</div>
<p>This graph does not contain multi-position tokens. All tokens span only one
position.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-basic.svg" alt="token graph basic">
</div>
</div>
<p><a id="analysis-word-delimiter-graph-wdg-token-graph"></a><span class="strong strong"><strong><code class="literal">word_delimiter_graph</code> graph with a multi-position token</strong></span></p>
<p>The <code class="literal">word_delimiter_graph</code> filter produces the following token graph for
<code class="literal">PowerShot2000</code> when <code class="literal">catenate_words</code> is <code class="literal">true</code>.</p>
<p>This graph correctly indicates the catenated <code class="literal">PowerShot</code> token spans two
positions.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-wdg.svg" alt="token graph wdg">
</div>
</div>
<p><a id="analysis-word-delimiter-graph-wd-token-graph"></a><span class="strong strong"><strong><code class="literal">word_delimiter</code> graph with a multi-position token</strong></span></p>
<p>When <code class="literal">catenate_words</code> is <code class="literal">true</code>, the <code class="literal">word_delimiter</code> filter produces
the following token graph for <code class="literal">PowerShot2000</code>.</p>
<p>Note that the catenated <code class="literal">PowerShot</code> token should span two positions but only
spans one in the token graph, making it invalid.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/analysis/token-graph-wd.svg" alt="token graph wd">
</div>
</div>
</div>
</details>
</div>

</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="analysis-tokenizers.html">« Tokenizer reference</a>
</span>
<span class="next">
<a href="analysis-charfilters.html">Character filters reference »</a>
</span>
</div>
</div>
</body>
</html>
