<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Tokenizer reference | Elasticsearch Guide | Elastic</title>
<meta class="elastic" name="content" content="Tokenizer reference | Elasticsearch Guide">

<link rel="home" href="index.html" title="Elasticsearch Guide"/>
<link rel="up" href="analysis.html" title="Text analysis"/>
<link rel="prev" href="analysis-analyzers.html" title="Built-in analyzer reference"/>
<link rel="next" href="analysis-tokenfilters.html" title="Token filter reference"/>
<meta class="elastic" name="product_version" content=""/>
<meta class="elastic" name="product_name" content=""/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/"/>
<meta name="DC.subject" content=""/>
<meta name="DC.identifier" content=""/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="analysis.html">Text analysis</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="analysis-analyzers.html">« Built-in analyzer reference</a>
</span>
<span class="next">
<a href="analysis-tokenfilters.html">Token filter reference »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-tokenizers"></a>Tokenizer reference<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers.asciidoc">edit</a></h2>
</div></div></div>
<p>A <em>tokenizer</em> receives a stream of characters, breaks it up into individual
<em>tokens</em> (usually individual words), and outputs a stream of <em>tokens</em>. For
instance, a <a class="xref" href="analysis-tokenizers.html#analysis-whitespace-tokenizer" title="Whitespace tokenizer"><code class="literal">whitespace</code></a> tokenizer breaks
text into tokens whenever it sees any whitespace. It would convert the text
<code class="literal">"Quick brown fox!"</code> into the terms <code class="literal">[Quick, brown, fox!]</code>.</p>
<p>The tokenizer is also responsible for recording the following:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Order or <em>position</em> of each term (used for phrase and word proximity queries)
</li>
<li class="listitem">
Start and end <em>character offsets</em> of the original word which the term
represents (used for highlighting search snippets).
</li>
<li class="listitem">
<em>Token type</em>, a classification of each term produced, such as <code class="literal">&lt;ALPHANUM&gt;</code>,
<code class="literal">&lt;HANGUL&gt;</code>, or <code class="literal">&lt;NUM&gt;</code>. Simpler analyzers only produce the <code class="literal">word</code> token type.
</li>
</ul>
</div>
<p>Elasticsearch has a number of built in tokenizers which can be used to build
<a class="xref" href="configure-text-analysis.html#analysis-custom-analyzer" title="Create a custom analyzer">custom analyzers</a>.</p>
<h3><a id="_word_oriented_tokenizers"></a>Word Oriented Tokenizers<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>The following tokenizers are usually used for tokenizing full text into
individual words:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-standard-tokenizer" title="Standard tokenizer">Standard Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">standard</code> tokenizer divides text into terms on word boundaries, as
defined by the Unicode Text Segmentation algorithm. It removes most
punctuation symbols. It is the best choice for most languages.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-letter-tokenizer" title="Letter tokenizer">Letter Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">letter</code> tokenizer divides text into terms whenever it encounters a
character which is not a letter.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-lowercase-tokenizer" title="Lowercase tokenizer">Lowercase Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">lowercase</code> tokenizer, like the <code class="literal">letter</code> tokenizer,  divides text into
terms whenever it encounters a character which is not a letter, but it also
lowercases all terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-whitespace-tokenizer" title="Whitespace tokenizer">Whitespace Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">whitespace</code> tokenizer divides text into terms whenever it encounters any
whitespace character.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-uaxurlemail-tokenizer" title="UAX URL email tokenizer">UAX URL Email Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">uax_url_email</code> tokenizer is like the <code class="literal">standard</code> tokenizer except that it
recognises URLs and email addresses as single tokens.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-classic-tokenizer" title="Classic tokenizer">Classic Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">classic</code> tokenizer is a grammar based tokenizer for the English Language.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-thai-tokenizer" title="Thai tokenizer">Thai Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">thai</code> tokenizer segments Thai text into words.
</dd>
</dl>
</div>
<h3><a id="_partial_word_tokenizers"></a>Partial Word Tokenizers<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>These tokenizers break up text or words into small fragments, for partial word
matching:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-ngram-tokenizer" title="N-gram tokenizer">N-Gram Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word: a sliding window of continuous letters, e.g. <code class="literal">quick</code> &#8594;
<code class="literal">[qu, ui, ic, ck]</code>.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-edgengram-tokenizer" title="Edge n-gram tokenizer">Edge N-Gram Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">edge_ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word which are anchored to the start of the word, e.g. <code class="literal">quick</code> &#8594;
<code class="literal">[q, qu, qui, quic, quick]</code>.
</dd>
</dl>
</div>
<h3><a id="_structured_text_tokenizers"></a>Structured Text Tokenizers<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>The following tokenizers are usually used with structured text like
identifiers, email addresses, zip codes, and paths, rather than with full
text:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-keyword-tokenizer" title="Keyword tokenizer">Keyword Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">keyword</code> tokenizer is a &#8220;noop&#8221; tokenizer that accepts whatever text it
is given and outputs the exact same text as a single term. It can be combined
with token filters like <a class="xref" href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter" title="Lowercase token filter"><code class="literal">lowercase</code></a> to
normalise the analysed terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-pattern-tokenizer" title="Pattern tokenizer">Pattern Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">pattern</code> tokenizer uses a regular expression to either split text into
terms whenever it matches a word separator, or to capture matching text as
terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-simplepattern-tokenizer" title="Simple pattern tokenizer">Simple Pattern Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">simple_pattern</code> tokenizer uses a regular expression to capture matching
text as terms. It uses a restricted subset of regular expression features
and is generally faster than the <code class="literal">pattern</code> tokenizer.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-chargroup-tokenizer" title="Character group tokenizer">Char Group Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">char_group</code> tokenizer is configurable through sets of characters to split
on, which is usually less expensive than running regular expressions.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-simplepatternsplit-tokenizer" title="Simple pattern split tokenizer">Simple Pattern Split Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">simple_pattern_split</code> tokenizer uses the same restricted regular expression
subset as the <code class="literal">simple_pattern</code> tokenizer, but splits the input at matches rather
than returning the matches as terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-tokenizers.html#analysis-pathhierarchy-tokenizer" title="Path hierarchy tokenizer">Path Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">path_hierarchy</code> tokenizer takes a hierarchical value like a filesystem
path, splits on the path separator, and emits a term for each component in the
tree, e.g. <code class="literal">/foo/bar/baz</code> &#8594; <code class="literal">[/foo, /foo/bar, /foo/bar/baz ]</code>.
</dd>
</dl>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-chargroup-tokenizer"></a>Character group tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/chargroup-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">char_group</code> tokenizer breaks text into terms whenever it encounters a
character which is in a defined set. It is mostly useful for cases where a simple
custom tokenization is desired, and the overhead of use of the <a class="xref" href="analysis-tokenizers.html#analysis-pattern-tokenizer" title="Pattern tokenizer"><code class="literal">pattern</code> tokenizer</a>
is not acceptable.</p>
<h3><a id="_configuration_8"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/chargroup-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">char_group</code> tokenizer accepts one parameter:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">tokenize_on_chars</code>
</p>
</td>
<td valign="top">
<p>
A list containing a list of characters to tokenize the string on. Whenever a character
from this list is encountered, a new token is started. This accepts either single
characters like e.g. <code class="literal">-</code>, or character groups: <code class="literal">whitespace</code>, <code class="literal">letter</code>, <code class="literal">digit</code>,
<code class="literal">punctuation</code>, <code class="literal">symbol</code>.
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">max_token_length</code>
</p>
</td>
<td valign="top">
<p>
The maximum token length. If a token is seen that exceeds this length then
it is split at <code class="literal">max_token_length</code> intervals. Defaults to <code class="literal">255</code>.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="_example_output_7"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/chargroup-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": {
    "type": "char_group",
    "tokenize_on_chars": [
      "whitespace",
      "-",
      "\n"
    ]
  },
  "text": "The QUICK brown-fox"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/427.console"></div>
<p>returns</p>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "tokens": [
    {
      "token": "The",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "QUICK",
      "start_offset": 4,
      "end_offset": 9,
      "type": "word",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 10,
      "end_offset": 15,
      "type": "word",
      "position": 2
    },
    {
      "token": "fox",
      "start_offset": 16,
      "end_offset": 19,
      "type": "word",
      "position": 3
    }
  ]
}</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-classic-tokenizer"></a>Classic tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/classic-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">classic</code> tokenizer is a grammar based tokenizer that is good for English
language documents. This tokenizer has heuristics for special treatment of
acronyms, company names, email addresses, and internet host names. However,
these rules don&#8217;t always work, and the tokenizer doesn&#8217;t work well for most
languages other than English:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
It splits words at most punctuation characters, removing punctuation. However, a
dot that&#8217;s not followed by whitespace is considered part of a token.
</li>
<li class="listitem">
It splits words at hyphens, unless there&#8217;s a number in the token, in which case
the whole token is interpreted as a product number and is not split.
</li>
<li class="listitem">
It recognizes email addresses and internet hostnames as one token.
</li>
</ul>
</div>
<h3><a id="_example_output_8"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/classic-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "classic",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/428.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]</pre>
</div>
<h3><a id="_configuration_9"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/classic-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">classic</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">max_token_length</code>
</p>
</td>
<td valign="top">
<p>
The maximum token length. If a token is seen that exceeds this length then
it is split at <code class="literal">max_token_length</code> intervals. Defaults to <code class="literal">255</code>.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="_example_configuration_6"></a>Example configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/classic-tokenizer.asciidoc">edit</a></h3>
<p>In this example, we configure the <code class="literal">classic</code> tokenizer to have a
<code class="literal">max_token_length</code> of 5 (for demonstration purposes):</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "classic",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/429.console"></div>
<p>The above example produces the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-edgengram-tokenizer"></a>Edge n-gram tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/edgengram-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">edge_ngram</code> tokenizer first breaks text down into words whenever it
encounters one of a list of specified characters, then it emits
<a href="https://en.wikipedia.org/wiki/N-gram" class="ulink" target="_top">N-grams</a> of each word where the start of
the N-gram is anchored to the beginning of the word.</p>
<p>Edge N-Grams are useful for <em>search-as-you-type</em> queries.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>When you need <em>search-as-you-type</em> for text which has a widely known
order, such as movie or song titles, the
<a class="xref" href="search.html#completion-suggester" title="Completion Suggester">completion suggester</a> is a much more efficient
choice than edge N-grams. Edge N-grams have the advantage when trying to
autocomplete words that can appear in any order.</p>
</div>
</div>
<h3><a id="_example_output_9"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/edgengram-tokenizer.asciidoc">edit</a></h3>
<p>With the default settings, the <code class="literal">edge_ngram</code> tokenizer treats the initial text as a
single token and produces N-grams with minimum length <code class="literal">1</code> and maximum length
<code class="literal">2</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "edge_ngram",
  "text": "Quick Fox"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/430.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Q, Qu ]</pre>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>These default gram lengths are almost entirely useless. You need to
configure the <code class="literal">edge_ngram</code> before using it.</p>
</div>
</div>
<h3><a id="_configuration_10"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/edgengram-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">edge_ngram</code> tokenizer accepts the following parameters:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">min_gram</code>
</span>
</dt>
<dd>
Minimum length of characters in a gram. Defaults to <code class="literal">1</code>.
</dd>
<dt>
<span class="term">
<code class="literal">max_gram</code>
</span>
</dt>
<dd>
<p>Maximum length of characters in a gram. Defaults to <code class="literal">2</code>.</p>
<p>See <a class="xref" href="analysis-tokenizers.html#max-gram-limits" title="Limitations of the max_gram parameter">Limitations of the <code class="literal">max_gram</code> parameter</a>.</p>
</dd>
<dt>
<span class="term">
<code class="literal">token_chars</code>
</span>
</dt>
<dd>
<p>
Character classes that should be included in a token. Elasticsearch
will split on characters that don&#8217;t belong to the classes specified.
Defaults to <code class="literal">[]</code> (keep all characters).
</p>
<p>Character classes may be any of the following:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">letter</code>&#8201;&#8212;&#8201;     for example <code class="literal">a</code>, <code class="literal">b</code>, <code class="literal">ï</code> or <code class="literal">京</code>
</li>
<li class="listitem">
<code class="literal">digit</code>&#8201;&#8212;&#8201;      for example <code class="literal">3</code> or <code class="literal">7</code>
</li>
<li class="listitem">
<code class="literal">whitespace</code>&#8201;&#8212;&#8201; for example <code class="literal">" "</code> or <code class="literal">"\n"</code>
</li>
<li class="listitem">
<code class="literal">punctuation</code>&#8201;&#8212;&#8201;for example <code class="literal">!</code> or <code class="literal">"</code>
</li>
<li class="listitem">
<code class="literal">symbol</code>&#8201;&#8212;&#8201;     for example <code class="literal">$</code> or <code class="literal">√</code>
</li>
<li class="listitem">
<code class="literal">custom</code>&#8201;&#8212;&#8201;     custom characters which need to be set using the
<code class="literal">custom_token_chars</code> setting.
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">custom_token_chars</code>
</span>
</dt>
<dd>
Custom characters that should be treated as part of a token. For example,
setting this to <code class="literal">+-_</code> will make the tokenizer treat the plus, minus and
underscore sign as part of a token.
</dd>
</dl>
</div>
<h3><a id="max-gram-limits"></a>Limitations of the <code class="literal">max_gram</code> parameter<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/edgengram-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">edge_ngram</code> tokenizer&#8217;s <code class="literal">max_gram</code> value limits the character length of
tokens. When the <code class="literal">edge_ngram</code> tokenizer is used with an index analyzer, this
means search terms longer than the <code class="literal">max_gram</code> length may not match any indexed
terms.</p>
<p>For example, if the <code class="literal">max_gram</code> is <code class="literal">3</code>, searches for <code class="literal">apple</code> won&#8217;t match the
indexed term <code class="literal">app</code>.</p>
<p>To account for this, you can use the
<a class="xref" href="analysis-tokenfilters.html#analysis-truncate-tokenfilter" title="Truncate token filter"><code class="literal">truncate</code></a> token filter with a search analyzer
to shorten search terms to the <code class="literal">max_gram</code> character length. However, this could
return irrelevant results.</p>
<p>For example, if the <code class="literal">max_gram</code> is <code class="literal">3</code> and search terms are truncated to three
characters, the search term <code class="literal">apple</code> is shortened to <code class="literal">app</code>. This means searches
for <code class="literal">apple</code> return any indexed terms matching <code class="literal">app</code>, such as <code class="literal">apply</code>, <code class="literal">approximate</code>
and <code class="literal">apple</code>.</p>
<p>We recommend testing both approaches to see which best fits your
use case and desired search experience.</p>
<h3><a id="_example_configuration_7"></a>Example configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/edgengram-tokenizer.asciidoc">edit</a></h3>
<p>In this example, we configure the <code class="literal">edge_ngram</code> tokenizer to treat letters and
digits as tokens, and to produce grams with minimum length <code class="literal">2</code> and maximum
length <code class="literal">10</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "2 Quick Foxes."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/431.console"></div>
<p>The above example produces the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Qu, Qui, Quic, Quick, Fo, Fox, Foxe, Foxes ]</pre>
</div>
<p>Usually we recommend using the same <code class="literal">analyzer</code> at index time and at search
time. In the case of the <code class="literal">edge_ngram</code> tokenizer, the advice is different. It
only makes sense to use the <code class="literal">edge_ngram</code> tokenizer at index time, to ensure
that partial words are available for matching in the index. At search time,
just search for the terms the user has typed in, for instance: <code class="literal">Quick Fo</code>.</p>
<p>Below is an example of how to set up a field for <em>search-as-you-type</em>.</p>
<p>Note that the <code class="literal">max_gram</code> value for the index analyzer is <code class="literal">10</code>, which limits
indexed terms to 10 characters. Search terms are not truncated, meaning that
search terms longer than 10 characters may not match any indexed terms.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "autocomplete": {
          "tokenizer": "autocomplete",
          "filter": [
            "lowercase"
          ]
        },
        "autocomplete_search": {
          "tokenizer": "lowercase"
        }
      },
      "tokenizer": {
        "autocomplete": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": [
            "letter"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "autocomplete",
        "search_analyzer": "autocomplete_search"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "title": "Quick Foxes" <a id="CO167-1"></a><i class="conum" data-value="1"></i>
}

POST my-index-000001/_refresh

GET my-index-000001/_search
{
  "query": {
    "match": {
      "title": {
        "query": "Quick Fo", <a id="CO167-2"></a><i class="conum" data-value="2"></i>
        "operator": "and"
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/432.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO167-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The <code class="literal">autocomplete</code> analyzer indexes the terms <code class="literal">[qu, qui, quic, quick, fo, fox, foxe, foxes]</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO167-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>The <code class="literal">autocomplete_search</code> analyzer searches for the terms <code class="literal">[quick, fo]</code>, both of which appear in the index.</p>
</td>
</tr>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-keyword-tokenizer"></a>Keyword tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/keyword-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">keyword</code> tokenizer is a &#8220;noop&#8221; tokenizer that accepts whatever text it
is given and outputs the exact same text as a single term. It can be combined
with token filters to normalise output, e.g. lower-casing email addresses.</p>
<h3><a id="_example_output_10"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/keyword-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "keyword",
  "text": "New York"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/433.console"></div>
<p>The above sentence would produce the following term:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ New York ]</pre>
</div>
<h3><a id="analysis-keyword-tokenizer-token-filters"></a>Combine with token filters<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/keyword-tokenizer.asciidoc">edit</a></h3>
<p>You can combine the <code class="literal">keyword</code> tokenizer with token filters to normalise
structured data, such as product IDs or email addresses.</p>
<p>For example, the following <a class="xref" href="indices.html#indices-analyze" title="Analyze API">analyze API</a> request uses the
<code class="literal">keyword</code> tokenizer and <a class="xref" href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter" title="Lowercase token filter"><code class="literal">lowercase</code></a> filter to
convert an email address to lowercase.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "keyword",
  "filter": [ "lowercase" ],
  "text": "john.SMITH@example.COM"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/434.console"></div>
<p>The request produces the following token:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ john.smith@example.com ]</pre>
</div>
<h3><a id="_configuration_11"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/keyword-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">keyword</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">buffer_size</code>
</p>
</td>
<td valign="top">
<p>
The number of characters read into the term buffer in a single pass.
Defaults to <code class="literal">256</code>. The term buffer will grow by this size until all the
text has been consumed. It is advisable not to change this setting.
</p>
</td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-letter-tokenizer"></a>Letter tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/letter-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">letter</code> tokenizer breaks text into terms whenever it encounters a
character which is not a letter. It does a reasonable job for most European
languages, but does a terrible job for some Asian languages, where words are
not separated by spaces.</p>
<h3><a id="_example_output_11"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/letter-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "letter",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/435.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, QUICK, Brown, Foxes, jumped, over, the, lazy, dog, s, bone ]</pre>
</div>
<h3><a id="_configuration_12"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/letter-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">letter</code> tokenizer is not configurable.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-lowercase-tokenizer"></a>Lowercase tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/lowercase-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">lowercase</code> tokenizer, like the
<a class="xref" href="analysis-tokenizers.html#analysis-letter-tokenizer" title="Letter tokenizer"><code class="literal">letter</code> tokenizer</a> breaks text into terms
whenever it encounters a character which is not a letter, but it also
lowercases all terms. It is functionally equivalent to the
<a class="xref" href="analysis-tokenizers.html#analysis-letter-tokenizer" title="Letter tokenizer"><code class="literal">letter</code> tokenizer</a> combined with the
<a class="xref" href="analysis-tokenfilters.html#analysis-lowercase-tokenfilter" title="Lowercase token filter"><code class="literal">lowercase</code> token filter</a>, but is more
efficient as it performs both steps in a single pass.</p>
<h3><a id="_example_output_12"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/lowercase-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "lowercase",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/436.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]</pre>
</div>
<h3><a id="_configuration_13"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/lowercase-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">lowercase</code> tokenizer is not configurable.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-ngram-tokenizer"></a>N-gram tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/ngram-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">ngram</code> tokenizer first breaks text down into words whenever it encounters
one of a list of specified characters, then it emits
<a href="https://en.wikipedia.org/wiki/N-gram" class="ulink" target="_top">N-grams</a> of each word of the specified
length.</p>
<p>N-grams are like a sliding window that moves across the word - a continuous
sequence of characters of the specified length. They are useful for querying
languages that don&#8217;t use spaces or that have long compound words, like German.</p>
<h3><a id="_example_output_13"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/ngram-tokenizer.asciidoc">edit</a></h3>
<p>With the default settings, the <code class="literal">ngram</code> tokenizer treats the initial text as a
single token and produces N-grams with minimum length <code class="literal">1</code> and maximum length
<code class="literal">2</code>:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "ngram",
  "text": "Quick Fox"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/437.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Q, Qu, u, ui, i, ic, c, ck, k, "k ", " ", " F", F, Fo, o, ox, x ]</pre>
</div>
<h3><a id="_configuration_14"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/ngram-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">ngram</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">min_gram</code>
</p>
</td>
<td valign="top">
<p>
Minimum length of characters in a gram. Defaults to <code class="literal">1</code>.
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">max_gram</code>
</p>
</td>
<td valign="top">
<p>
Maximum length of characters in a gram. Defaults to <code class="literal">2</code>.
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">token_chars</code>
</p>
</td>
<td valign="top">
<p>
<p>
Character classes that should be included in a token. Elasticsearch
will split on characters that don&#8217;t belong to the classes specified.
Defaults to <code class="literal">[]</code> (keep all characters).
</p>
<p>Character classes may be any of the following:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">letter</code>&#8201;&#8212;&#8201;     for example <code class="literal">a</code>, <code class="literal">b</code>, <code class="literal">ï</code> or <code class="literal">京</code>
</li>
<li class="listitem">
<code class="literal">digit</code>&#8201;&#8212;&#8201;      for example <code class="literal">3</code> or <code class="literal">7</code>
</li>
<li class="listitem">
<code class="literal">whitespace</code>&#8201;&#8212;&#8201; for example <code class="literal">" "</code> or <code class="literal">"\n"</code>
</li>
<li class="listitem">
<code class="literal">punctuation</code>&#8201;&#8212;&#8201;for example <code class="literal">!</code> or <code class="literal">"</code>
</li>
<li class="listitem">
<code class="literal">symbol</code>&#8201;&#8212;&#8201;     for example <code class="literal">$</code> or <code class="literal">√</code>
</li>
<li class="listitem">
<code class="literal">custom</code>&#8201;&#8212;&#8201;     custom characters which need to be set using the
<code class="literal">custom_token_chars</code> setting.
</li>
</ul>
</div>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">custom_token_chars</code>
</p>
</td>
<td valign="top">
<p>
Custom characters that should be treated as part of a token. For example,
setting this to <code class="literal">+-_</code> will make the tokenizer treat the plus, minus and
underscore sign as part of a token.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>It usually makes sense to set <code class="literal">min_gram</code> and <code class="literal">max_gram</code> to the same
value. The smaller the length, the more documents will match but the lower
the quality of the matches. The longer the length, the more specific the
matches. A tri-gram (length <code class="literal">3</code>) is a good place to start.</p>
</div>
</div>
<p>The index level setting <code class="literal">index.max_ngram_diff</code> controls the maximum allowed
difference between <code class="literal">max_gram</code> and <code class="literal">min_gram</code>.</p>
<h3><a id="_example_configuration_8"></a>Example configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/ngram-tokenizer.asciidoc">edit</a></h3>
<p>In this example, we configure the <code class="literal">ngram</code> tokenizer to treat letters and
digits as tokens, and to produce tri-grams (grams of length <code class="literal">3</code>):</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "ngram",
          "min_gram": 3,
          "max_gram": 3,
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "2 Quick Foxes."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/438.console"></div>
<p>The above example produces the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Qui, uic, ick, Fox, oxe, xes ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-pathhierarchy-tokenizer"></a>Path hierarchy tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/pathhierarchy-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">path_hierarchy</code> tokenizer takes a hierarchical value like a filesystem
path, splits on the path separator, and emits a term for each component in the
tree. The <code class="literal">path_hierarcy</code> tokenizer uses Lucene&#8217;s
<a href="https://lucene.apache.org/core/9_7_0/analysis/common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html" class="ulink" target="_top">PathHierarchyTokenizer</a>
underneath.</p>
<h3><a id="_example_output_14"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/pathhierarchy-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": "/one/two/three"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/439.console"></div>
<p>The above text would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ /one, /one/two, /one/two/three ]</pre>
</div>
<h3><a id="_configuration_15"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/pathhierarchy-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">path_hierarchy</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">delimiter</code>
</p>
</td>
<td valign="top">
<p>
The character to use as the path separator. Defaults to <code class="literal">/</code>.
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">replacement</code>
</p>
</td>
<td valign="top">
<p>
An optional replacement character to use for the delimiter.
Defaults to the <code class="literal">delimiter</code>.
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">buffer_size</code>
</p>
</td>
<td valign="top">
<p>
The number of characters read into the term buffer in a single pass.
Defaults to <code class="literal">1024</code>. The term buffer will grow by this size until all the
text has been consumed. It is advisable not to change this setting.
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">reverse</code>
</p>
</td>
<td valign="top">
<p>
If <code class="literal">true</code>, uses Lucene&#8217;s
<a href="http://lucene.apache.org/core/9_7_0/analysis/common/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.html" class="ulink" target="_top">ReversePathHierarchyTokenizer</a>,
which is suitable for domain–like hierarchies. Defaults to <code class="literal">false</code>.
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">skip</code>
</p>
</td>
<td valign="top">
<p>
The number of initial tokens to skip. Defaults to <code class="literal">0</code>.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="_example_configuration_9"></a>Example configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/pathhierarchy-tokenizer.asciidoc">edit</a></h3>
<p>In this example, we configure the <code class="literal">path_hierarchy</code> tokenizer to split on <code class="literal">-</code>
characters, and to replace them with <code class="literal">/</code>. The first two tokens are skipped:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "path_hierarchy",
          "delimiter": "-",
          "replacement": "/",
          "skip": 2
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "one-two-three-four-five"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/440.console"></div>
<p>The above example produces the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ /three, /three/four, /three/four/five ]</pre>
</div>
<p>If we were to set <code class="literal">reverse</code> to <code class="literal">true</code>, it would produce the following:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ one/two/three/, two/three/, three/ ]</pre>
</div>
<h3><a id="analysis-pathhierarchy-tokenizer-detailed-examples"></a>Detailed examples<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/pathhierarchy-tokenizer.asciidoc">edit</a></h3>
<p>A common use-case for the <code class="literal">path_hierarchy</code> tokenizer is filtering results by
file paths. If indexing a file path along with the data, the use of the
<code class="literal">path_hierarchy</code> tokenizer to analyze the path allows filtering the results
by different parts of the file path string.</p>
<p>This example configures an index to have two custom analyzers and applies
those analyzers to multifields of the <code class="literal">file_path</code> text field that will
store filenames. One of the two analyzers uses reverse tokenization.
Some sample documents are then indexed to represent some file paths
for photos inside photo folders of two different users.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT file-path-test
{
  "settings": {
    "analysis": {
      "analyzer": {
        "custom_path_tree": {
          "tokenizer": "custom_hierarchy"
        },
        "custom_path_tree_reversed": {
          "tokenizer": "custom_hierarchy_reversed"
        }
      },
      "tokenizer": {
        "custom_hierarchy": {
          "type": "path_hierarchy",
          "delimiter": "/"
        },
        "custom_hierarchy_reversed": {
          "type": "path_hierarchy",
          "delimiter": "/",
          "reverse": "true"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "file_path": {
        "type": "text",
        "fields": {
          "tree": {
            "type": "text",
            "analyzer": "custom_path_tree"
          },
          "tree_reversed": {
            "type": "text",
            "analyzer": "custom_path_tree_reversed"
          }
        }
      }
    }
  }
}

POST file-path-test/_doc/1
{
  "file_path": "/User/alice/photos/2017/05/16/my_photo1.jpg"
}

POST file-path-test/_doc/2
{
  "file_path": "/User/alice/photos/2017/05/16/my_photo2.jpg"
}

POST file-path-test/_doc/3
{
  "file_path": "/User/alice/photos/2017/05/16/my_photo3.jpg"
}

POST file-path-test/_doc/4
{
  "file_path": "/User/alice/photos/2017/05/15/my_photo1.jpg"
}

POST file-path-test/_doc/5
{
  "file_path": "/User/bob/photos/2017/05/16/my_photo1.jpg"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/441.console"></div>
<p>A search for a particular file path string against the text field matches all
the example documents, with Bob&#8217;s documents ranking highest due to <code class="literal">bob</code> also
being one of the terms created by the standard analyzer boosting relevance for
Bob&#8217;s documents.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET file-path-test/_search
{
  "query": {
    "match": {
      "file_path": "/User/bob/photos/2017/05"
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/442.console"></div>
<p>It&#8217;s simple to match or filter documents with file paths that exist within a
particular directory using the <code class="literal">file_path.tree</code> field.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET file-path-test/_search
{
  "query": {
    "term": {
      "file_path.tree": "/User/alice/photos/2017/05/16"
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/443.console"></div>
<p>With the reverse parameter for this tokenizer, it&#8217;s also possible to match
from the other end of the file path, such as individual file names or a deep
level subdirectory. The following example shows a search for all files named
<code class="literal">my_photo1.jpg</code> within any directory via the <code class="literal">file_path.tree_reversed</code> field
configured to use the reverse parameter in the mapping.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET file-path-test/_search
{
  "query": {
    "term": {
      "file_path.tree_reversed": {
        "value": "my_photo1.jpg"
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/444.console"></div>
<p>Viewing the tokens generated with both forward and reverse is instructive
in showing the tokens created for the same file path value.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST file-path-test/_analyze
{
  "analyzer": "custom_path_tree",
  "text": "/User/alice/photos/2017/05/16/my_photo1.jpg"
}

POST file-path-test/_analyze
{
  "analyzer": "custom_path_tree_reversed",
  "text": "/User/alice/photos/2017/05/16/my_photo1.jpg"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/445.console"></div>
<p>It&#8217;s also useful to be able to filter with file paths when combined with other
types of searches, such as this example looking for any files paths with <code class="literal">16</code>
that also must be in Alice&#8217;s photo directory.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET file-path-test/_search
{
  "query": {
    "bool" : {
      "must" : {
        "match" : { "file_path" : "16" }
      },
      "filter": {
        "term" : { "file_path.tree" : "/User/alice" }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/446.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-pattern-tokenizer"></a>Pattern tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/pattern-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">pattern</code> tokenizer uses a regular expression to either split text into
terms whenever it matches a word separator, or to capture matching text as
terms.</p>
<p>The default pattern is <code class="literal">\W+</code>, which splits text whenever it encounters
non-word characters.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<h3>Beware of Pathological Regular Expressions</h3>
<p>The pattern tokenizer uses
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html" class="ulink" target="_top">Java Regular Expressions</a>.</p>
<p>A badly written regular expression could run very slowly or even throw a
StackOverflowError and cause the node it is running on to exit suddenly.</p>
<p>Read more about <a href="https://www.regular-expressions.info/catastrophic.html" class="ulink" target="_top">pathological regular expressions and how to avoid them</a>.</p>
</div>
</div>
<h3><a id="_example_output_15"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/pattern-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "pattern",
  "text": "The foo_bar_size's default is 5."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/447.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, foo_bar_size, s, default, is, 5 ]</pre>
</div>
<h3><a id="_configuration_16"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/pattern-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">pattern</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">pattern</code>
</p>
</td>
<td valign="top">
<p>
A <a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html" class="ulink" target="_top">Java regular expression</a>, defaults to <code class="literal">\W+</code>.
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">flags</code>
</p>
</td>
<td valign="top">
<p>
Java regular expression <a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#field.summary" class="ulink" target="_top">flags</a>.
Flags should be pipe-separated, eg <code class="literal">"CASE_INSENSITIVE|COMMENTS"</code>.
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">group</code>
</p>
</td>
<td valign="top">
<p>
Which capture group to extract as tokens. Defaults to <code class="literal">-1</code> (split).
</p>
</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="_example_configuration_10"></a>Example configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/pattern-tokenizer.asciidoc">edit</a></h3>
<p>In this example, we configure the <code class="literal">pattern</code> tokenizer to break text into
tokens when it encounters commas:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "pattern",
          "pattern": ","
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "comma,separated,values"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/448.console"></div>
<p>The above example produces the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ comma, separated, values ]</pre>
</div>
<p>In the next example, we configure the <code class="literal">pattern</code> tokenizer to capture values
enclosed in double quotes (ignoring embedded escaped quotes <code class="literal">\"</code>). The regex
itself looks like this:</p>
<pre class="literallayout">"((?:\\"|[^"]|\\")*)"</pre>

<p>And reads as follows:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
A literal <code class="literal">"</code>
</li>
<li class="listitem">
<p>Start capturing:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
A literal <code class="literal">\"</code> OR any character except <code class="literal">"</code>
</li>
<li class="listitem">
Repeat until no more characters match
</li>
</ul>
</div>
</li>
<li class="listitem">
A literal closing <code class="literal">"</code>
</li>
</ul>
</div>
<p>When the pattern is specified in JSON, the <code class="literal">"</code> and <code class="literal">\</code> characters need to be
escaped, so the pattern ends up looking like:</p>
<pre class="literallayout">\"((?:\\\\\"|[^\"]|\\\\\")+)\"</pre>

<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "pattern",
          "pattern": "\"((?:\\\\\"|[^\"]|\\\\\")+)\"",
          "group": 1
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "\"value\", \"value with embedded \\\" quote\""
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/449.console"></div>
<p>The above example produces the following two terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ value, value with embedded \" quote ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-simplepattern-tokenizer"></a>Simple pattern tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/simplepattern-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">simple_pattern</code> tokenizer uses a regular expression to capture matching
text as terms. The set of regular expression features it supports is more
limited than the <a class="xref" href="analysis-tokenizers.html#analysis-pattern-tokenizer" title="Pattern tokenizer"><code class="literal">pattern</code></a> tokenizer, but the
tokenization is generally faster.</p>
<p>This tokenizer does not support splitting the input on a pattern match, unlike
the <a class="xref" href="analysis-tokenizers.html#analysis-pattern-tokenizer" title="Pattern tokenizer"><code class="literal">pattern</code></a> tokenizer. To split on pattern
matches using the same restricted regular expression subset, see the
<a class="xref" href="analysis-tokenizers.html#analysis-simplepatternsplit-tokenizer" title="Simple pattern split tokenizer"><code class="literal">simple_pattern_split</code></a> tokenizer.</p>
<p>This tokenizer uses <a href="https://lucene.apache.org/core/9_7_0/core/org/apache/lucene/util/automaton/RegExp.html" class="ulink" target="_top">Lucene regular expressions</a>.
For an explanation of the supported features and syntax, see <a class="xref" href="regexp-syntax.html" title="Regular expression syntax">Regular Expression Syntax</a>.</p>
<p>The default pattern is the empty string, which produces no terms. This
tokenizer should always be configured with a non-default pattern.</p>
<h3><a id="_configuration_17"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/simplepattern-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">simple_pattern</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">pattern</code>
</p>
</td>
<td valign="top">
<p>
<a href="https://lucene.apache.org/core/9_7_0/core/org/apache/lucene/util/automaton/RegExp.html" class="ulink" target="_top">Lucene regular expression</a>, defaults to the empty string.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="_example_configuration_11"></a>Example configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/simplepattern-tokenizer.asciidoc">edit</a></h3>
<p>This example configures the <code class="literal">simple_pattern</code> tokenizer to produce terms that are
three-digit numbers</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "simple_pattern",
          "pattern": "[0123456789]{3}"
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "fd-786-335-514-x"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/450.console"></div>
<p>The above example produces these terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ 786, 335, 514 ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-simplepatternsplit-tokenizer"></a>Simple pattern split tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/simplepatternsplit-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">simple_pattern_split</code> tokenizer uses a regular expression to split the
input into terms at pattern matches. The set of regular expression features it
supports is more limited than the <a class="xref" href="analysis-tokenizers.html#analysis-pattern-tokenizer" title="Pattern tokenizer"><code class="literal">pattern</code></a>
tokenizer, but the tokenization is generally faster.</p>
<p>This tokenizer does not produce terms from the matches themselves. To produce
terms from matches using patterns in the same restricted regular expression
subset, see the <a class="xref" href="analysis-tokenizers.html#analysis-simplepattern-tokenizer" title="Simple pattern tokenizer"><code class="literal">simple_pattern</code></a>
tokenizer.</p>
<p>This tokenizer uses <a href="https://lucene.apache.org/core/9_7_0/core/org/apache/lucene/util/automaton/RegExp.html" class="ulink" target="_top">Lucene regular expressions</a>.
For an explanation of the supported features and syntax, see <a class="xref" href="regexp-syntax.html" title="Regular expression syntax">Regular Expression Syntax</a>.</p>
<p>The default pattern is the empty string, which produces one term containing the
full input. This tokenizer should always be configured with a non-default
pattern.</p>
<h3><a id="_configuration_18"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/simplepatternsplit-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">simple_pattern_split</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">pattern</code>
</p>
</td>
<td valign="top">
<p>
A <a href="https://lucene.apache.org/core/9_7_0/core/org/apache/lucene/util/automaton/RegExp.html" class="ulink" target="_top">Lucene regular expression</a>, defaults to the empty string.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="_example_configuration_12"></a>Example configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/simplepatternsplit-tokenizer.asciidoc">edit</a></h3>
<p>This example configures the <code class="literal">simple_pattern_split</code> tokenizer to split the input
text on underscores.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "simple_pattern_split",
          "pattern": "_"
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "an_underscored_phrase"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/451.console"></div>
<p>The above example produces these terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ an, underscored, phrase ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-standard-tokenizer"></a>Standard tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/standard-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">standard</code> tokenizer provides grammar based tokenization (based on the
Unicode Text Segmentation algorithm, as specified in
<a href="https://unicode.org/reports/tr29/" class="ulink" target="_top">Unicode Standard Annex #29</a>) and works well
for most languages.</p>
<h3><a id="_example_output_16"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/standard-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/452.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]</pre>
</div>
<h3><a id="_configuration_19"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/standard-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">standard</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">max_token_length</code>
</p>
</td>
<td valign="top">
<p>
The maximum token length. If a token is seen that exceeds this length then
it is split at <code class="literal">max_token_length</code> intervals. Defaults to <code class="literal">255</code>.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="_example_configuration_13"></a>Example configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/standard-tokenizer.asciidoc">edit</a></h3>
<p>In this example, we configure the <code class="literal">standard</code> tokenizer to have a
<code class="literal">max_token_length</code> of 5 (for demonstration purposes):</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/453.console"></div>
<p>The above example produces the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-thai-tokenizer"></a>Thai tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/thai-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">thai</code> tokenizer segments Thai text into words, using the Thai
segmentation algorithm included with Java. Text in other languages in general
will be treated the same as the
<a class="xref" href="analysis-tokenizers.html#analysis-standard-tokenizer" title="Standard tokenizer"><code class="literal">standard</code> tokenizer</a>.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>This tokenizer may not be supported by all JREs. It is known to work
with Sun/Oracle and OpenJDK. If your application needs to be fully portable,
consider using the <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/8.9/analysis-icu-tokenizer.html" class="ulink" target="_top">ICU Tokenizer</a> instead.</p>
</div>
</div>
<h3><a id="_example_output_17"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/thai-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "thai",
  "text": "การที่ได้ต้องแสดงว่างานดี"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/454.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ การ, ที่, ได้, ต้อง, แสดง, ว่า, งาน, ดี ]</pre>
</div>
<h3><a id="_configuration_20"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/thai-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">thai</code> tokenizer is not configurable.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-uaxurlemail-tokenizer"></a>UAX URL email tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/uaxurlemail-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">uax_url_email</code> tokenizer is like the <a class="xref" href="analysis-tokenizers.html#analysis-standard-tokenizer" title="Standard tokenizer"><code class="literal">standard</code> tokenizer</a> except that it
recognises URLs and email addresses as single tokens.</p>
<h3><a id="_example_output_18"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/uaxurlemail-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "uax_url_email",
  "text": "Email me at john.smith@global-international.com"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/455.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Email, me, at, john.smith@global-international.com ]</pre>
</div>
<p>while the <code class="literal">standard</code> tokenizer would produce:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ Email, me, at, john.smith, global, international.com ]</pre>
</div>
<h3><a id="_configuration_21"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/uaxurlemail-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">uax_url_email</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">max_token_length</code>
</p>
</td>
<td valign="top">
<p>
The maximum token length. If a token is seen that exceeds this length then
it is split at <code class="literal">max_token_length</code> intervals. Defaults to <code class="literal">255</code>.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="_example_configuration_14"></a>Example configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/uaxurlemail-tokenizer.asciidoc">edit</a></h3>
<p>In this example, we configure the <code class="literal">uax_url_email</code> tokenizer to have a
<code class="literal">max_token_length</code> of 5 (for demonstration purposes):</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "uax_url_email",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "john.smith@global-international.com"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/456.console"></div>
<p>The above example produces the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ john, smith, globa, l, inter, natio, nal.c, om ]</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-whitespace-tokenizer"></a>Whitespace tokenizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/whitespace-tokenizer.asciidoc">edit</a></h2>
</div></div></div>

<p>The <code class="literal">whitespace</code> tokenizer breaks text into terms whenever it encounters a
whitespace character.</p>
<h3><a id="_example_output_19"></a>Example output<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/whitespace-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "whitespace",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/457.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog's, bone. ]</pre>
</div>
<h3><a id="_configuration_22"></a>Configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/analysis/tokenizers/whitespace-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">whitespace</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">max_token_length</code>
</p>
</td>
<td valign="top">
<p>
The maximum token length. If a token is seen that exceeds this length then
it is split at <code class="literal">max_token_length</code> intervals. Defaults to <code class="literal">255</code>.
</p>
</td>
</tr>
</tbody>
</table>
</div>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="analysis-analyzers.html">« Built-in analyzer reference</a>
</span>
<span class="next">
<a href="analysis-tokenfilters.html">Token filter reference »</a>
</span>
</div>
</div>
</body>
</html>
